{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pk5hIYW8e-W"
      },
      "source": [
        "### 1- Preprocessing :\n",
        "\n",
        "define characters accepted and tashkeel accepted and remove from the training set any tashkeel and unwanted characters (eg. brackets, numbers... etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zyj-hwPy8e-X"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pyarabic.araby as araby\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import tensorflow as tf\n",
        "# import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional ,Dropout, Concatenate, Flatten,Input, concatenate,TimeDistributed\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDCbPGSl8e-Z",
        "outputId": "fdd65803-038e-497d-fe80-e053a65675d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "18515271\n"
          ]
        }
      ],
      "source": [
        "arabicDictionary=['ى', 'ع', 'ظ', 'ح', 'ر', 'س', 'ي', 'ش', 'ض', 'ق', ' ', 'ث', 'ل', 'ص', 'ط', 'ك', 'آ', 'م', 'ا', 'إ', 'ه', 'ز', 'ء', 'أ', 'ف', 'ؤ', 'غ', 'ج', 'ئ', 'د', 'ة', 'خ', 'و', 'ب', 'ذ', 'ت', 'ن']\n",
        "punctuations = [\"،\", \":\", \"؛\", \"-\", \"؟\"]\n",
        "#reading the training dataset\n",
        "f = open(r\"train.txt\", \"r\",encoding=\"utf-8\").read()\n",
        "print(type(f))\n",
        "print(len(f))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DT8E5vQk8e-Z"
      },
      "outputs": [],
      "source": [
        "def cleanDataFromNonArabicLetters(data):\n",
        "    #regex to keep arabic letters only and remove any other character (eg. brackets, numbers ...etc)\n",
        "    characters_regex =r'[\\s\\.\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD]+'\n",
        "    processedData = re.findall(characters_regex,data)\n",
        "    processedData = \" \".join(processedData)\n",
        "    processedData = re.sub(r\"\\s+\",\" \" ,processedData) #substitute many spaces with one space only\n",
        "    return processedData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWbsLqx68e-a",
        "outputId": "55a001cd-6d9a-4e7f-92f0-ee34d5487d6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17997403\n"
          ]
        }
      ],
      "source": [
        "processedData=cleanDataFromNonArabicLetters(f)\n",
        "print(len(processedData))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "msZvg8zl8e-a"
      },
      "outputs": [],
      "source": [
        "def removeDiacratics(processedData):\n",
        "    without_diacritics= araby.strip_diacritics(processedData)\n",
        "    return without_diacritics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFe_FguB8e-a",
        "outputId": "77a526a4-f0b8-4585-caa4-669733a5633c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10711561\n"
          ]
        }
      ],
      "source": [
        "without_diacritics = removeDiacratics(processedData)\n",
        "print(len(without_diacritics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zNPODnYT8e-a"
      },
      "outputs": [],
      "source": [
        "#take the procrssed text and separate it into sentences\n",
        "def generateListsWithDiacraticsandWithout(with_diacratics, without_diacritics):\n",
        "    #---------------------Preprocessing of words with diacratics------------\n",
        "\n",
        "    #generate longStringSplited which is the list of procrssed text without brackets and numbers and dots\n",
        "    #the rest of the punctuation still there\n",
        "    longStringWithDecimalPoint=re.sub(r\"\\n\", ' ', with_diacratics) #string of data with diacratics removed endlines from it\n",
        "    for element in punctuations:\n",
        "        longStringWithDecimalPoint=re.sub(element, '', longStringWithDecimalPoint) #remove punctuations from the string\n",
        "\n",
        "    longStringSplited=longStringWithDecimalPoint.split('.') #split the string by dots (segment sentences) and make list of them\n",
        "    longString=' '.join(longStringSplited)  #long string without decimal points\n",
        "\n",
        "    #-------------------------------------\n",
        "\n",
        "    #---------------------Preprocessing of words without diacratics------------\n",
        "    without_diacratics_longStringWithDecimals=re.sub(r\"\\n\", ' ', without_diacritics) #string of data without diacratics removed endlines from it\n",
        "    for element in punctuations:\n",
        "        without_diacratics_longStringWithDecimals=re.sub(element, '', without_diacratics_longStringWithDecimals) #remove punctuations from the string\n",
        "\n",
        "    longStringSplited_withoutDiacratics=without_diacratics_longStringWithDecimals.split('.') #split the string by dots (segment sentences) and make list of them\n",
        "    longString_withoutDiacratics=' '.join(longStringSplited_withoutDiacratics)  #long string without decimal points\n",
        "\n",
        "    #-------------------------------------------------------------------------\n",
        "\n",
        "    list_of_sentences=[]\n",
        "    for line in longStringSplited_withoutDiacratics: #list of lists of sentences splitted in words without diacratics (used in embeddings)\n",
        "        list_of_sentences.append(line.split(\" \"))\n",
        "\n",
        "\n",
        "    # now the variable called longString has a single string with all the processed words in it\n",
        "    listOfwordsWith_Diacritics=list()\n",
        "    listOfwordsWith_NoDiacritics=list()\n",
        "\n",
        "    listOfwordsWith_Diacritics=re.sub(r\"\\s+\", ' ', longString)\n",
        "    listOfwordsWith_Diacritics=listOfwordsWith_Diacritics.split(\" \")\n",
        "\n",
        "    listOfwordsWith_NoDiacritics=re.sub(r\"\\s+\", ' ', longString_withoutDiacratics)\n",
        "    listOfwordsWith_NoDiacritics=listOfwordsWith_NoDiacritics.split(\" \")\n",
        "\n",
        "    return list_of_sentences,listOfwordsWith_Diacritics,listOfwordsWith_NoDiacritics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WN4oFLlv8e-a"
      },
      "outputs": [],
      "source": [
        "list_of_sentences,listOfwordsWith_Diacritics,listOfwordsWith_NoDiacritics= generateListsWithDiacraticsandWithout(processedData, without_diacritics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCiZwq3T8e-b",
        "outputId": "5952e769-525b-408d-a946-23b75ce31872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40836\n",
            "2102054\n",
            "2102054\n",
            "['أَوْ', 'سَيِّدُهَا', 'فِي', 'ذَلِكَ', 'جَازَ', 'لِأَنَّ', 'لَهُ', 'غَرَضًا', 'فِي', 'تَزَيُّنِهَا', 'لَهُ', 'كَمَا', 'فِي', 'الرَّوْضَةِ', 'وَهُوَ', 'الْأَوْجَهُ', 'وَإِنْ', 'جَرَى', 'فِي', 'التَّحْقِيقِ', 'عَلَى', 'خِلَافِ', 'ذَلِكَ', 'فِي', 'الْوَصْلِ', 'وَالْوَشْرِ', 'فَأَلْحَقَهُمَا', 'بِالْوَشْمِ', 'فِي', 'الْمَنْعِ', 'مُطْلَقًا', 'وَيُكْرَهُ', 'أَنْ', 'يَنْتِفَ', 'الشَّيْبَ', 'مِنْ', 'الْمَحَلِّ', 'الَّذِي', 'لَا', 'يُطْلَبُ', 'مِنْهُ', 'إزَالَةُ', 'شَعْرِهِ', 'وَيُسَنُّ', 'خَضْبُهُ', 'بِالْحِنَّاءِ', 'وَنَحْوِهِ', 'وَيُسَنُّ', 'لِلْمَرْأَةِ', 'الْمُزَوَّجَةِ', 'وَالْمَمْلُوكَةِ', 'خَضْبُ', 'كَفِّهَا', 'وَقَدَمِهَا', 'بِذَلِكَ', 'تَعْمِيمًا', 'لِأَنَّهُ', 'زِينَةٌ', 'وَهِيَ', 'مَطْلُوبَةٌ', 'مِنْهَا', 'لِحَلِيلِهَا', 'أَمَّا', 'النَّقْشُ', 'وَالتَّطْرِيفُ', 'فَلَا', 'يُسَنُّ', 'وَخَرَجَ', 'بِالْمُزَوَّجَةِ', 'وَالْمَمْلُوكَةِ', 'غَيْرُهُمَا', 'فَيُكْرَهُ', 'لَهُ', 'وَبِالْمَرْأَةِ', 'الرَّجُلُ', 'وَالْخُنْثَى', 'فَيَحْرُمُ', 'الْخِضَابُ', 'عَلَيْهِمَا', 'إلَّا', 'لِعُذْرٍ', 'نِهَايَةٌ', 'وَمُغْنِي', 'قَالَ', 'ع', 'ش', 'قَوْلُهُ', 'م', 'ر', 'وَيَحْرُمُ', 'عَلَى', 'الْمَرْأَةِ', 'خَرَجَ', 'بِالْمَرْأَةِ', 'غَيْرُهَا', 'مِنْ', 'ذَكَرٍ', 'وَأُنْثَى', 'صَغِيرَيْنِ', 'فَيَجُوزُ']\n",
            "['أو', 'سيدها', 'في', 'ذلك', 'جاز', 'لأن', 'له', 'غرضا', 'في', 'تزينها', 'له', 'كما', 'في', 'الروضة', 'وهو', 'الأوجه', 'وإن', 'جرى', 'في', 'التحقيق', 'على', 'خلاف', 'ذلك', 'في', 'الوصل', 'والوشر', 'فألحقهما', 'بالوشم', 'في', 'المنع', 'مطلقا', 'ويكره', 'أن', 'ينتف', 'الشيب', 'من', 'المحل', 'الذي', 'لا', 'يطلب', 'منه', 'إزالة', 'شعره', 'ويسن', 'خضبه', 'بالحناء', 'ونحوه', 'ويسن', 'للمرأة', 'المزوجة', 'والمملوكة', 'خضب', 'كفها', 'وقدمها', 'بذلك', 'تعميما', 'لأنه', 'زينة', 'وهي', 'مطلوبة', 'منها', 'لحليلها', 'أما', 'النقش', 'والتطريف', 'فلا', 'يسن', 'وخرج', 'بالمزوجة', 'والمملوكة', 'غيرهما', 'فيكره', 'له', 'وبالمرأة', 'الرجل', 'والخنثى', 'فيحرم', 'الخضاب', 'عليهما', 'إلا', 'لعذر', 'نهاية', 'ومغني', 'قال', 'ع', 'ش', 'قوله', 'م', 'ر', 'ويحرم', 'على', 'المرأة', 'خرج', 'بالمرأة', 'غيرها', 'من', 'ذكر', 'وأنثى', 'صغيرين', 'فيجوز']\n"
          ]
        }
      ],
      "source": [
        "print(len(list_of_sentences))\n",
        "print(len(listOfwordsWith_NoDiacritics))\n",
        "print(len(listOfwordsWith_Diacritics))\n",
        "print(listOfwordsWith_Diacritics[2101000:2101100])\n",
        "print(listOfwordsWith_NoDiacritics[2101000:2101100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A9KcNsc78e-b"
      },
      "outputs": [],
      "source": [
        "def generateDiacraticsList(listOfwordsWith_Diacritics):\n",
        "    #now that we have two separated lists we need to get the diacritics list\n",
        "\n",
        "\n",
        "    listofDiacritrcs_ToWord=list()\n",
        "    temp=list()\n",
        "    counter=0\n",
        "    for word in listOfwordsWith_Diacritics:\n",
        "        while counter<len(word):\n",
        "            if word[counter] in arabicDictionary: #checking if the character is a letter\n",
        "                if (counter+1)<len(word):\n",
        "                    #checking if the next character is also a letter, then that means that the diacritics of the current letter is none so add empty string to the list\n",
        "                    if word[counter +1] in arabicDictionary:\n",
        "                        temp.append(\"\")\n",
        "                        counter+=1\n",
        "                        continue\n",
        "                counter+=1 #if it is the end of the word (no more letters) or the next character is a diacritics -> continue looping\n",
        "                continue\n",
        "            else:\n",
        "                if (counter+1)<len(word):\n",
        "                    if word[(counter+1)] not in arabicDictionary: #if the current and the next characters are diacritics, add them together in the list\n",
        "                        temp.append(word[counter]+word[counter+1])\n",
        "                        counter+=2\n",
        "                        continue\n",
        "                temp.append(word[counter]) #if the current character only is the diacritics add it to the list\n",
        "                counter+=1\n",
        "        listofDiacritrcs_ToWord.append(temp.copy())\n",
        "        temp.clear()\n",
        "        counter=0\n",
        "    return listofDiacritrcs_ToWord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s6qEYdcc8e-b"
      },
      "outputs": [],
      "source": [
        "listofDiacritrcs_ToWord= generateDiacraticsList(listOfwordsWith_Diacritics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfd12Wcp8e-b",
        "outputId": "48c6aa14-a2a9-41b6-f003-b0f7ceeb7df5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2102054\n",
            "الْأَوَّلُ\n",
            "الأول\n",
            "['', 'ْ', 'َ', 'َّ', 'ُ']\n",
            " ْ َ َّ ُ\n",
            "[['َ', 'ْ', 'ُ', 'ُ'], ['َ', 'ْ'], ['َ', 'َ', 'َ'], ['', 'ْ', 'َ', 'َّ', 'ُ'], ['َ', 'َ', 'ُ'], ['', 'َ', 'ْ'], ['َ', '', 'َ'], ['', '', 'َّ', 'ْ', 'َ', 'ِ', 'ُّ'], ['', 'ْ', 'ُ'], ['َ', 'َ', 'َ', 'َ'], ['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'ْ', 'ٍ'], ['َ', 'ْ', 'َ', 'ِ', ''], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['َ', 'ْ', 'ِ'], ['َ', 'ِ', '', 'ٍ'], ['ِ', '', 'ْ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['ُ', 'ُ', '', 'َ'], ['َ'], ['ُ', 'ِ', 'َ'], ['ُ', 'ُ', '', 'ُ', 'ُ'], ['ِ', 'ْ'], ['', '', 'ِّ', '', 'ِ'], ['َ', 'ُ', '', 'َ', 'ً'], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['ُ', 'ْ', 'َ', 'ٍ'], ['ِ', 'َ', 'َ', 'ٍ'], ['َ', 'َ', 'ِّ'], ['ُ', 'َّ', '', 'ٍ'], ['', 'ْ', 'ُ'], ['َ', 'َ', 'َ', 'َ'], ['َ', 'ْ', 'ُ'], ['', 'ْ', 'ِ'], ['َ', '', 'ٍ'], ['َ', 'ْ'], ['ِ', 'ِ', 'ْ', 'ٍ'], ['َ', 'َ', 'َ', 'َّ', 'ُ', 'ُ'], ['ُ', 'َ'], ['َ', 'ُ', 'ْ', 'ِ'], ['', '', 'ُّ', 'َّ', '', 'ِ'], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['', 'ْ', 'ُ', 'ْ', 'َ', 'ِ'], ['ِ'], ['َ', 'ِ', '', 'ِ'], ['', '', 'َّ', 'َ', '', 'َ', 'ِ'], ['َ', '', '', 'ُّ', 'ُ', '', 'ِ'], ['ِ', '', 'َّ', 'َ', 'ِ'], ['َ', 'َ', 'ْ', 'ِ'], ['َ', 'ِ', 'َ'], ['َ', 'ِ', 'ْ', 'ٍ'], ['ُ', 'َ', 'َّ', 'ٌ'], ['َ', 'ْ', 'ُ'], ['َ', '', 'ِ', 'ٍ'], ['َ', 'َ', 'ْ', 'َ', '', 'ِ', 'ِ'], ['َ', 'َّ'], ['', '', 'َّ', '', 'ِ', 'َ'], ['َ', '', 'ِ', 'ٌ'], ['ِ', 'َ', '', 'َّ', 'ِ'], ['َ', 'َ', '', 'َ'], ['َ', '', 'َ'], ['َ', '', 'ِ', 'ٌ'], ['ُ', 'َ'], ['َ', '', '', 'ِّ', 'ْ', 'ِ', '', 'ِ'], ['', 'َ'], ['َ', 'ِ', 'َ'], ['', '', 'ِّ', 'ْ', 'َ'], ['ِ', 'َ', 'ْ', 'ِ', 'ِ'], ['ُ', 'ِ', 'َ'], ['َ', 'َ', 'ْ'], ['ُ', 'ْ', 'َ', 'َ', 'ْ'], ['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'َ', 'ِ'], ['َ'], ['َ', 'َ', 'َ', 'َّ', 'ُ'], ['', 'َ', 'ْ'], ['َ', 'ْ'], ['', 'ْ', 'َ', 'ِ', 'َّ', 'ُ'], ['َ', 'ْ', 'ُ', 'ُ'], ['َ'], ['َ', 'َّ'], ['َ', 'ْ'], ['ُ', 'َ', 'ْ', 'َ'], ['َ', 'ْ', 'ِ'], ['', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', 'ْ'], ['َ', 'َ', 'ْ'], ['', 'ْ', 'َ', 'َ', 'َ'], ['َ', 'َ'], ['َ', 'ْ', 'َ', 'ْ'], ['َ', 'ُ'], ['ِ', 'َ', '', 'ٍ'], ['َ', 'ْ'], ['َ', 'ْ', 'ُ', '', 'ُ'], ['َ', '', 'ً'], ['َ', 'َ'], ['َ', 'َ', 'َ'], ['َ', 'ُ'], ['ِ', 'ْ', 'َ'], ['', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'ْ']]\n"
          ]
        }
      ],
      "source": [
        "print(len(listofDiacritrcs_ToWord))\n",
        "print(listOfwordsWith_Diacritics[3])\n",
        "print(listOfwordsWith_NoDiacritics[3])\n",
        "print(listofDiacritrcs_ToWord[3])\n",
        "print(\" \".join(listofDiacritrcs_ToWord[3]))\n",
        "print(listofDiacritrcs_ToWord[0:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ1PfuoL8e-c"
      },
      "source": [
        "2-Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PcEuZUcR8e-c"
      },
      "outputs": [],
      "source": [
        "def word2vecmodel(input):\n",
        "    # Train Word2Vec model\n",
        "    Word2Vec_model = Word2Vec(input, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "    Word2Vec_model.train(input,total_examples=len(listOfwordsWith_NoDiacritics),epochs=10)\n",
        "    word_embeddings = {word: Word2Vec_model.wv[word] for word in Word2Vec_model.wv.index_to_key}\n",
        "    return Word2Vec_model, word_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkhOJfrv8e-c",
        "outputId": "81432630-38c4-4465-c0ae-fb20e5de579b"
      },
      "outputs": [],
      "source": [
        "word_embeddings_model, word_embeddings= word2vecmodel(list_of_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqvLfTXu8e-c"
      },
      "source": [
        "3-Building the LSTM model for character level classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lurACkX88e-c"
      },
      "outputs": [],
      "source": [
        "def formatingXandYtrain(listofDiacritrcs_ToWord,listOfwordsWith_NoDiacritics):\n",
        "    for i in range(len(listofDiacritrcs_ToWord)):\n",
        "        listofDiacritrcs_ToWord[i] = \" \".join(listofDiacritrcs_ToWord[i])\n",
        "    Y_train= np.array([listofDiacritrcs_ToWord],dtype=object).T\n",
        "    X_train = np.array([listOfwordsWith_NoDiacritics],dtype=object)\n",
        "    print(str(X_train.shape))\n",
        "    print(str(Y_train.shape))\n",
        "    return X_train,Y_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHlZPh-K8e-c",
        "outputId": "bf41f93e-21e2-4bd0-d32e-7e9df95d637a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 2102054)\n",
            "(2102054, 1)\n"
          ]
        }
      ],
      "source": [
        "X_train,Y_train= formatingXandYtrain(listofDiacritrcs_ToWord,listOfwordsWith_NoDiacritics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qIMz0yEH8e-c"
      },
      "outputs": [],
      "source": [
        "def prepareForTraining(listOfwordsWith_NoDiacritics,listofDiacritrcs_ToWord):\n",
        "    # Tokenize the input words and diacritics\n",
        "    word_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters='')\n",
        "    word_tokenizer.fit_on_texts(listOfwordsWith_NoDiacritics)\n",
        "    word_sequences = word_tokenizer.texts_to_sequences(listOfwordsWith_NoDiacritics)\n",
        "\n",
        "    diacritic_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters='')\n",
        "    diacritic_tokenizer.fit_on_texts([''.join(d) for d in listofDiacritrcs_ToWord])\n",
        "    diacritic_sequences = diacritic_tokenizer.texts_to_sequences([''.join(d) for d in listofDiacritrcs_ToWord])\n",
        "\n",
        "    # Pad sequences to have the same length\n",
        "    max_len = max(max(len(seq) for seq in word_sequences), max(len(seq) for seq in diacritic_sequences))\n",
        "\n",
        "    return word_tokenizer,diacritic_tokenizer,max_len,word_sequences,diacritic_sequences\n",
        "\n",
        "def padSequences(word_sequences,diacritic_sequences, max_len):\n",
        "    # Pad sequences to have the same length\n",
        "    padded_word_sequences = pad_sequences(word_sequences, maxlen=max_len, padding='post')\n",
        "    padded_diacritic_sequences = pad_sequences(diacritic_sequences, maxlen=max_len, padding='post')\n",
        "    return padded_word_sequences,padded_diacritic_sequences\n",
        "\n",
        "def trainDiacritics(word_tokenizer,diacritic_tokenizer,max_len,padded_word_sequences,padded_diacritic_sequences):\n",
        "\n",
        "    # Build the LSTM model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=50, input_length=max_len))\n",
        "    model.add(LSTM(100, return_sequences=True))\n",
        "    model.add(Dense(len(diacritic_tokenizer.word_index) + 1, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(padded_word_sequences, np.expand_dims(padded_diacritic_sequences, -1), epochs=10, batch_size=32)\n",
        "\n",
        "    # Save the model for later use\n",
        "    model.save(\"diacritic_prediction_model.h5\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Now, you can use the trained model to predict diacritics for new Arabic words\n",
        "def predict_diacritics(model,word_tokenizer, diacritic_tokenizer,max_len, word):\n",
        "    word_sequence = word_tokenizer.texts_to_sequences([word])\n",
        "    padded_word_sequence = pad_sequences(word_sequence, maxlen=max_len, padding='post')\n",
        "    predicted_diacritic_sequence = model.predict(padded_word_sequence)\n",
        "    predicted_diacritic_sequence = np.argmax(predicted_diacritic_sequence, axis=-1)\n",
        "    predicted_diacritic = diacritic_tokenizer.sequences_to_texts(predicted_diacritic_sequence)\n",
        "    return predicted_diacritic[0]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "0Y8-PRNu8e-d",
        "outputId": "879e92aa-40a8-4c60-bb9a-48b13b885ffa"
      },
      "outputs": [],
      "source": [
        "word_tokenizer,diacritic_tokenizer,max_len,word_sequences,diacritic_sequences = prepareForTraining(listOfwordsWith_NoDiacritics,listofDiacritrcs_ToWord)\n",
        "padded_word_sequences,padded_diacritic_sequences=padSequences(word_sequences,diacritic_sequences,max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBWiJoUl8e-d"
      },
      "source": [
        "# !!!! DO NOT RUN THE COMING CELL UNLESS YOU HAVE MADE UPDATES IN THE MODEL\n",
        "you can use the model by calling:\n",
        "### model = load_model('diacritic_prediction_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkS8ReGy8e-d",
        "outputId": "d4cffa80-ac28-4a58-b09a-0f6d1e1f9a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "65690/65690 [==============================] - 919s 14ms/step - loss: 0.1176 - accuracy: 0.9577\n",
            "Epoch 2/10\n",
            "65690/65690 [==============================] - 926s 14ms/step - loss: 0.0918 - accuracy: 0.9656\n",
            "Epoch 3/10\n",
            "65690/65690 [==============================] - 15081s 230ms/step - loss: 0.0885 - accuracy: 0.9666\n",
            "Epoch 4/10\n",
            "65690/65690 [==============================] - 1742s 27ms/step - loss: 0.0869 - accuracy: 0.9671\n",
            "Epoch 5/10\n",
            "65690/65690 [==============================] - 1458s 22ms/step - loss: 0.0860 - accuracy: 0.9674\n",
            "Epoch 6/10\n",
            "65690/65690 [==============================] - 1331s 20ms/step - loss: 0.0853 - accuracy: 0.9676\n",
            "Epoch 7/10\n",
            "65690/65690 [==============================] - 1434s 22ms/step - loss: 0.0849 - accuracy: 0.9677\n",
            "Epoch 8/10\n",
            "65690/65690 [==============================] - 1538s 23ms/step - loss: 0.0845 - accuracy: 0.9678\n",
            "Epoch 9/10\n",
            "65690/65690 [==============================] - 2518s 38ms/step - loss: 0.0843 - accuracy: 0.9679\n",
            "Epoch 10/10\n",
            "65690/65690 [==============================] - 2832s 43ms/step - loss: 0.0840 - accuracy: 0.9679\n"
          ]
        }
      ],
      "source": [
        "model=trainDiacritics(word_tokenizer,diacritic_tokenizer,max_len,padded_word_sequences,padded_diacritic_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JQLjXSbw8e-d"
      },
      "outputs": [],
      "source": [
        "model = load_model('./models/diacritic_prediction_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHuqtTuf8e-d",
        "outputId": "5b6a1160-1c33-48ce-f5ec-92522a207d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of words 4\n",
            "1/1 [==============================] - 1s 527ms/step\n",
            "ذَهَبَ\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "عَلَيَّ\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "إلَى\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "الَّْشاِطِئ\n",
            "ذَهَبَ عَلَيَّ إلَى الَّْشاِطِئ \n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "# input = \"الذي علم بالقلم\"\n",
        "input= \"ذهب علي إلى الشاطئ\"\n",
        "# input = \"قال الزركشي ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر\"\n",
        "input_word = input.split(' ')\n",
        "word_vectors = [word_embeddings_model.wv[word] for word in input_word if word in word_embeddings_model.wv]\n",
        "# print(word_vectors)\n",
        "outputString=\"\"\n",
        "print(\"number of words\",len(input_word))\n",
        "for word in input_word:\n",
        "  predicted_diacritics1 = predict_diacritics(model,word_tokenizer, diacritic_tokenizer,max_len, word)\n",
        "  #print(predicted_diacritics1.split(' '))\n",
        "  predictedDiacritics=(predicted_diacritics1.split('  '))\n",
        "  temp=list()\n",
        "  for diacrtic in predictedDiacritics:\n",
        "    # print(diacrtic)\n",
        "    # print(len(diacrtic))\n",
        "    if ' ' in diacrtic:\n",
        "      # print(\"after space removel\")\n",
        "      diacrtic=diacrtic.replace(' ','')\n",
        "      # print(diacrtic)\n",
        "      # print(len(diacrtic))\n",
        "      temp.append(diacrtic)\n",
        "      continue\n",
        "    temp.append(diacrtic)\n",
        "  predictedDiacritics=temp.copy()\n",
        "  tempString=\"\"\n",
        "  for i in range(len(word)):\n",
        "    tempString+=word[i]\n",
        "    if i<len(predictedDiacritics):\n",
        "      tempString+=predictedDiacritics[i]\n",
        "  print(tempString)\n",
        "  tempString+=\" \"\n",
        "  outputString+=tempString\n",
        "\n",
        "print(outputString)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TzBU3UGN8e-d"
      },
      "outputs": [],
      "source": [
        "def get_word_context_embedding(word):\n",
        "    try:\n",
        "        return word_embeddings_model.wv[word]\n",
        "    except KeyError:\n",
        "        return np.zeros(word_embeddings_model.vector_size)  # Return zero vector for out-of-vocabulary words\n",
        "\n",
        "word_context_embeddings = [get_word_context_embedding(word) for word in listOfwordsWith_NoDiacritics]\n",
        "word_context_embeddings_padded = pad_sequences(word_context_embeddings, maxlen=max_len, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2102054, 24)\n",
            "(2102054, 24)\n"
          ]
        }
      ],
      "source": [
        "print(word_context_embeddings_padded.shape)\n",
        "print(padded_word_sequences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "r3DUsB-z8e-d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2102054, 48)\n"
          ]
        }
      ],
      "source": [
        "combined_embeddings = np.concatenate([word_context_embeddings_padded, padded_word_sequences], axis=-1)\n",
        "max_len_diacritic = combined_embeddings.shape[1]  # Adjust to match the expected model input length\n",
        "print(combined_embeddings.shape)\n",
        "\n",
        "# padded_word_sequences = pad_sequences(word_sequences, maxlen=max_len, padding='post')\n",
        "padded_diacritic_sequences = pad_sequences(diacritic_sequences, maxlen=max_len_diacritic, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Modify the LSTM Model\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=200, input_length=max_len_diacritic))\n",
        "# model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "# model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "# model.add(Dense(len(diacritic_tokenizer.word_index) + 1, activation='softmax'))\n",
        "\n",
        "# # Compile and Train the Model\n",
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# max_index = len(diacritic_tokenizer.word_index)  # Maximum index in the vocabulary\n",
        "# if np.max(padded_diacritic_sequences) > max_index or np.min(padded_diacritic_sequences) < 0:\n",
        "#     # Adjust indices if they are out of range\n",
        "#     padded_diacritic_sequences = np.clip(padded_diacritic_sequences, 0, max_index)\n",
        "\n",
        "# # Reshape y_true to match the model's expected output shape\n",
        "# # Expand dimensions for the sparse_categorical_crossentropy\n",
        "# y_true = np.expand_dims(padded_diacritic_sequences, -1)\n",
        "\n",
        "# # Train the model\n",
        "# print(\"combined_embeddings shape:\", combined_embeddings.shape)\n",
        "# print(\"y_true shape:\", y_true.shape)\n",
        "\n",
        "# print(\"Word tokenizer vocab size:\", len(word_tokenizer.word_index))\n",
        "# print(\"Diacritic tokenizer vocab size:\", len(diacritic_tokenizer.word_index))\n",
        "\n",
        "# model.fit(combined_embeddings, y_true, epochs=1, batch_size=64)\n",
        "\n",
        "\n",
        "# # Save the model for later use\n",
        "# model.save(\"diacritic_prediction_model_with_word2vec.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model2=trainDiacritics(word_tokenizer,diacritic_tokenizer,max_len,word_sequences,diacritic_sequences,padded_word_sequences,padded_diacritic_sequences)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
