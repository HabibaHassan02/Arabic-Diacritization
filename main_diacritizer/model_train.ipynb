{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "from constants import *\n",
    "from encoding_decoding_lookup import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diacritizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check steps in model_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just like the lab, but with tf, we would do init with all needed variables and a forward fn\n",
    "# #what we need: self, window_size, lstm_size, dropout_rate,embedding_size\n",
    "# class Diacritizer:\n",
    "#     def __init__(self)\n",
    "#     def __init__(self, vocab_size=35181, embedding_dim=50, hidden_size=50, n_classes=len(tag_map)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure if we need vocab size since we use window\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "\n",
    "class Diacritizer(Model):\n",
    "    def __init__(self, embedding_size=DEFAULT_EMBEDDING_SIZE, lstm_size=DEFAULT_LSTM_SIZE, window_size=DEFAULT_WINDOW_SIZE,dropout_rate=DEFAULT_DROPOUT_RATE):\n",
    "        super(Diacritizer, self).__init__()\n",
    "\n",
    "        # in the initialization, we define: input, embeddings, and output\n",
    "        # in lab 5 we had a linear where number of neurons= number of classes but here we have dense\n",
    "\n",
    "        ######################################## Creating the layers of our model #####################\n",
    "\n",
    "        # Step 1: define the input\n",
    "        # input would be the fixed window size after lookup \n",
    "        # Input in tf defines the input layer with shape=(window_size,)\n",
    "        # it represents the input data.\n",
    "        self.inputs = Input(shape=(window_size,), name='input')\n",
    "\n",
    "\n",
    "        # Step 2: define the embeddings\n",
    "        # it converts the integer index to dense vectors with fixed size\n",
    "        # input dim is dim of the letters (length of valid input letters we have)\n",
    "        # the output is the dense vector (embeddings size which is 128)\n",
    "        # the ( ) at the end mean that this layer would be applied to the input layer\n",
    "        self.embedding = Embedding(input_dim= len(SORTED_VALID_INPUT_LETTERS) + 1, output_dim= embedding_size, name='embedding')(self.inputs)\n",
    "        \n",
    "\n",
    "        # Step 3: Define the Bidirectional LSTM layers (we have 4 layers of each class and an initial layer)\n",
    "        \n",
    "        # initial layer that would be applied to the embeddings\n",
    "        self.initial_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                      name='initial_layer')(self.embedding)\n",
    "        \n",
    "        # first layer is sukun layer that would be applied on initial layer\n",
    "        self.sukoon_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                     name='sukoon_layer')(self.initial_layer)\n",
    "        \n",
    "        # sec layer is shadda layer that would be applied to sukun layer\n",
    "        self.shadda_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                     name='shadda_layer')(self.sukoon_layer)\n",
    "        \n",
    "        # third layer is sec layer that would be applied to shadda layer\n",
    "        self.secondary_diacritics_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                                   name='secondary_diacritics_layer')(self.shadda_layer)\n",
    "        \n",
    "        # forth layer is primary layer that would be applied to sec layer\n",
    "        self.primary_diacritics_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                                 name='primary_diacritics_layer')(self.secondary_diacritics_layer)\n",
    "\n",
    "        # Step 4: Define the output layers (we have 4 outputs with diff dense)\n",
    "        # dense functions define the activation functions in tensor flow\n",
    "        # where dense=1 means sigmoid and dense =2 means softmax\n",
    "\n",
    "        # Sukun output would have a sigmoid as its binary and would come from the sukun layer\n",
    "        # dense= 1 means single neuorn we just wanna know if 0 no sukun or 1 yes sukun\n",
    "        # SIGMOID\n",
    "        self.sukoon_output = Dense(1, activation='sigmoid',name='sukoon_output')(self.sukoon_layer)\n",
    "\n",
    "        # shadda output\n",
    "        # SIGMOID\n",
    "        self.shadda_output = Dense(1,activation='sigmoid',   name='shadda_output')(self.shadda_layer)\n",
    "\n",
    "        # sec output\n",
    "        # dense= 4 as we have multi classification task, where each neuron assigned to one class\n",
    "        # we have 0,1,2,3 as per the lookup table\n",
    "        # SOFTMAX\n",
    "        self.secondary_diacritics_output = Dense(4, activation='softmax' , name='secondary_diacritics_output')(self.secondary_diacritics_layer)\n",
    "\n",
    "        # prim output\n",
    "        # SOFTMAX\n",
    "        self.primary_diacritics_output = Dense(4,  activation='softmax' ,name='primary_diacritics_output')(self.primary_diacritics_layer)\n",
    "       \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Step 4: Define the forward pass through the layers\n",
    "        initial_layer = self.initial_layer(inputs)\n",
    "        sukoon_layer = self.sukoon_layer(initial_layer)\n",
    "        shadda_layer = self.shadda_layer(sukoon_layer)\n",
    "        secondary_diacritics_layer = self.secondary_diacritics_layer(shadda_layer)\n",
    "        primary_diacritics_layer = self.primary_diacritics_layer(secondary_diacritics_layer)\n",
    "\n",
    "\n",
    "        # Step 5: Separate outputs for each diacritic\n",
    "        sukoon_output = self.sukoon_output(sukoon_layer)\n",
    "        shadda_output = self.shadda_output(shadda_layer)\n",
    "        secondary_diacritics_output = self.secondary_diacritics_output(secondary_diacritics_layer)\n",
    "        primary_diacritics_output = self.primary_diacritics_output(primary_diacritics_layer)\n",
    "       \n",
    "\n",
    "        return primary_diacritics_output, secondary_diacritics_output, shadda_output, sukoon_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Diacritizer object at 0x0000025BF61C94C0>\n"
     ]
    }
   ],
   "source": [
    "model = Diacritizer()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input/output preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences with Diacritics: قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ ابْنُ عَرَفَةَ قَوْلُهُ بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ابْنُ عَرَفَةَ قَوْلُ ابْنِ شَاسٍ أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ وَسِحْرٍ مُحَمَّدٌ قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِرٌ بِاَللَّهِ تَعَالَى قَالَ مَالِكٌ هُوَ كَالزِّنْدِيقِ إذَا عَمِلَ السِّحْرَ بِنَفْسِهِ قُتِلَ وَلَمْ يُسْتَتَبْ \n",
      "\n",
      "Sentences without Diacritics: قوله أو قطع الأول يده إلخ قال الزركشي ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر وشد زنار ابن عرفة قول ابن شاس أو بفعل يتضمنه هو كلبس الزنار وإلقاء المصحف في صريح النجاسة والسجود للصنم ونحو ذلك وسحر محمد قول مالك وأصحابه أن الساحر كافر بالله تعالى قال مالك هو كالزنديق إذا عمل السحر بنفسه قتل ولم يستتب \n",
      "\n",
      "Length of Sentences with Diacritics: 40836\n",
      "Length of Sentences without Diacritics: 40836\n"
     ]
    }
   ],
   "source": [
    "def read_sentences_from_file(file1_path, file2_path):\n",
    "    sentences_with_diacritics = []\n",
    "    sentences_without_diacritics = []\n",
    "\n",
    "    with open(file1_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            sentences_with_diacritics.append(line)\n",
    "\n",
    "    with open(file2_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            sentences_without_diacritics.append(line)\n",
    "\n",
    "    return sentences_with_diacritics, sentences_without_diacritics\n",
    "\n",
    "# Example usage:\n",
    "file_path_with_diacritics = '../data/sentences_with_diacritics.txt'\n",
    "file_path_without_diacritics = '../data/sentences_without_diacritics.txt'\n",
    "\n",
    "sentences_with_diacritics, sentences_without_diacritics = read_sentences_from_file(file_path_with_diacritics,file_path_without_diacritics)\n",
    "print(\"Sentences with Diacritics:\", sentences_with_diacritics[0])\n",
    "print(\"Sentences without Diacritics:\", sentences_without_diacritics[0])\n",
    "print(\"Length of Sentences with Diacritics:\", len(sentences_with_diacritics))\n",
    "print(\"Length of Sentences without Diacritics:\", len(sentences_without_diacritics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قَوْلُهُ\n",
      "قوله \n"
     ]
    }
   ],
   "source": [
    "print(sentences_with_diacritics[0][0:8])\n",
    "print(sentences_without_diacritics[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabicDictionary=['ى', 'ع', 'ظ', 'ح', 'ر', 'س', 'ي', 'ش', 'ض', 'ق', ' ', 'ث', 'ل', 'ص', 'ط', 'ك', 'آ', 'م', 'ا', 'إ', 'ه', 'ز', 'ء', 'أ', 'ف', 'ؤ', 'غ', 'ج', 'ئ', 'د', 'ة', 'خ', 'و', 'ب', 'ذ', 'ت', 'ن']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDiacriticsForSentence(listOfSentencesWithDiacritics):\n",
    "    #now that we have two separated lists we need to get the diacritics list\n",
    "    short_vowels_list=list()\n",
    "    double_case_endings_list=list()\n",
    "    shadda_list=list()\n",
    "    sukoon_list=list()\n",
    "    counter=0\n",
    "    # letters_counter=0\n",
    "    for word in listOfSentencesWithDiacritics:\n",
    "        while counter<len(word):\n",
    "            if word[counter] in arabicDictionary: #checking if the character is a letter\n",
    "                # letters_counter+=1\n",
    "                if (counter+1)<len(word):\n",
    "                    #checking if the next character is also a letter, then that means that the diacritics of the current letter is none so add empty string to the list\n",
    "                    if word[counter +1] in arabicDictionary:\n",
    "                        short_vowels_list.append(\"\")\n",
    "                        double_case_endings_list.append(\"\")\n",
    "                        shadda_list.append(\"\")\n",
    "                        sukoon_list.append(\"\")\n",
    "                        counter+=2\n",
    "                        # letters_counter+=1\n",
    "                        continue\n",
    "                counter+=1 #if it is the end of the word (no more letters) or the next character is a diacritics -> continue looping\n",
    "                continue\n",
    "            else:\n",
    "                if word[counter] in SHORT_VOWELS:\n",
    "                    short_vowels_list.append(word[counter])\n",
    "                    double_case_endings_list.append(\"\")\n",
    "                    shadda_list.append(\"\")\n",
    "                    sukoon_list.append(\"\")\n",
    "                elif word[counter] in DOUBLE_CASE_ENDINGS:\n",
    "                    double_case_endings_list.append(word[counter])\n",
    "                    short_vowels_list.append(\"\")\n",
    "                    shadda_list.append(\"\")\n",
    "                    sukoon_list.append(\"\")\n",
    "                elif word[counter] == SHADDA:\n",
    "                    shadda_list.append(word[counter])\n",
    "                    short_vowels_list.append(\"\")\n",
    "                    double_case_endings_list.append(\"\")\n",
    "                    sukoon_list.append(\"\")\n",
    "                else:\n",
    "                    sukoon_list.append(word[counter])\n",
    "                    short_vowels_list.append(\"\")\n",
    "                    double_case_endings_list.append(\"\")\n",
    "                    shadda_list.append(\"\")\n",
    "                counter+=1\n",
    "        counter=0\n",
    "    # print(letters_counter)\n",
    "    return short_vowels_list,double_case_endings_list,shadda_list,sukoon_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDiacriticsForDataSet(sentences_with_diacritics):\n",
    "    short_vowels_list=list()\n",
    "    double_case_endings_list=list()\n",
    "    shadda_list=list()\n",
    "    sukoon_list=list()\n",
    "    for sentence in sentences_with_diacritics:\n",
    "        list_to_be_sent= sentence.split(\" \")\n",
    "        if (\"\\n\") in list_to_be_sent:\n",
    "            list_to_be_sent.remove(\"\\n\")\n",
    "        sv_list,dce_list,sh_list,su_list= getDiacriticsForSentence(list_to_be_sent)\n",
    "        short_vowels_list.append(sv_list)\n",
    "        double_case_endings_list.append(dce_list)\n",
    "        shadda_list.append(sh_list)\n",
    "        sukoon_list.append(su_list)\n",
    "    return short_vowels_list,double_case_endings_list,shadda_list,sukoon_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_vowels_list,double_case_endings_list,shadda_list,sukoon_list = getDiacriticsForDataSet(sentences_with_diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', 'ّ', '', '', '', '', '', '', '', '', '', '', '', '', 'ّ', '', '', '', '', 'ّ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ّ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ّ', '', '', 'ّ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ّ', '', '', '', '', '', '', '', '', '', '', 'ّ', '', 'ّ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ّ', '', '', '', '', '', '', '', 'ّ', '', '', '', '', '', '', 'ّ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ّ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ّ', '', '', 'ّ', '', '', '', '', '', '', '', '', '', '', '', 'ّ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ّ', '', '', '', '', '', '', '', '', '', '', '', 'ّ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "# print(len(sentences_with_diacritics[0].split(\" \")))\n",
    "print(len(shadda_list[0]))\n",
    "print(shadda_list[0]) #this shows the diacritics of the first sentence\n",
    "# print(\" \".join(short_vowels_list[0])) #diacritics not in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_lists=getDiacraticsForSentence(sentences_with_diacritics[0])\n",
    "# # print(list_of_lists[0:6])\n",
    "# # flattened_list = [item for sublist in list_of_lists for item in sublist]\n",
    "# # print(flattened_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flattened_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24900\\2084679647.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# trying first with one sentence:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mencoded_sentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mENCODE_LETTERS_LOOKUP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mflattened_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'flattened_list' is not defined"
     ]
    }
   ],
   "source": [
    "# trying first with one sentence:\n",
    "encoded_sentence=[ENCODE_LETTERS_LOOKUP.lookup(tf.constant(char)).numpy() for char in flattened_list[0]]\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 1, 0, 0, 3, 0, 0, 1, 0, 0, 0, 3, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 3, 0, 0, 2, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 3, 0, 1, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 1, 0, 0, 3, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 3, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 3, 0, 0, 1, 0, 3, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 3, 0, 0, 3, 0, 0, 0, 1, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 3, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 3, 0, 0, 3, 0, 0, 0, 1, 0, 1, 0, 3, 0, 0, 1, 0, 1, 0, 0, 0, 3, 0, 0, 1, 0, 3, 0, 1, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 3, 0, 3, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 1, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 1, 0, 3, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 3, 0, 1, 0, 0, 0, 3, 0, 3, 0, 0, 2, 0, 3, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "encoded_sentence=[ENCODE_SHORT_VOWELS_LOOKUP.lookup(tf.constant(char)).numpy() for char in sentences_with_diacritics[0][0]]\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m               encoded_output_sentences\u001b[38;5;241m.\u001b[39mappend(encode_output_sentence_for_sukun_and_shadda(sentence))\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_output_sentences\n\u001b[1;32m---> 42\u001b[0m encoded_input\u001b[38;5;241m=\u001b[39m \u001b[43mencode_input_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_without_diacritics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m encoded_gold_output_for_short_vowels\u001b[38;5;241m=\u001b[39m encode_output_sentences_for_short_vowels(sentences_with_diacritics)\n\u001b[0;32m     44\u001b[0m encoded_gold_output_for_double_case_endings\u001b[38;5;241m=\u001b[39m encode_output_sentences_for_double_case_endings(sentences_with_diacritics)\n",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m, in \u001b[0;36mencode_input_sentences\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m      7\u001b[0m encoded_input_sentences\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m----> 9\u001b[0m       encoded_input_sentences\u001b[38;5;241m.\u001b[39mappend(\u001b[43mencode_input_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_input_sentences\n",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m, in \u001b[0;36mencode_input_sentence\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_input_sentence\u001b[39m(sentence):\n\u001b[1;32m----> 2\u001b[0m     encoded_sentence\u001b[38;5;241m=\u001b[39m\u001b[43m[\u001b[49m\u001b[43mENCODE_LETTERS_LOOKUP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_sentence\n",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_input_sentence\u001b[39m(sentence):\n\u001b[1;32m----> 2\u001b[0m     encoded_sentence\u001b[38;5;241m=\u001b[39m[\u001b[43mENCODE_LETTERS_LOOKUP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m sentence]\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_sentence\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\tensorflow\\python\\ops\\lookup_ops.py:257\u001b[0m, in \u001b[0;36mInitializableLookupTableBase.lookup\u001b[1;34m(self, keys, name)\u001b[0m\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDtype of argument `keys` must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceived: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\n\u001b[0;32m    255\u001b[0m     name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_Lookup\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    256\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresource_handle, key_tensor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_value)):\n\u001b[1;32m--> 257\u001b[0m   values \u001b[38;5;241m=\u001b[39m \u001b[43mgen_lookup_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup_table_find_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mkey_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m                                               \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m values\u001b[38;5;241m.\u001b[39mset_shape(key_tensor\u001b[38;5;241m.\u001b[39mget_shape())\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(keys, sparse_tensor\u001b[38;5;241m.\u001b[39mSparseTensor):\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_lookup_ops.py:993\u001b[0m, in \u001b[0;36mlookup_table_find_v2\u001b[1;34m(table_handle, keys, default_value, name)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    992\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 993\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLookupTableFindV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    996\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def encode_input_sentence(sentence):\n",
    "    encoded_sentence=[ENCODE_LETTERS_LOOKUP.lookup(tf.constant(char)).numpy() for char in sentence]\n",
    "    return encoded_sentence\n",
    "\n",
    "def encode_input_sentences(sentences):\n",
    "        # loop on each sentence and encode it\n",
    "        encoded_input_sentences=[]\n",
    "        for sentence in sentences:\n",
    "              encoded_input_sentences.append(encode_input_sentence(sentence))\n",
    "        return encoded_input_sentences\n",
    "\n",
    "def encode_output_sentence_for_short_vowels(sentence):\n",
    "    encoded_sentence=[ENCODE_SHORT_VOWELS_LOOKUP.lookup(tf.constant(char)).numpy() for char in sentence]\n",
    "    return encoded_sentence\n",
    "\n",
    "def encode_output_sentence_for_double_case_endings(sentence):\n",
    "    encoded_sentence=[ENCODE_DOUBLE_CASE_ENDINGS_LOOKUP.lookup(tf.constant(char)).numpy() for char in sentence]\n",
    "    return encoded_sentence\n",
    "\n",
    "def encode_output_sentence_for_sukun_and_shadda(sentence):\n",
    "    encoded_sentence=[ENCODE_BINARY_LOOKUP.lookup(tf.constant(char)).numpy() for char in sentence]\n",
    "    return encoded_sentence\n",
    "\n",
    "def encode_output_sentences_for_short_vowels(sentences):\n",
    "    encoded_output_sentences=[]\n",
    "    for sentence in sentences:\n",
    "              encoded_output_sentences.append(encode_output_sentence_for_short_vowels(sentence))\n",
    "    return encoded_output_sentences\n",
    "\n",
    "def encode_output_sentences_for_double_case_endings(sentences):\n",
    "    encoded_output_sentences=[]\n",
    "    for sentence in sentences:\n",
    "              encoded_output_sentences.append(encode_output_sentence_for_double_case_endings(sentence))\n",
    "    return encoded_output_sentences\n",
    "\n",
    "def encode_output_sentences_for_sukun_and_shadda(sentences):\n",
    "    encoded_output_sentences=[]\n",
    "    for sentence in sentences:\n",
    "              encoded_output_sentences.append(encode_output_sentence_for_sukun_and_shadda(sentence))\n",
    "    return encoded_output_sentences\n",
    "\n",
    "encoded_input= encode_input_sentences(sentences_without_diacritics)\n",
    "encoded_gold_output_for_short_vowels= encode_output_sentences_for_short_vowels(sentences_with_diacritics)\n",
    "encoded_gold_output_for_double_case_endings= encode_output_sentences_for_double_case_endings(sentences_with_diacritics)\n",
    "encoded_gold_output_for_sukun= encode_output_sentences_for_sukun_and_shadda(sentences_with_diacritics)\n",
    "encoded_gold_output_for_shada= encode_output_sentences_for_sukun_and_shadda(sentences_with_diacritics)\n",
    "\n",
    "print(sentences_with_diacritics[0])\n",
    "print(sentences_without_diacritics[0])\n",
    "print(\"Encoded Input:\", encoded_input[0])\n",
    "print(\"Encoded Gold Output Primary Class:\", encoded_gold_output_for_short_vowels[0])\n",
    "print(\"Encoded Gold Output Sec Class:\", encoded_gold_output_for_double_case_endings[0])\n",
    "print(\"Encoded Gold Output Sukun Class:\", encoded_gold_output_for_sukun[0])\n",
    "print(\"Encoded Gold Output Shadda Class:\", encoded_gold_output_for_shada[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input/output concat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch and input preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
