{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "from constants import *\n",
    "from encoding_decoding_lookup import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diacritizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check steps in model_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just like the lab, but with tf, we would do init with all needed variables and a forward fn\n",
    "# #what we need: self, window_size, lstm_size, dropout_rate,embedding_size\n",
    "# class Diacritizer:\n",
    "#     def __init__(self)\n",
    "#     def __init__(self, vocab_size=35181, embedding_dim=50, hidden_size=50, n_classes=len(tag_map)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure if we need vocab size since we use window\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "\n",
    "class Diacritizer(Model):\n",
    "    def __init__(self, embedding_size=DEFAULT_EMBEDDING_SIZE, lstm_size=DEFAULT_LSTM_SIZE, window_size=DEFAULT_WINDOW_SIZE,dropout_rate=DEFAULT_DROPOUT_RATE):\n",
    "        super(Diacritizer, self).__init__()\n",
    "\n",
    "        # in the initialization, we define: input, embeddings, and output\n",
    "        # in lab 5 we had a linear where number of neurons= number of classes but here we have dense\n",
    "\n",
    "        ######################################## Creating the layers of our model #####################\n",
    "\n",
    "        # Step 1: define the input\n",
    "        # input would be the fixed window size after lookup \n",
    "        # Input in tf defines the input layer with shape=(window_size,)\n",
    "        # it represents the input data.\n",
    "        self.inputs = Input(shape=(window_size,), name='input')\n",
    "\n",
    "\n",
    "        # Step 2: define the embeddings\n",
    "        # it converts the integer index to dense vectors with fixed size\n",
    "        # input dim is dim of the letters (length of valid input letters we have)\n",
    "        # the output is the dense vector (embeddings size which is 128)\n",
    "        # the ( ) at the end mean that this layer would be applied to the input layer\n",
    "        self.embedding = Embedding(input_dim= len(SORTED_VALID_INPUT_LETTERS) + 1, output_dim= embedding_size, name='embedding')(self.inputs)\n",
    "        \n",
    "\n",
    "        # Step 3: Define the Bidirectional LSTM layers (we have 4 layers of each class and an initial layer)\n",
    "        \n",
    "        # initial layer that would be applied to the embeddings\n",
    "        self.initial_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                      name='initial_layer')(self.embedding)\n",
    "        \n",
    "        # first layer is sukun layer that would be applied on initial layer\n",
    "        self.sukoon_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                     name='sukoon_layer')(self.initial_layer)\n",
    "        \n",
    "        # sec layer is shadda layer that would be applied to sukun layer\n",
    "        self.shadda_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                     name='shadda_layer')(self.sukoon_layer)\n",
    "        \n",
    "        # third layer is sec layer that would be applied to shadda layer\n",
    "        self.secondary_diacritics_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                                   name='secondary_diacritics_layer')(self.shadda_layer)\n",
    "        \n",
    "        # forth layer is primary layer that would be applied to sec layer\n",
    "        self.primary_diacritics_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                                 name='primary_diacritics_layer')(self.secondary_diacritics_layer)\n",
    "\n",
    "        # Step 4: Define the output layers (we have 4 outputs with diff dense)\n",
    "        # dense functions define the activation functions in tensor flow\n",
    "        # where dense=1 means sigmoid and dense =2 means softmax\n",
    "\n",
    "        # Sukun output would have a sigmoid as its binary and would come from the sukun layer\n",
    "        # dense= 1 means single neuorn we just wanna know if 0 no sukun or 1 yes sukun\n",
    "        # SIGMOID\n",
    "        self.sukoon_output = Dense(1, activation='sigmoid',name='sukoon_output')(self.sukoon_layer)\n",
    "\n",
    "        # shadda output\n",
    "        # SIGMOID\n",
    "        self.shadda_output = Dense(1,activation='sigmoid',   name='shadda_output')(self.shadda_layer)\n",
    "\n",
    "        # sec output\n",
    "        # dense= 4 as we have multi classification task, where each neuron assigned to one class\n",
    "        # we have 0,1,2,3 as per the lookup table\n",
    "        # SOFTMAX\n",
    "        self.secondary_diacritics_output = Dense(4, activation='softmax' , name='secondary_diacritics_output')(self.secondary_diacritics_layer)\n",
    "\n",
    "        # prim output\n",
    "        # SOFTMAX\n",
    "        self.primary_diacritics_output = Dense(4,  activation='softmax' ,name='primary_diacritics_output')(self.primary_diacritics_layer)\n",
    "       \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Step 4: Define the forward pass through the layers\n",
    "        initial_layer = self.initial_layer(inputs)\n",
    "        sukoon_layer = self.sukoon_layer(initial_layer)\n",
    "        shadda_layer = self.shadda_layer(sukoon_layer)\n",
    "        secondary_diacritics_layer = self.secondary_diacritics_layer(shadda_layer)\n",
    "        primary_diacritics_layer = self.primary_diacritics_layer(secondary_diacritics_layer)\n",
    "\n",
    "\n",
    "        # Step 5: Separate outputs for each diacritic\n",
    "        sukoon_output = self.sukoon_output(sukoon_layer)\n",
    "        shadda_output = self.shadda_output(shadda_layer)\n",
    "        secondary_diacritics_output = self.secondary_diacritics_output(secondary_diacritics_layer)\n",
    "        primary_diacritics_output = self.primary_diacritics_output(primary_diacritics_layer)\n",
    "       \n",
    "\n",
    "        return primary_diacritics_output, secondary_diacritics_output, shadda_output, sukoon_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "<__main__.Diacritizer object at 0x0000012D96351D10>\n"
     ]
    }
   ],
   "source": [
    "model = Diacritizer()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input/output preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input/output concat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch and input preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
