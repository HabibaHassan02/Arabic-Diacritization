{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "from constants import *\n",
    "from encoding_decoding_lookup import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diacritizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training we need to follow these steps:\n",
    "'''\n",
    "1. input is from lookup table all sentences concatenated together\n",
    "2. For variable length input we need sliding window and also it helps to get the contextual meaning due to overlapping\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just like the lab, but with tf, we would do init with all needed variables and a forward fn\n",
    "# #what we need: self, window_size, lstm_size, dropout_rate,embedding_size\n",
    "# class Diacritizer:\n",
    "#     def __init__(self)\n",
    "#     def __init__(self, vocab_size=35181, embedding_dim=50, hidden_size=50, n_classes=len(tag_map)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure if we need vocab size since we use window\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "\n",
    "class Diacritizer(Model):\n",
    "    def __init__(self, embedding_size=DEFAULT_EMBEDDING_SIZE, lstm_size=DEFAULT_LSTM_SIZE, window_size=DEFAULT_WINDOW_SIZE,dropout_rate=DEFAULT_DROPOUT_RATE):\n",
    "        super(Diacritizer, self).__init__()\n",
    "\n",
    "        ######################################## Creating the layers of our model #####################\n",
    "\n",
    "        # Step 1: define the input\n",
    "        # input would be the fixed window size after lookup \n",
    "        # Input in tf defines the input layer with shape=(window_size,)\n",
    "        # it represents the input data.\n",
    "        self.inputs = Input(shape=(window_size,), name='input')\n",
    "\n",
    "\n",
    "        # Step 2: define the embeddings\n",
    "        # it converts the integer index to dense vectors with fixed size\n",
    "        # input dim is dim of the letters (length of valid input letters we have)\n",
    "        # the output is the dense vector (embeddings size which is 128)\n",
    "        # the ( ) at the end mean that this layer would be applied to the input layer\n",
    "        self.embedding = Embedding(input_dim= len(SORTED_VALID_INPUT_LETTERS) + 1, output_dim= embedding_size, name='embedding')(self.inputs)\n",
    "        \n",
    "\n",
    "        # Step 3: Define the Bidirectional LSTM layers (we have 4 layers of each class and an initial layer)\n",
    "        \n",
    "        # initial layer that would be applied to the embeddings\n",
    "        self.initial_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                      name='initial_layer')(self.embedding)\n",
    "        \n",
    "        # first layer is sukun layer that would be applied on initial layer\n",
    "        self.sukoon_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                     name='sukoon_layer')(self.initial_layer)\n",
    "        \n",
    "        # sec layer is shadda layer that would be applied to sukun layer\n",
    "        self.shadda_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                     name='shadda_layer')(self.sukoon_layer)\n",
    "        \n",
    "        # third layer is sec layer that would be applied to shadda layer\n",
    "        self.secondary_diacritics_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                                   name='secondary_diacritics_layer')(self.shadda_layer)\n",
    "        \n",
    "        # forth layer is primary layer that would be applied to sec layer\n",
    "        self.primary_diacritics_layer = Bidirectional(LSTM(lstm_size, dropout=dropout_rate, return_sequences=True),\n",
    "                                                 name='primary_diacritics_layer')(self.secondary_diacritics_layer)\n",
    "\n",
    "        # Step 4: Define the output layers (we have 4 outputs with diff dense)\n",
    "        # dense functions define the activation functions in tensor flow\n",
    "        # where dense=1 means sigmoid and dense =2 means softmax\n",
    "\n",
    "        # Sukun output would have a sigmoid as its binary and would come from the sukun layer\n",
    "        # dense= 1 means single neuorn we just wanna know if 0 no sukun or 1 yes sukun\n",
    "        # SIGMOID\n",
    "        self.sukoon_output = Dense(1, name='sukoon_output')(self.sukoon_layer)\n",
    "\n",
    "        # shadda output\n",
    "        # SIGMOID\n",
    "        self.shadda_output = Dense(1, name='shadda_output')(self.shadda_layer)\n",
    "\n",
    "        # sec output\n",
    "        # dense= 4 as we have multi classification task, where each neuron assigned to one class\n",
    "        # we have 0,1,2,3 as per the lookup table\n",
    "        # SOFTMAX\n",
    "        self.secondary_diacritics_output = Dense(4, name='secondary_diacritics_output')(self.secondary_diacritics_layer)\n",
    "\n",
    "        # prim output\n",
    "        # SOFTMAX\n",
    "        self.primary_diacritics_output = Dense(4, name='primary_diacritics_output')(self.primary_diacritics_layer)\n",
    "       \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Step 4: Define the forward pass through the layers\n",
    "       \n",
    "\n",
    "        # Step 5: Separate outputs for each diacritic\n",
    "       \n",
    "\n",
    "        return primary_output, secondary_output, shadda_output, sukoon_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
