{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pyarabic.araby as araby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "18515271\n",
      "ق\n"
     ]
    }
   ],
   "source": [
    "arabicDictionary=['ى', 'ع', 'ظ', 'ح', 'ر', 'س', 'ي', 'ش', 'ض', 'ق', ' ', 'ث', 'ل', 'ص', 'ط', 'ك', 'آ', 'م', 'ا', 'إ', 'ه', 'ز', 'ء', 'أ', 'ف', 'ؤ', 'غ', 'ج', 'ئ', 'د', 'ة', 'خ', 'و', 'ب', 'ذ', 'ت', 'ن']\n",
    "punctuations = [\"،\", \":\", \"؛\", \"-\", \"؟\"]\n",
    "#reading the training dataset\n",
    "f = open(r\"train.txt\", \"r\",encoding=\"utf-8\").read()\n",
    "print(type(f))\n",
    "print(len(f))\n",
    "print(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataFromNonArabicLetters(data):\n",
    "    #regex to keep arabic letters only and remove any other character (eg. brackets, numbers ...etc)\n",
    "    characters_regex =r'[\\s\\.\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD]+'\n",
    "    processedData = re.findall(characters_regex,data)\n",
    "    processedData = \" \".join(processedData)\n",
    "    processedData = re.sub(r\"\\s+\",\" \" ,processedData) #substitute many spaces with one space only\n",
    "    return processedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17997403\n",
      "َ\n"
     ]
    }
   ],
   "source": [
    "processedData=cleanDataFromNonArabicLetters(f)\n",
    "print(len(processedData))\n",
    "print(processedData[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDiacratics(processedData):\n",
    "    without_diacritics= araby.strip_diacritics(processedData)\n",
    "    return without_diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10711561\n",
      "و\n"
     ]
    }
   ],
   "source": [
    "without_diacritics = removeDiacratics(processedData)\n",
    "print(len(without_diacritics))\n",
    "print(without_diacritics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the procrssed text and separate it into sentences\n",
    "def generateListsWithDiacraticsandWithout(with_diacratics, without_diacritics):\n",
    "    #---------------------Preprocessing of words with diacratics------------\n",
    "\n",
    "    #generate longStringSplited which is the list of procrssed text without brackets and numbers and dots \n",
    "    #the rest of the punctuation still there\n",
    "    longStringWithDecimalPoint=re.sub(r\"\\n\", ' ', with_diacratics) #string of data with diacratics removed endlines from it\n",
    "    for element in punctuations:\n",
    "        longStringWithDecimalPoint=re.sub(element, '', longStringWithDecimalPoint) #remove punctuations from the string\n",
    "\n",
    "    longStringSplited=longStringWithDecimalPoint.split('.') #split the string by dots (segment sentences) and make list of them\n",
    "    longString=' '.join(longStringSplited)  #long string without decimal points\n",
    "\n",
    "    #-------------------------------------\n",
    "\n",
    "    #---------------------Preprocessing of words without diacratics------------\n",
    "    without_diacratics_longStringWithDecimals=re.sub(r\"\\n\", ' ', without_diacritics) #string of data without diacratics removed endlines from it\n",
    "    for element in punctuations:\n",
    "        without_diacratics_longStringWithDecimals=re.sub(element, '', without_diacratics_longStringWithDecimals) #remove punctuations from the string\n",
    "\n",
    "    longStringSplited_withoutDiacratics=without_diacratics_longStringWithDecimals.split('.') #split the string by dots (segment sentences) and make list of them\n",
    "    longString_withoutDiacratics=' '.join(longStringSplited_withoutDiacratics)  #long string without decimal points\n",
    "\n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "    list_of_sentences=[]\n",
    "    for line in longStringSplited_withoutDiacratics: #list of lists of sentences splitted in words without diacratics (used in embeddings)\n",
    "        list_of_sentences.append(line.split(\" \"))\n",
    "    \n",
    "\n",
    "    # now the variable called longString has a single string with all the processed words in it \n",
    "    listOfwordsWith_Diacritics=list()\n",
    "    listOfwordsWith_NoDiacritics=list()\n",
    "\n",
    "    listOfwordsWith_Diacritics=re.sub(r\"\\s+\", ' ', longString)\n",
    "    listOfwordsWith_Diacritics=listOfwordsWith_Diacritics.split(\" \")\n",
    "\n",
    "    listOfwordsWith_NoDiacritics=re.sub(r\"\\s+\", ' ', longString_withoutDiacratics)\n",
    "    listOfwordsWith_NoDiacritics=listOfwordsWith_NoDiacritics.split(\" \")\n",
    "\n",
    "    return list_of_sentences,listOfwordsWith_Diacritics,listOfwordsWith_NoDiacritics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences,listOfwordsWith_Diacritics,listOfwordsWith_NoDiacritics= generateListsWithDiacraticsandWithout(processedData, without_diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40836\n",
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة', 'قوله', 'بلفظ', 'يقتضيه', 'كإنكار', 'غير', 'حديث', 'بالإسلام', 'وجوب', 'ما', 'علم', 'وجوبه', 'من', 'الدين', 'ضرورة', 'كإلقاء', 'مصحف', 'بقذر', 'وشد', 'زنار', 'ابن', 'عرفة', 'قول', 'ابن', 'شاس', 'أو', 'بفعل', 'يتضمنه', 'هو', 'كلبس', 'الزنار', 'وإلقاء', 'المصحف', 'في', 'صريح', 'النجاسة', 'والسجود', 'للصنم', 'ونحو', 'ذلك', 'وسحر', 'محمد', 'قول', 'مالك', 'وأصحابه', 'أن', 'الساحر', 'كافر', 'بالله', 'تعالى', 'قال', 'مالك', 'هو', 'كالزنديق', 'إذا', 'عمل', 'السحر', 'بنفسه', 'قتل', 'ولم', 'يستتب', '']\n",
      "2102054\n",
      "2102054\n",
      "['أَوْ', 'سَيِّدُهَا', 'فِي', 'ذَلِكَ', 'جَازَ', 'لِأَنَّ', 'لَهُ', 'غَرَضًا', 'فِي', 'تَزَيُّنِهَا', 'لَهُ', 'كَمَا', 'فِي', 'الرَّوْضَةِ', 'وَهُوَ', 'الْأَوْجَهُ', 'وَإِنْ', 'جَرَى', 'فِي', 'التَّحْقِيقِ', 'عَلَى', 'خِلَافِ', 'ذَلِكَ', 'فِي', 'الْوَصْلِ', 'وَالْوَشْرِ', 'فَأَلْحَقَهُمَا', 'بِالْوَشْمِ', 'فِي', 'الْمَنْعِ', 'مُطْلَقًا', 'وَيُكْرَهُ', 'أَنْ', 'يَنْتِفَ', 'الشَّيْبَ', 'مِنْ', 'الْمَحَلِّ', 'الَّذِي', 'لَا', 'يُطْلَبُ', 'مِنْهُ', 'إزَالَةُ', 'شَعْرِهِ', 'وَيُسَنُّ', 'خَضْبُهُ', 'بِالْحِنَّاءِ', 'وَنَحْوِهِ', 'وَيُسَنُّ', 'لِلْمَرْأَةِ', 'الْمُزَوَّجَةِ', 'وَالْمَمْلُوكَةِ', 'خَضْبُ', 'كَفِّهَا', 'وَقَدَمِهَا', 'بِذَلِكَ', 'تَعْمِيمًا', 'لِأَنَّهُ', 'زِينَةٌ', 'وَهِيَ', 'مَطْلُوبَةٌ', 'مِنْهَا', 'لِحَلِيلِهَا', 'أَمَّا', 'النَّقْشُ', 'وَالتَّطْرِيفُ', 'فَلَا', 'يُسَنُّ', 'وَخَرَجَ', 'بِالْمُزَوَّجَةِ', 'وَالْمَمْلُوكَةِ', 'غَيْرُهُمَا', 'فَيُكْرَهُ', 'لَهُ', 'وَبِالْمَرْأَةِ', 'الرَّجُلُ', 'وَالْخُنْثَى', 'فَيَحْرُمُ', 'الْخِضَابُ', 'عَلَيْهِمَا', 'إلَّا', 'لِعُذْرٍ', 'نِهَايَةٌ', 'وَمُغْنِي', 'قَالَ', 'ع', 'ش', 'قَوْلُهُ', 'م', 'ر', 'وَيَحْرُمُ', 'عَلَى', 'الْمَرْأَةِ', 'خَرَجَ', 'بِالْمَرْأَةِ', 'غَيْرُهَا', 'مِنْ', 'ذَكَرٍ', 'وَأُنْثَى', 'صَغِيرَيْنِ', 'فَيَجُوزُ']\n",
      "['أو', 'سيدها', 'في', 'ذلك', 'جاز', 'لأن', 'له', 'غرضا', 'في', 'تزينها', 'له', 'كما', 'في', 'الروضة', 'وهو', 'الأوجه', 'وإن', 'جرى', 'في', 'التحقيق', 'على', 'خلاف', 'ذلك', 'في', 'الوصل', 'والوشر', 'فألحقهما', 'بالوشم', 'في', 'المنع', 'مطلقا', 'ويكره', 'أن', 'ينتف', 'الشيب', 'من', 'المحل', 'الذي', 'لا', 'يطلب', 'منه', 'إزالة', 'شعره', 'ويسن', 'خضبه', 'بالحناء', 'ونحوه', 'ويسن', 'للمرأة', 'المزوجة', 'والمملوكة', 'خضب', 'كفها', 'وقدمها', 'بذلك', 'تعميما', 'لأنه', 'زينة', 'وهي', 'مطلوبة', 'منها', 'لحليلها', 'أما', 'النقش', 'والتطريف', 'فلا', 'يسن', 'وخرج', 'بالمزوجة', 'والمملوكة', 'غيرهما', 'فيكره', 'له', 'وبالمرأة', 'الرجل', 'والخنثى', 'فيحرم', 'الخضاب', 'عليهما', 'إلا', 'لعذر', 'نهاية', 'ومغني', 'قال', 'ع', 'ش', 'قوله', 'م', 'ر', 'ويحرم', 'على', 'المرأة', 'خرج', 'بالمرأة', 'غيرها', 'من', 'ذكر', 'وأنثى', 'صغيرين', 'فيجوز']\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_sentences))\n",
    "print(list_of_sentences[0])\n",
    "print(len(listOfwordsWith_NoDiacritics))\n",
    "print(len(listOfwordsWith_Diacritics))\n",
    "print(listOfwordsWith_Diacritics[2101000:2101100])\n",
    "print(listOfwordsWith_NoDiacritics[2101000:2101100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDiacraticsList(listOfwordsWith_Diacritics):\n",
    "    #now that we have two separated lists we need to get the diacritics list \n",
    "\n",
    "\n",
    "    listofDiacritrcs_ToWord=list()\n",
    "    temp=list()\n",
    "    counter=0\n",
    "    for word in listOfwordsWith_Diacritics:\n",
    "        while counter<len(word):\n",
    "            if word[counter] in arabicDictionary: #checking if the character is a letter\n",
    "                if (counter+1)<len(word):\n",
    "                    #checking if the next character is also a letter, then that means that the diacritics of the current letter is none so add empty string to the list\n",
    "                    if word[counter +1] in arabicDictionary: \n",
    "                        temp.append(\"\")\n",
    "                        counter+=1\n",
    "                        continue\n",
    "                counter+=1 #if it is the end of the word (no more letters) or the next character is a diacritics -> continue looping\n",
    "                continue\n",
    "            else:\n",
    "                if (counter+1)<len(word):\n",
    "                    if word[(counter+1)] not in arabicDictionary: #if the current and the next characters are diacritics, add them together in the list\n",
    "                        temp.append(word[counter]+word[counter+1])\n",
    "                        counter+=2\n",
    "                        continue\n",
    "                temp.append(word[counter]) #if the current character only is the diacritics add it to the list\n",
    "                counter+=1    \n",
    "        listofDiacritrcs_ToWord.append(temp.copy())     \n",
    "        temp.clear() \n",
    "        counter=0\n",
    "    return listofDiacritrcs_ToWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofDiacritrcs_ToWord= generateDiacraticsList(listOfwordsWith_Diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2102054\n",
      "الْأَوَّلُ\n",
      "الأول\n",
      "['', 'ْ', 'َ', 'َّ', 'ُ']\n",
      " ْ َ َّ ُ\n",
      "[['َ', 'ْ', 'ُ', 'ُ'], ['َ', 'ْ'], ['َ', 'َ', 'َ'], ['', 'ْ', 'َ', 'َّ', 'ُ'], ['َ', 'َ', 'ُ'], ['', 'َ', 'ْ'], ['َ', '', 'َ'], ['', '', 'َّ', 'ْ', 'َ', 'ِ', 'ُّ'], ['', 'ْ', 'ُ'], ['َ', 'َ', 'َ', 'َ'], ['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'ْ', 'ٍ'], ['َ', 'ْ', 'َ', 'ِ', ''], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['َ', 'ْ', 'ِ'], ['َ', 'ِ', '', 'ٍ'], ['ِ', '', 'ْ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['ُ', 'ُ', '', 'َ'], ['َ'], ['ُ', 'ِ', 'َ'], ['ُ', 'ُ', '', 'ُ', 'ُ'], ['ِ', 'ْ'], ['', '', 'ِّ', '', 'ِ'], ['َ', 'ُ', '', 'َ', 'ً'], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['ُ', 'ْ', 'َ', 'ٍ'], ['ِ', 'َ', 'َ', 'ٍ'], ['َ', 'َ', 'ِّ'], ['ُ', 'َّ', '', 'ٍ'], ['', 'ْ', 'ُ'], ['َ', 'َ', 'َ', 'َ'], ['َ', 'ْ', 'ُ'], ['', 'ْ', 'ِ'], ['َ', '', 'ٍ'], ['َ', 'ْ'], ['ِ', 'ِ', 'ْ', 'ٍ'], ['َ', 'َ', 'َ', 'َّ', 'ُ', 'ُ'], ['ُ', 'َ'], ['َ', 'ُ', 'ْ', 'ِ'], ['', '', 'ُّ', 'َّ', '', 'ِ'], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['', 'ْ', 'ُ', 'ْ', 'َ', 'ِ'], ['ِ'], ['َ', 'ِ', '', 'ِ'], ['', '', 'َّ', 'َ', '', 'َ', 'ِ'], ['َ', '', '', 'ُّ', 'ُ', '', 'ِ'], ['ِ', '', 'َّ', 'َ', 'ِ'], ['َ', 'َ', 'ْ', 'ِ'], ['َ', 'ِ', 'َ'], ['َ', 'ِ', 'ْ', 'ٍ'], ['ُ', 'َ', 'َّ', 'ٌ'], ['َ', 'ْ', 'ُ'], ['َ', '', 'ِ', 'ٍ'], ['َ', 'َ', 'ْ', 'َ', '', 'ِ', 'ِ'], ['َ', 'َّ'], ['', '', 'َّ', '', 'ِ', 'َ'], ['َ', '', 'ِ', 'ٌ'], ['ِ', 'َ', '', 'َّ', 'ِ'], ['َ', 'َ', '', 'َ'], ['َ', '', 'َ'], ['َ', '', 'ِ', 'ٌ'], ['ُ', 'َ'], ['َ', '', '', 'ِّ', 'ْ', 'ِ', '', 'ِ'], ['', 'َ'], ['َ', 'ِ', 'َ'], ['', '', 'ِّ', 'ْ', 'َ'], ['ِ', 'َ', 'ْ', 'ِ', 'ِ'], ['ُ', 'ِ', 'َ'], ['َ', 'َ', 'ْ'], ['ُ', 'ْ', 'َ', 'َ', 'ْ'], ['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'َ', 'ِ'], ['َ'], ['َ', 'َ', 'َ', 'َّ', 'ُ'], ['', 'َ', 'ْ'], ['َ', 'ْ'], ['', 'ْ', 'َ', 'ِ', 'َّ', 'ُ'], ['َ', 'ْ', 'ُ', 'ُ'], ['َ'], ['َ', 'َّ'], ['َ', 'ْ'], ['ُ', 'َ', 'ْ', 'َ'], ['َ', 'ْ', 'ِ'], ['', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', 'ْ'], ['َ', 'َ', 'ْ'], ['', 'ْ', 'َ', 'َ', 'َ'], ['َ', 'َ'], ['َ', 'ْ', 'َ', 'ْ'], ['َ', 'ُ'], ['ِ', 'َ', '', 'ٍ'], ['َ', 'ْ'], ['َ', 'ْ', 'ُ', '', 'ُ'], ['َ', '', 'ً'], ['َ', 'َ'], ['َ', 'َ', 'َ'], ['َ', 'ُ'], ['ِ', 'ْ', 'َ'], ['', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'ْ']]\n"
     ]
    }
   ],
   "source": [
    "print(len(listofDiacritrcs_ToWord))\n",
    "print(listOfwordsWith_Diacritics[3])\n",
    "print(listOfwordsWith_NoDiacritics[3])\n",
    "print(listofDiacritrcs_ToWord[3])\n",
    "print(\" \".join(listofDiacritrcs_ToWord[3]))\n",
    "print(listofDiacritrcs_ToWord[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import  TFBertModel, BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قوله أو قطع الأول يده إلخ قال الزركشي ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر وشد زنار ابن عرفة قول ابن شاس أو بفعل يتضمنه هو كلبس الزنار وإلقاء المصحف في صريح النجاسة والسجود للصنم ونحو ذلك وسحر محمد قول مالك وأصحابه أن الساحر كافر بالله تعالى قال مالك هو كالزنديق إذا عمل السحر بنفسه قتل ولم يستتب \n",
      " قوله لعدم ما تتعلق إلخ أي الوصية قوله ما مر أي قبيل قول المتن لغت ولو اقتصر على أوصيت له بشاة أو أعطوه شاة ولا غنم له عند الموت هل تبطل الوصية أو يشترى له شاة ويؤخذ من قوله الآتي كما لو لم يقل من مالي ولا من غنمي أنها لا تبطل  وعبارة الكنز ولو لم يقل من مالي ولا من غنمي لم يتعين غنمه إن كانت انتهت ا ه سم قوله فيعطى واحدة منها إلخ كما لو كانت موجودة عند الوصية والموت  ولا يجوز أن يعطى واحدة من غير غنمه في الصورتين وإن تراضيا  لأنه صلح على مجهول مغني ونهاية قال ع ش قوله واحدة منها أي كاملة  ولا يجوز أن يعطى نصفين من شاتين  لأنه لا يسمى شاة وقوله ولا يجوز أن يعطى واحدة من غير غنمه وينبغي أن يقال مثل ذلك في الأرقاء ا ه \n",
      "40836\n"
     ]
    }
   ],
   "source": [
    "list_of_string_sentences=[]\n",
    "for i,sentence in enumerate(list_of_sentences):\n",
    "    list_of_words = list_of_sentences[i]\n",
    "    sentence_as_string = \" \".join(list_of_words)\n",
    "    list_of_string_sentences.append(sentence_as_string)\n",
    "print(list_of_string_sentences[0])\n",
    "print(list_of_string_sentences[1])\n",
    "print(len(list_of_string_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Arabert tokenizer and model\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config= AutoConfig.from_pretrained(model_name,output_hidden_states= True)\n",
    "model = AutoModel.from_pretrained(model_name, )\n",
    "\n",
    "# we dont need to do the following because we are not training, we are using it in inference mode\n",
    "\n",
    "# # Load pre-trained model (weights)\n",
    "# model = BertModel.from_pretrained('bert-base-uncased',\n",
    "#                                   output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "#                                   )\n",
    "# # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and obtain embeddings for each sentence\n",
    "sentence_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('input_ids', [[2, 78, 81, 25153, 217, 82, 53786, 26306, 55494, 5954, 179, 11388, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 78, 92, 31335, 24832, 74, 6350, 85, 40211, 266, 177, 18, 78, 70, 250, 88, 219, 77, 17654, 211, 35227, 3], [2, 78, 81, 25153, 217, 73, 219, 11388, 94, 20042, 177, 38831, 12971, 82, 5732, 179, 11388, 3, 0, 0, 0, 0], [2, 46, 92, 31335, 24832, 76, 7957, 180, 218, 29603, 87, 35667, 83, 21419, 3, 0, 0, 0, 0, 0, 0, 0]]), ('token_type_ids', [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), ('attention_mask', [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])])\n",
      "1- input_ids: [[2, 78, 81, 25153, 217, 82, 53786, 26306, 55494, 5954, 179, 11388, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 78, 92, 31335, 24832, 74, 6350, 85, 40211, 266, 177, 18, 78, 70, 250, 88, 219, 77, 17654, 211, 35227, 3], [2, 78, 81, 25153, 217, 73, 219, 11388, 94, 20042, 177, 38831, 12971, 82, 5732, 179, 11388, 3, 0, 0, 0, 0], [2, 46, 92, 31335, 24832, 76, 7957, 180, 218, 29603, 87, 35667, 83, 21419, 3, 0, 0, 0, 0, 0, 0, 0]]\n",
      "1- token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "1- attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]\n",
      "---------------------------------------\n",
      "1- input_ids: [[2, 60, 210, 219, 92, 14000, 57, 4854, 254, 21268, 55439, 36, 3, 42038, 24505, 4854, 254, 42, 180, 264, 25927, 21268, 55439, 80, 179, 21419, 179, 70, 223, 57, 4854, 254, 21268, 55439, 44764, 70, 75, 5960, 221, 8977, 6403, 50959, 5954, 55435, 4048, 17323, 38, 51244, 179, 33787, 203, 223, 3], [2, 60, 210, 219, 44764, 50, 45026, 5037, 30359, 30756, 177, 177, 3, 38025, 217, 44764, 70, 81, 12203, 73, 12067, 35031, 5037, 80, 7880, 221, 254, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "1- token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "1- attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "---------------------------------------\n",
      "1- input_ids: [[2, 60, 210, 219, 92, 14000, 57, 4854, 254, 21268, 55439, 36, 3], [2, 60, 210, 219, 44764, 50, 45026, 5037, 30359, 30756, 177, 177, 3]]\n",
      "1- token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "1- attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# #trial cell:\n",
    "# trial_sentences=['i love machine learning','i want to eat pizza, i am so hungry', 'i love doing yoga at the morning', 'I want to graduate right now']\n",
    "\n",
    "# encoding= tokenizer(trial_sentences, padding=True)\n",
    "# # trial_tokens=tokenizer.batch_encode_plus([trial_sentences],padding=True)\n",
    "# print(encoding.items())\n",
    "\n",
    "# for key, value in encoding.items():\n",
    "#     print('1- {}: {}'.format(key, value))\n",
    "\n",
    "\n",
    "# print(\"---------------------------------------\")\n",
    "# q1 = 'Who was Tony Stark?'\n",
    "# q2= 'Who is Mahinour Alaa'\n",
    "# c1 = 'Anthony Edward Stark known as Tony Stark is a fictional character in Avengers'\n",
    "# c2='She is a lil dolphin kitty'\n",
    "\n",
    "\n",
    "# encoding2 = tokenizer([q1,q2], [c1,c2],padding=True)\n",
    "\n",
    "# encoding3=  tokenizer([q1,q2],padding=True)\n",
    "# for key, value in encoding2.items():\n",
    "#     print('1- {}: {}'.format(key, value))\n",
    "\n",
    "# print(\"---------------------------------------\")\n",
    "# for key, value in encoding3.items():\n",
    "#     print('1- {}: {}'.format(key, value))\n",
    "\n",
    "# #from here we notice that segments are only if we have diff things or classifications\n",
    "# #OR if we have DIFFERENT BATCHES\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate through sentences\n",
    "# max=0\n",
    "# for sentence in list_of_string_sentences:\n",
    "\n",
    "#     # Tokenize the sentence\n",
    "#     #This line does the same thing as iterating through each and adding the SEP and CLS tokens\n",
    "#     tokens = tokenizer.encode_plus(sentence, return_tensors=\"pt\", add_special_tokens=True, max_length=512, padding=\"max_length\",truncation=False)\n",
    "   \n",
    "\n",
    "#    # Check that the tokens are all the same length\n",
    "#     if len(tokens['input_ids'][0])>512:\n",
    "#         if(len(tokens['input_ids'][0])>max):\n",
    "#             max=len(tokens['input_ids'][0])\n",
    "\n",
    "#     ''' \n",
    "#     tokens['input_ids'][0]\n",
    "#     tensor([    2,  4151,   440,  4377,   810, 11035, 22323,   754, 19567, 19593,\n",
    "#           193,  2298, 25425,  4151, 33729,   231, 24607,   195, 15527, 12978,\n",
    "#           304,   650,  2643, 30771, 14513,   394,  1805, 14513,   195,   306,\n",
    "#          1220,  2096, 15527, 10648, 24950,   198,  1211,  1871, 41872, 26823,\n",
    "#           304,  2298, 25425,  5316,  2298, 18644,   440,  8842, 51492,   583,\n",
    "#         30897,   213, 43496,   183, 29583, 28919,   305, 22300, 12007,   438,\n",
    "#          1573,  6000,   196,  3373, 13361, 19042,   563, 56110,   582,  5316,\n",
    "#          8210, 11674,   195,   331, 33948, 59474, 10515,  5698,   754,  8210,\n",
    "#           583,  2818,  2126,  4299,  1217,   733, 22413,  8970,  2222,  1174,\n",
    "#           885, 13655,     3,     0,     0,     0,     0,     0,     0,     0,\n",
    "#             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "#             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "#             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "# ...\n",
    "#             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "#             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "#             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "#             0,     0])\n",
    "#     '''\n",
    "#     '''\n",
    "#     return_tensors=\"pt\": This parameter specifies that the output should be PyTorch tensors. \n",
    "#     The option \"pt\" stands for PyTorch. It returns the tokenized inputs as PyTorch tensors.\n",
    "#     add_special_tokens=True: This parameter indicates that special tokens like [CLS] (classification), [SEP] (separator), and [PAD] (padding) should be added to the tokenized sequence. \n",
    "#     These tokens are important for the model to understand the structure and context of the input.\n",
    "#     '''\n",
    "\n",
    "\n",
    "#     #There is a problem now that some tokens can exceed max limit (512)\n",
    "\n",
    "#     # Forward pass through the model to get embeddings\n",
    "# #     with torch.no_grad():\n",
    "# #         outputs = model(**tokens)\n",
    "\n",
    "# #     # Extract embeddings from the last hidden states\n",
    "# #     last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# #     # You might want to use the [CLS] token embedding as the sentence embedding\n",
    "# #     sentence_embedding = last_hidden_states[:, 0, :].squeeze().numpy()\n",
    "    \n",
    "# #     # Append the sentence embedding to the list\n",
    "# #     sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "# # # Convert the list of embeddings to a numpy array\n",
    "# # sentence_embeddings = np.array(sentence_embeddings)\n",
    "\n",
    "# # # Now, sentence_embeddings contains the Arabert embeddings for each sentence\n",
    "# # print(sentence_embeddings.shape)\n",
    "    \n",
    "# print(max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #From Research i noticed that truncation=True would do batches so it doesnt exceed length, no data islost\n",
    "\n",
    "\n",
    "# tokens= tokenizer(list_of_string_sentences, return_tensors=\"pt\", add_special_tokens=True, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# for key, value in tokens.items():\n",
    "#         print('1- {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nUnderstanding the Output\\nhidden_states has four dimensions, in the following order:\\n\\nThe layer number (13 layers) : 13 because the first element is the input embeddings, the rest is the outputs of each of BERT’s 12 layers.\\nThe batch number (1 sentence)\\nThe word / token number (22 tokens in our sentence)\\nThe hidden unit / feature number (768 features)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Understanding the Output\n",
    "hidden_states has four dimensions, in the following order:\n",
    "\n",
    "The layer number (13 layers) : 13 because the first element is the input embeddings, the rest is the outputs of each of BERT’s 12 layers.\n",
    "The batch number (1 sentence)\n",
    "The word / token number (22 tokens in our sentence)\n",
    "The hidden unit / feature number (768 features)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 500 \n",
    "# for i in range(0, len(list_of_string_sentences), batch_size):\n",
    "#     batch_sentences = list_of_string_sentences[i:i + batch_size]\n",
    "#     tokens= tokenizer(batch_sentences, return_tensors=\"pt\", add_special_tokens=True, padding=\"max_length\", truncation=True)\n",
    "\n",
    "#     # Now, proceed with the embedding step for each batch\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**tokens)\n",
    "    \n",
    "#     # Extract embeddings from the last hidden states\n",
    "#     last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "#     #print(last_hidden_states.shape) #torch.Size([batch size, seq length, hidden units size(768)])\n",
    "\n",
    "#     sentence_embeddings.append(last_hidden_states) # appending tensors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Convert the list of embeddings to a tensor\n",
    "# sentence_embeddings = torch.cat(sentence_embeddings, dim=0)\n",
    "\n",
    "# # Print the shape of the final tensor\n",
    "# print(sentence_embeddings.shape)  #[total_sentences, max_seq_length, hidden_units_size]\n",
    "\n",
    "# # If you want to print the embeddings themselves\n",
    "# print(sentence_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Forward pass through the model to get embeddings\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**tokens)\n",
    "\n",
    "# # Extract embeddings from the last hidden states\n",
    "# last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# # You might want to use the [CLS] token embedding as the sentence embedding\n",
    "# sentence_embedding = last_hidden_states[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# # Append the sentence embedding to the list\n",
    "# sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "# # Convert the list of embeddings to a numpy array\n",
    "# sentence_embeddings = np.array(sentence_embeddings)\n",
    "\n",
    "# # Now, sentence_embeddings contains the Arabert embeddings for each sentence\n",
    "# print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out auto bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e8832dbb5d4ae29305c961d0338b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98646d741b8449f3a3f3f99c951371f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56df50f19a9475a9d68e379f2aaa4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/334k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b16bd32873e435f862a5ddb900091cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3522c30d7674b0f8bb7d0e109784a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/asafaya/bert-base-arabic/bfb09a6415da43a9e5b5ae32de9de6940e132a4d8f3c20c67bec50345a8b65fc?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1704140267&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNDE0MDI2N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9hc2FmYXlhL2JlcnQtYmFzZS1hcmFiaWMvYmZiMDlhNjQxNWRhNDNhOWU1YjVhZTMyZGU5ZGU2OTQwZTEzMmE0ZDhmM2MyMGM2N2JlYzUwMzQ1YThiNjVmYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=duQeEYrsGb2pjhWbpiB5U866GVegbBCNJ16ZNM6DmH8yPH6wmkJN-TL-MLyIzi3ZoZ1Nc3UvCPUsVqGqEpeL1omUSCVG40-ZhyaBraHRST2wlix63LY779IH6jwgj205739qacUf7u9KDpepp758swv8b9cLKndAV82spAJx7xgb5OjlhGHKlJCEbJ9K7UUjbUojECCtIYbPP8QwayK7bl%7Exn2A9IRkaDFS6igXrlL993JfCVTnpLOVuEqRFt-pvYcku78efEWf2wYbwU-arh%7E-jgx-8ELhEimY%7E3eDMlYwqIGfoleDm4XPNxLz8P1ATuGz7cZJ7mLnj-5EOFLC8pQ__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7667a3774b944cf2b86f1613b1818677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  94%|#########4| 419M/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m     model_output\u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokens)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 10\u001b[0m auto_sentences_embeddings\u001b[38;5;241m=\u001b[39m\u001b[43m[\u001b[49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlist_of_string_sentences\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(auto_sentences_embeddings))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# train_embeddings=[get_embeddings(line) for line in file_content]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# val_embeddings=[get_embeddings(line) for line in file_content_2]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# print(\"Train embeddings:\", len(train_embeddings))\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# print(\"val embeddings:\", len(val_embeddings))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m     model_output\u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokens)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 10\u001b[0m auto_sentences_embeddings\u001b[38;5;241m=\u001b[39m[\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m list_of_string_sentences]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(auto_sentences_embeddings))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# train_embeddings=[get_embeddings(line) for line in file_content]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# val_embeddings=[get_embeddings(line) for line in file_content_2]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# print(\"Train embeddings:\", len(train_embeddings))\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# print(\"val embeddings:\", len(val_embeddings))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embeddings\u001b[39m(text):\n\u001b[0;32m      5\u001b[0m     tokens\u001b[38;5;241m=\u001b[39m tokenizer(text,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     model_output\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\transformers\\pytorch_utils.py:242\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:452\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    451\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 452\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\transformers\\activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "model= AutoModel.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "\n",
    "def get_embeddings(text):\n",
    "    tokens= tokenizer(text,return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    model_output= model(**tokens)\n",
    "\n",
    "    return model_output.last_hidden_state.detach().numpy()\n",
    "\n",
    "auto_sentences_embeddings=[get_embeddings(sentence) for sentence in list_of_string_sentences]\n",
    "\n",
    "print(len(auto_sentences_embeddings))\n",
    "\n",
    "# train_embeddings=[get_embeddings(line) for line in file_content]\n",
    "# val_embeddings=[get_embeddings(line) for line in file_content_2]\n",
    "\n",
    "# print(\"Train embeddings:\", len(train_embeddings))\n",
    "# print(\"val embeddings:\", len(val_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
