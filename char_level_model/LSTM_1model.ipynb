{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pk5hIYW8e-W"
      },
      "source": [
        "### 1- Preprocessing :\n",
        "\n",
        "define characters accepted and tashkeel accepted and remove from the training set any tashkeel and unwanted characters (eg. brackets, numbers... etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zyj-hwPy8e-X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pyarabic.araby as araby\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDCbPGSl8e-Z",
        "outputId": "fdd65803-038e-497d-fe80-e053a65675d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "18515271\n"
          ]
        }
      ],
      "source": [
        "arabicDictionary=['ى', 'ع', 'ظ', 'ح', 'ر', 'س', 'ي', 'ش', 'ض', 'ق', ' ', 'ث', 'ل', 'ص', 'ط', 'ك', 'آ', 'م', 'ا', 'إ', 'ه', 'ز', 'ء', 'أ', 'ف', 'ؤ', 'غ', 'ج', 'ئ', 'د', 'ة', 'خ', 'و', 'ب', 'ذ', 'ت', 'ن']\n",
        "punctuations = [\"،\", \":\", \"؛\", \"-\", \"؟\"]\n",
        "#reading the training dataset\n",
        "f = open(r\"../data/train.txt\", \"r\",encoding=\"utf-8\").read()\n",
        "print(type(f))\n",
        "print(len(f))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DT8E5vQk8e-Z"
      },
      "outputs": [],
      "source": [
        "def cleanDataFromNonArabicLetters(data):\n",
        "    #regex to keep arabic letters only and remove any other character (eg. brackets, numbers ...etc)\n",
        "    characters_regex =r'[\\s\\.\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD]+'\n",
        "    processedData = re.findall(characters_regex,data)\n",
        "    processedData = \" \".join(processedData)\n",
        "    processedData = re.sub(r\"\\s+\",\" \" ,processedData) #substitute many spaces with one space only\n",
        "    return processedData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWbsLqx68e-a",
        "outputId": "55a001cd-6d9a-4e7f-92f0-ee34d5487d6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17997403\n"
          ]
        }
      ],
      "source": [
        "processedData=cleanDataFromNonArabicLetters(f)\n",
        "print(len(processedData))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ؐ', 'ؑ', 'ؒ', 'ؓ', 'ؔ', 'ؕ', 'ؖ', 'ؗ', 'ؘ', 'ؙ', 'ؚ', 'ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ّ', 'ْ', 'ٓ', 'ٔ', 'ٕ', 'ٖ', 'ٗ', '٘', 'ٙ', 'ٚ', 'ٛ', 'ٜ', 'ٝ', 'ٞ', 'ٟ', 'ٰ', 'ۖ', 'ۗ', 'ۘ', 'ۙ', 'ۚ', 'ۛ', 'ۜ', '۟', '۠', 'ۡ', 'ۢ', 'ۣ', 'ۤ', 'ۧ', 'ۨ', '۪', '۫', '۬', 'ۭ']\n",
            "('ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ْ', 'ّ')\n"
          ]
        }
      ],
      "source": [
        "print(araby.DIACRITICS)\n",
        "print(araby.TASHKEEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "msZvg8zl8e-a"
      },
      "outputs": [],
      "source": [
        "def removeDiacratics(processedData):\n",
        "    without_diacritics= araby.strip_diacritics(processedData)\n",
        "    return without_diacritics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFe_FguB8e-a",
        "outputId": "77a526a4-f0b8-4585-caa4-669733a5633c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10711561\n"
          ]
        }
      ],
      "source": [
        "without_diacritics = removeDiacratics(processedData)\n",
        "print(len(without_diacritics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zNPODnYT8e-a"
      },
      "outputs": [],
      "source": [
        "#take the procrssed text and separate it into sentences\n",
        "def generateListsWithDiacraticsandWithout(with_diacratics, without_diacritics):\n",
        "    #---------------------Preprocessing of words with diacratics------------\n",
        "\n",
        "    #generate longStringSplited which is the list of procrssed text without brackets and numbers and dots\n",
        "    #the rest of the punctuation still there\n",
        "    longStringWithDecimalPoint=re.sub(r\"\\n\", ' ', with_diacratics) #string of data with diacratics removed endlines from it\n",
        "    for element in punctuations:\n",
        "        longStringWithDecimalPoint=re.sub(element, '', longStringWithDecimalPoint) #remove punctuations from the string\n",
        "\n",
        "    longStringSplited=longStringWithDecimalPoint.split('.') #split the string by dots (segment sentences) and make list of them\n",
        "    longString=' '.join(longStringSplited)  #long string without decimal points\n",
        "\n",
        "    #-------------------------------------\n",
        "\n",
        "    #---------------------Preprocessing of words without diacratics------------\n",
        "    without_diacratics_longStringWithDecimals=re.sub(r\"\\n\", ' ', without_diacritics) #string of data without diacratics removed endlines from it\n",
        "    for element in punctuations:\n",
        "        without_diacratics_longStringWithDecimals=re.sub(element, '', without_diacratics_longStringWithDecimals) #remove punctuations from the string\n",
        "\n",
        "    longStringSplited_withoutDiacratics=without_diacratics_longStringWithDecimals.split('.') #split the string by dots (segment sentences) and make list of them\n",
        "    longString_withoutDiacratics=' '.join(longStringSplited_withoutDiacratics)  #long string without decimal points\n",
        "\n",
        "    #-------------------------------------------------------------------------\n",
        "\n",
        "    list_of_sentences=[]\n",
        "    for line in longStringSplited_withoutDiacratics: #list of lists of sentences splitted in words without diacratics (used in embeddings)\n",
        "        list_of_sentences.append(line.split(\" \"))\n",
        "\n",
        "\n",
        "    # now the variable called longString has a single string with all the processed words in it\n",
        "    listOfwordsWith_Diacritics=list()\n",
        "    listOfwordsWith_NoDiacritics=list()\n",
        "\n",
        "    listOfwordsWith_Diacritics=re.sub(r\"\\s+\", ' ', longString)\n",
        "    listOfwordsWith_Diacritics=listOfwordsWith_Diacritics.split(\" \")\n",
        "\n",
        "    listOfwordsWith_NoDiacritics=re.sub(r\"\\s+\", ' ', longString_withoutDiacratics)\n",
        "    listOfwordsWith_NoDiacritics=listOfwordsWith_NoDiacritics.split(\" \")\n",
        "\n",
        "    return list_of_sentences,listOfwordsWith_Diacritics,listOfwordsWith_NoDiacritics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WN4oFLlv8e-a"
      },
      "outputs": [],
      "source": [
        "list_of_sentences,listOfwordsWith_Diacritics,listOfwordsWith_NoDiacritics= generateListsWithDiacraticsandWithout(processedData, without_diacritics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCiZwq3T8e-b",
        "outputId": "5952e769-525b-408d-a946-23b75ce31872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40836\n",
            "2102054\n",
            "2102054\n",
            "['أَوْ', 'سَيِّدُهَا', 'فِي', 'ذَلِكَ', 'جَازَ', 'لِأَنَّ', 'لَهُ', 'غَرَضًا', 'فِي', 'تَزَيُّنِهَا', 'لَهُ', 'كَمَا', 'فِي', 'الرَّوْضَةِ', 'وَهُوَ', 'الْأَوْجَهُ', 'وَإِنْ', 'جَرَى', 'فِي', 'التَّحْقِيقِ', 'عَلَى', 'خِلَافِ', 'ذَلِكَ', 'فِي', 'الْوَصْلِ', 'وَالْوَشْرِ', 'فَأَلْحَقَهُمَا', 'بِالْوَشْمِ', 'فِي', 'الْمَنْعِ', 'مُطْلَقًا', 'وَيُكْرَهُ', 'أَنْ', 'يَنْتِفَ', 'الشَّيْبَ', 'مِنْ', 'الْمَحَلِّ', 'الَّذِي', 'لَا', 'يُطْلَبُ', 'مِنْهُ', 'إزَالَةُ', 'شَعْرِهِ', 'وَيُسَنُّ', 'خَضْبُهُ', 'بِالْحِنَّاءِ', 'وَنَحْوِهِ', 'وَيُسَنُّ', 'لِلْمَرْأَةِ', 'الْمُزَوَّجَةِ', 'وَالْمَمْلُوكَةِ', 'خَضْبُ', 'كَفِّهَا', 'وَقَدَمِهَا', 'بِذَلِكَ', 'تَعْمِيمًا', 'لِأَنَّهُ', 'زِينَةٌ', 'وَهِيَ', 'مَطْلُوبَةٌ', 'مِنْهَا', 'لِحَلِيلِهَا', 'أَمَّا', 'النَّقْشُ', 'وَالتَّطْرِيفُ', 'فَلَا', 'يُسَنُّ', 'وَخَرَجَ', 'بِالْمُزَوَّجَةِ', 'وَالْمَمْلُوكَةِ', 'غَيْرُهُمَا', 'فَيُكْرَهُ', 'لَهُ', 'وَبِالْمَرْأَةِ', 'الرَّجُلُ', 'وَالْخُنْثَى', 'فَيَحْرُمُ', 'الْخِضَابُ', 'عَلَيْهِمَا', 'إلَّا', 'لِعُذْرٍ', 'نِهَايَةٌ', 'وَمُغْنِي', 'قَالَ', 'ع', 'ش', 'قَوْلُهُ', 'م', 'ر', 'وَيَحْرُمُ', 'عَلَى', 'الْمَرْأَةِ', 'خَرَجَ', 'بِالْمَرْأَةِ', 'غَيْرُهَا', 'مِنْ', 'ذَكَرٍ', 'وَأُنْثَى', 'صَغِيرَيْنِ', 'فَيَجُوزُ']\n",
            "['أو', 'سيدها', 'في', 'ذلك', 'جاز', 'لأن', 'له', 'غرضا', 'في', 'تزينها', 'له', 'كما', 'في', 'الروضة', 'وهو', 'الأوجه', 'وإن', 'جرى', 'في', 'التحقيق', 'على', 'خلاف', 'ذلك', 'في', 'الوصل', 'والوشر', 'فألحقهما', 'بالوشم', 'في', 'المنع', 'مطلقا', 'ويكره', 'أن', 'ينتف', 'الشيب', 'من', 'المحل', 'الذي', 'لا', 'يطلب', 'منه', 'إزالة', 'شعره', 'ويسن', 'خضبه', 'بالحناء', 'ونحوه', 'ويسن', 'للمرأة', 'المزوجة', 'والمملوكة', 'خضب', 'كفها', 'وقدمها', 'بذلك', 'تعميما', 'لأنه', 'زينة', 'وهي', 'مطلوبة', 'منها', 'لحليلها', 'أما', 'النقش', 'والتطريف', 'فلا', 'يسن', 'وخرج', 'بالمزوجة', 'والمملوكة', 'غيرهما', 'فيكره', 'له', 'وبالمرأة', 'الرجل', 'والخنثى', 'فيحرم', 'الخضاب', 'عليهما', 'إلا', 'لعذر', 'نهاية', 'ومغني', 'قال', 'ع', 'ش', 'قوله', 'م', 'ر', 'ويحرم', 'على', 'المرأة', 'خرج', 'بالمرأة', 'غيرها', 'من', 'ذكر', 'وأنثى', 'صغيرين', 'فيجوز']\n"
          ]
        }
      ],
      "source": [
        "print(len(list_of_sentences))\n",
        "print(len(listOfwordsWith_NoDiacritics))\n",
        "print(len(listOfwordsWith_Diacritics))\n",
        "print(listOfwordsWith_Diacritics[2101000:2101100])\n",
        "print(listOfwordsWith_NoDiacritics[2101000:2101100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "A9KcNsc78e-b"
      },
      "outputs": [],
      "source": [
        "def generateDiacraticsList(listOfwordsWith_Diacritics):\n",
        "    #now that we have two separated lists we need to get the diacritics list\n",
        "\n",
        "\n",
        "    listofDiacritrcs_ToWord=list()\n",
        "    temp=list()\n",
        "    counter=0\n",
        "    for word in listOfwordsWith_Diacritics:\n",
        "        while counter<len(word):\n",
        "            if word[counter] in arabicDictionary: #checking if the character is a letter\n",
        "                if (counter+1)<len(word):\n",
        "                    #checking if the next character is also a letter, then that means that the diacritics of the current letter is none so add empty string to the list\n",
        "                    if word[counter +1] in arabicDictionary:\n",
        "                        temp.append(\"\")\n",
        "                        counter+=1\n",
        "                        continue\n",
        "                counter+=1 #if it is the end of the word (no more letters) or the next character is a diacritics -> continue looping\n",
        "                continue\n",
        "            else:\n",
        "                if (counter+1)<len(word):\n",
        "                    if word[(counter+1)] not in arabicDictionary: #if the current and the next characters are diacritics, add them together in the list\n",
        "                        temp.append(word[counter]+word[counter+1])\n",
        "                        counter+=2\n",
        "                        continue\n",
        "                temp.append(word[counter]) #if the current character only is the diacritics add it to the list\n",
        "                counter+=1\n",
        "        listofDiacritrcs_ToWord.append(temp.copy())\n",
        "        temp.clear()\n",
        "        counter=0\n",
        "    return listofDiacritrcs_ToWord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "s6qEYdcc8e-b"
      },
      "outputs": [],
      "source": [
        "listofDiacritrcs_ToWord= generateDiacraticsList(listOfwordsWith_Diacritics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfd12Wcp8e-b",
        "outputId": "48c6aa14-a2a9-41b6-f003-b0f7ceeb7df5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2102054\n",
            "الْأَوَّلُ\n",
            "الأول\n",
            "['', 'ْ', 'َ', 'َّ', 'ُ']\n",
            " ْ َ َّ ُ\n",
            "[['َ', 'ْ', 'ُ', 'ُ'], ['َ', 'ْ'], ['َ', 'َ', 'َ'], ['', 'ْ', 'َ', 'َّ', 'ُ'], ['َ', 'َ', 'ُ'], ['', 'َ', 'ْ'], ['َ', '', 'َ'], ['', '', 'َّ', 'ْ', 'َ', 'ِ', 'ُّ'], ['', 'ْ', 'ُ'], ['َ', 'َ', 'َ', 'َ'], ['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'ْ', 'ٍ'], ['َ', 'ْ', 'َ', 'ِ', ''], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['َ', 'ْ', 'ِ'], ['َ', 'ِ', '', 'ٍ'], ['ِ', '', 'ْ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['ُ', 'ُ', '', 'َ'], ['َ'], ['ُ', 'ِ', 'َ'], ['ُ', 'ُ', '', 'ُ', 'ُ'], ['ِ', 'ْ'], ['', '', 'ِّ', '', 'ِ'], ['َ', 'ُ', '', 'َ', 'ً'], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['ُ', 'ْ', 'َ', 'ٍ'], ['ِ', 'َ', 'َ', 'ٍ'], ['َ', 'َ', 'ِّ'], ['ُ', 'َّ', '', 'ٍ'], ['', 'ْ', 'ُ'], ['َ', 'َ', 'َ', 'َ'], ['َ', 'ْ', 'ُ'], ['', 'ْ', 'ِ'], ['َ', '', 'ٍ'], ['َ', 'ْ'], ['ِ', 'ِ', 'ْ', 'ٍ'], ['َ', 'َ', 'َ', 'َّ', 'ُ', 'ُ'], ['ُ', 'َ'], ['َ', 'ُ', 'ْ', 'ِ'], ['', '', 'ُّ', 'َّ', '', 'ِ'], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['', 'ْ', 'ُ', 'ْ', 'َ', 'ِ'], ['ِ'], ['َ', 'ِ', '', 'ِ'], ['', '', 'َّ', 'َ', '', 'َ', 'ِ'], ['َ', '', '', 'ُّ', 'ُ', '', 'ِ'], ['ِ', '', 'َّ', 'َ', 'ِ'], ['َ', 'َ', 'ْ', 'ِ'], ['َ', 'ِ', 'َ'], ['َ', 'ِ', 'ْ', 'ٍ'], ['ُ', 'َ', 'َّ', 'ٌ'], ['َ', 'ْ', 'ُ'], ['َ', '', 'ِ', 'ٍ'], ['َ', 'َ', 'ْ', 'َ', '', 'ِ', 'ِ'], ['َ', 'َّ'], ['', '', 'َّ', '', 'ِ', 'َ'], ['َ', '', 'ِ', 'ٌ'], ['ِ', 'َ', '', 'َّ', 'ِ'], ['َ', 'َ', '', 'َ'], ['َ', '', 'َ'], ['َ', '', 'ِ', 'ٌ'], ['ُ', 'َ'], ['َ', '', '', 'ِّ', 'ْ', 'ِ', '', 'ِ'], ['', 'َ'], ['َ', 'ِ', 'َ'], ['', '', 'ِّ', 'ْ', 'َ'], ['ِ', 'َ', 'ْ', 'ِ', 'ِ'], ['ُ', 'ِ', 'َ'], ['َ', 'َ', 'ْ'], ['ُ', 'ْ', 'َ', 'َ', 'ْ'], ['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'َ', 'ِ'], ['َ'], ['َ', 'َ', 'َ', 'َّ', 'ُ'], ['', 'َ', 'ْ'], ['َ', 'ْ'], ['', 'ْ', 'َ', 'ِ', 'َّ', 'ُ'], ['َ', 'ْ', 'ُ', 'ُ'], ['َ'], ['َ', 'َّ'], ['َ', 'ْ'], ['ُ', 'َ', 'ْ', 'َ'], ['َ', 'ْ', 'ِ'], ['', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', 'ْ'], ['َ', 'َ', 'ْ'], ['', 'ْ', 'َ', 'َ', 'َ'], ['َ', 'َ'], ['َ', 'ْ', 'َ', 'ْ'], ['َ', 'ُ'], ['ِ', 'َ', '', 'ٍ'], ['َ', 'ْ'], ['َ', 'ْ', 'ُ', '', 'ُ'], ['َ', '', 'ً'], ['َ', 'َ'], ['َ', 'َ', 'َ'], ['َ', 'ُ'], ['ِ', 'ْ', 'َ'], ['', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'ْ']]\n"
          ]
        }
      ],
      "source": [
        "print(len(listofDiacritrcs_ToWord))\n",
        "print(listOfwordsWith_Diacritics[3])\n",
        "print(listOfwordsWith_NoDiacritics[3])\n",
        "print(listofDiacritrcs_ToWord[3])\n",
        "print(\" \".join(listofDiacritrcs_ToWord[3]))\n",
        "print(listofDiacritrcs_ToWord[0:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ1PfuoL8e-c"
      },
      "source": [
        "2-Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PcEuZUcR8e-c"
      },
      "outputs": [],
      "source": [
        "def word2vecmodel(input):\n",
        "    # Train Word2Vec model\n",
        "    Word2Vec_model = Word2Vec(input, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "    Word2Vec_model.train(input,total_examples=len(listOfwordsWith_NoDiacritics),epochs=10)\n",
        "    word_embeddings = {word: Word2Vec_model.wv[word] for word in Word2Vec_model.wv.index_to_key}\n",
        "    return Word2Vec_model, word_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkhOJfrv8e-c",
        "outputId": "81432630-38c4-4465-c0ae-fb20e5de579b"
      },
      "outputs": [],
      "source": [
        "word_embeddings_model, word_embeddings= word2vecmodel(list_of_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 3.3951161e+00  8.5592109e-01 -2.2918375e+00 -2.2231481e+00\n",
            "  2.5383675e-01  9.9132782e-01  2.4807212e+00 -3.2732353e-01\n",
            " -3.6040537e+00 -2.0316505e-01  2.6530492e+00 -2.7301304e+00\n",
            " -4.0684748e+00  1.9026032e-01  2.2310562e-01 -8.6407579e-02\n",
            "  2.9369183e+00  1.4168760e+00 -1.1637067e+00 -1.5514373e+00\n",
            " -2.1446209e+00  1.0675027e+00  1.3981719e+00 -2.0157531e-01\n",
            " -2.4993584e+00  8.7615705e-01 -6.2097263e-02 -2.9746735e+00\n",
            "  2.2204550e-01  1.3652519e+00  7.9523170e-01  7.1530253e-01\n",
            "  2.0851970e+00  1.3443938e+00  8.6962658e-01 -2.8256896e+00\n",
            "  2.2478557e+00  1.0769629e+00  1.1638831e+00  2.6684468e+00\n",
            " -2.3647439e+00  4.2594150e-01  1.1452788e+00  3.3812528e+00\n",
            " -2.4977138e+00 -3.8255936e-01 -5.5910254e-01  3.8480799e+00\n",
            " -1.6152053e+00 -1.8216179e-01  2.1340044e+00  1.9389017e+00\n",
            "  7.6469445e-01  1.1301445e+00  3.5559771e+00  2.2294021e+00\n",
            " -2.1925030e+00  4.2428469e-04 -1.6035904e+00 -1.0678327e+00\n",
            " -7.9947644e-01  9.1474766e-01  5.1341707e-01 -2.9360211e-01\n",
            "  2.7112889e+00 -2.7934201e+00 -1.3809750e+00  1.5789708e+00\n",
            "  1.1352438e+00  2.9507973e+00  2.7956358e-01 -1.5543653e+00\n",
            "  8.3576195e-02 -3.9421329e-01  1.9107968e+00 -9.3178385e-01\n",
            " -5.0781977e-01 -1.3178898e+00 -6.5640199e-01 -3.1228402e+00\n",
            "  2.3681676e+00 -7.6880074e-01 -4.2440715e-01  3.7627336e-01\n",
            "  1.1436762e+00  2.5473804e+00  2.1523765e-01  2.3753495e+00\n",
            "  1.3690187e+00 -5.6259978e-01  9.6613568e-01  1.2254591e+00\n",
            "  3.1977518e+00  1.8736868e+00  2.5083301e+00  1.5481935e-03\n",
            " -2.0475795e+00 -2.4681432e+00 -7.6121527e-01 -1.4187913e+00]\n",
            "[ 0.48490542  0.41690192 -0.07400803 -1.179984    0.458406   -1.4125257\n",
            "  0.9145639   0.37492642 -0.15295516 -1.1568216  -0.7753386  -0.11354851\n",
            "  2.3032308   0.56503826  3.0633607  -0.92887515  1.7681037   0.2518812\n",
            " -2.2426195  -1.065388    2.5886354  -1.7936237   0.4650507  -1.416897\n",
            "  0.8620183   0.35492614 -0.1984788  -0.41751048 -1.7131019   0.2751004\n",
            "  0.95101583  0.40855044  1.5314354  -0.33285302  1.3663834  -0.23995277\n",
            "  0.72564495  1.1182871   0.1423567   0.985058   -1.5728809   0.9156748\n",
            " -1.4941099  -0.8326466  -0.072021   -0.60060114  1.3847568   1.3421034\n",
            " -1.3531029   0.20828974 -2.5567503  -0.41079986 -2.4826174  -0.794331\n",
            " -1.5020655   0.99548125 -0.40596873  1.2464496   0.23088372  0.8306194\n",
            "  1.1367617   1.2058344   1.1562831  -1.2298018   0.01979249  1.3565588\n",
            "  1.3617613  -0.8956149  -1.9608853   2.7799127   0.97542787 -0.5721677\n",
            "  0.1253523   0.1250112  -0.4360465   0.7029983   1.1155112   1.7153438\n",
            "  1.225944    2.038959   -1.3685628  -1.4669013   0.68468916  0.8812113\n",
            " -1.0932752   0.9164129   2.7727623  -0.48922256 -1.651263    0.3963265\n",
            "  0.7209451   1.5138012  -0.17232716  0.51760817  0.13992289 -1.7860239\n",
            "  0.5159883  -1.9830835  -0.6332107   2.2147148 ]\n",
            "Word2Vec<vocab=105746, vector_size=100, alpha=0.025>\n"
          ]
        }
      ],
      "source": [
        "print(word_embeddings[\"الأول\"])\n",
        "print(word_embeddings[\"ل\"])\n",
        "# print(word_embeddings[\" ِ\"])   # error\n",
        "#print(word_embeddings[\"الْأَوَّلُ\"])  #error\n",
        "print(word_embeddings_model)  #Word2Vec<vocab=105746, vector_size=100, alpha=0.025>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqvLfTXu8e-c"
      },
      "source": [
        "3-Building the LSTM model for character level classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lurACkX88e-c"
      },
      "outputs": [],
      "source": [
        "def formatingXandYtrain(listofDiacritrcs_ToWord,listOfwordsWith_NoDiacritics):\n",
        "    for i in range(len(listofDiacritrcs_ToWord)):\n",
        "        if i==0:\n",
        "            print(listofDiacritrcs_ToWord[0])\n",
        "        listofDiacritrcs_ToWord[i] = \" \".join(listofDiacritrcs_ToWord[i])\n",
        "        if i==0:\n",
        "            print(listofDiacritrcs_ToWord[0])\n",
        "    Y_train= np.array([listofDiacritrcs_ToWord],dtype=object).T\n",
        "    X_train = np.array([listOfwordsWith_NoDiacritics],dtype=object)\n",
        "    print(str(X_train.shape))\n",
        "    print(str(Y_train.shape))\n",
        "    return X_train,Y_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHlZPh-K8e-c",
        "outputId": "bf41f93e-21e2-4bd0-d32e-7e9df95d637a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['َ', 'ْ', 'ُ', 'ُ']\n",
            "َ ْ ُ ُ\n",
            "(1, 2102054)\n",
            "(2102054, 1)\n"
          ]
        }
      ],
      "source": [
        "X_train,Y_train= formatingXandYtrain(listofDiacritrcs_ToWord,listOfwordsWith_NoDiacritics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "َ ْ ُ ُ\n",
            "َ\n",
            " \n",
            "ْ\n",
            " \n",
            "قوله\n",
            "ق\n",
            "و\n",
            "ل\n",
            "ه\n",
            "['قوله' 'أو' 'قطع' ... 'قبل' 'موته' '']\n",
            "['َ ْ ُ ُ']\n",
            "[['قوله' 'أو' 'قطع' ... 'قبل' 'موته' '']]\n",
            "[['َ ْ ُ ُ']\n",
            " ['َ ْ']\n",
            " ['َ َ َ']\n",
            " ...\n",
            " ['َ ْ َ']\n",
            " ['َ ْ ِ ِ']\n",
            " ['']]\n",
            "(1, 2102054)\n",
            "(2102054, 1)\n"
          ]
        }
      ],
      "source": [
        "print(listofDiacritrcs_ToWord[0])\n",
        "print(listofDiacritrcs_ToWord[0][0])\n",
        "print(listofDiacritrcs_ToWord[0][1])\n",
        "print(listofDiacritrcs_ToWord[0][2])\n",
        "print(listofDiacritrcs_ToWord[0][3])\n",
        "print(listOfwordsWith_NoDiacritics[0])\n",
        "print(listOfwordsWith_NoDiacritics[0][0])\n",
        "print(listOfwordsWith_NoDiacritics[0][1])\n",
        "print(listOfwordsWith_NoDiacritics[0][2])\n",
        "print(listOfwordsWith_NoDiacritics[0][3])\n",
        "#قَوْلُهُ   \n",
        "# mn 8er a5r 7arf??????????? idk\n",
        "\n",
        "\n",
        "print(X_train[0])\n",
        "print(Y_train[0])\n",
        "\n",
        "print(X_train)\n",
        "print(Y_train)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qIMz0yEH8e-c"
      },
      "outputs": [],
      "source": [
        "def prepareForTraining(listOfwordsWith_NoDiacritics,listofDiacritrcs_ToWord):\n",
        "    # Tokenize the input words and diacritics\n",
        "    word_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters='')\n",
        "    word_tokenizer.fit_on_texts(listOfwordsWith_NoDiacritics)\n",
        "    word_sequences = word_tokenizer.texts_to_sequences(listOfwordsWith_NoDiacritics)\n",
        "\n",
        "    diacritic_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters='')\n",
        "    diacritic_tokenizer.fit_on_texts([''.join(d) for d in listofDiacritrcs_ToWord])\n",
        "    diacritic_sequences = diacritic_tokenizer.texts_to_sequences([''.join(d) for d in listofDiacritrcs_ToWord])\n",
        "\n",
        "    # Pad sequences to have the same length\n",
        "    max_len = max(max(len(seq) for seq in word_sequences), max(len(seq) for seq in diacritic_sequences))\n",
        "\n",
        "    return word_tokenizer,diacritic_tokenizer,max_len,word_sequences,diacritic_sequences\n",
        "\n",
        "def padSequences(word_sequences,diacritic_sequences):\n",
        "    # Pad sequences to have the same length\n",
        "    padded_word_sequences = pad_sequences(word_sequences, maxlen=max_len, padding='post')\n",
        "    padded_diacritic_sequences = pad_sequences(diacritic_sequences, maxlen=max_len, padding='post')\n",
        "    return padded_word_sequences,padded_diacritic_sequences\n",
        "\n",
        "def trainDiacritics(word_tokenizer,diacritic_tokenizer,max_len,word_sequences,diacritic_sequences,padded_word_sequences,padded_diacritic_sequences):\n",
        "\n",
        "    # Build the LSTM model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=50, input_length=max_len))\n",
        "    model.add(LSTM(100, return_sequences=True))\n",
        "    model.add(Dense(len(diacritic_tokenizer.word_index) + 1, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(padded_word_sequences, np.expand_dims(padded_diacritic_sequences, -1), epochs=10, batch_size=32)\n",
        "\n",
        "    # Save the model for later use\n",
        "    model.save(\"diacritic_prediction_model.h5\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Now, you can use the trained model to predict diacritics for new Arabic words\n",
        "def predict_diacritics(model,word_tokenizer, diacritic_tokenizer,max_len, word):\n",
        "    word_sequence = word_tokenizer.texts_to_sequences([word])\n",
        "    padded_word_sequence = pad_sequences(word_sequence, maxlen=max_len, padding='post')\n",
        "    predicted_diacritic_sequence = model.predict(padded_word_sequence)\n",
        "    predicted_diacritic_sequence = np.argmax(predicted_diacritic_sequence, axis=-1)\n",
        "    predicted_diacritic = diacritic_tokenizer.sequences_to_texts(predicted_diacritic_sequence)\n",
        "    return predicted_diacritic[0]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "0Y8-PRNu8e-d",
        "outputId": "879e92aa-40a8-4c60-bb9a-48b13b885ffa"
      },
      "outputs": [],
      "source": [
        "word_tokenizer,diacritic_tokenizer,max_len,word_sequences,diacritic_sequences = prepareForTraining(listOfwordsWith_NoDiacritics,listofDiacritrcs_ToWord)\n",
        "padded_word_sequences,padded_diacritic_sequences=padSequences(word_sequences,diacritic_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBWiJoUl8e-d"
      },
      "source": [
        "# !!!! DO NOT RUN THE COMING CELL UNLESS YOU HAVE MADE UPDATES IN THE MODEL\n",
        "you can use the model by calling:\n",
        "### model = load_model('diacritic_prediction_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkS8ReGy8e-d",
        "outputId": "d4cffa80-ac28-4a58-b09a-0f6d1e1f9a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "65690/65690 [==============================] - 919s 14ms/step - loss: 0.1176 - accuracy: 0.9577\n",
            "Epoch 2/10\n",
            "65690/65690 [==============================] - 926s 14ms/step - loss: 0.0918 - accuracy: 0.9656\n",
            "Epoch 3/10\n",
            "65690/65690 [==============================] - 15081s 230ms/step - loss: 0.0885 - accuracy: 0.9666\n",
            "Epoch 4/10\n",
            "65690/65690 [==============================] - 1742s 27ms/step - loss: 0.0869 - accuracy: 0.9671\n",
            "Epoch 5/10\n",
            "65690/65690 [==============================] - 1458s 22ms/step - loss: 0.0860 - accuracy: 0.9674\n",
            "Epoch 6/10\n",
            "65690/65690 [==============================] - 1331s 20ms/step - loss: 0.0853 - accuracy: 0.9676\n",
            "Epoch 7/10\n",
            "65690/65690 [==============================] - 1434s 22ms/step - loss: 0.0849 - accuracy: 0.9677\n",
            "Epoch 8/10\n",
            "65690/65690 [==============================] - 1538s 23ms/step - loss: 0.0845 - accuracy: 0.9678\n",
            "Epoch 9/10\n",
            "65690/65690 [==============================] - 2518s 38ms/step - loss: 0.0843 - accuracy: 0.9679\n",
            "Epoch 10/10\n",
            "65690/65690 [==============================] - 2832s 43ms/step - loss: 0.0840 - accuracy: 0.9679\n"
          ]
        }
      ],
      "source": [
        "model=trainDiacritics(word_tokenizer,diacritic_tokenizer,max_len,word_sequences,diacritic_sequences,padded_word_sequences,padded_diacritic_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JQLjXSbw8e-d"
      },
      "outputs": [],
      "source": [
        "model = load_model('diacritic_prediction_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHuqtTuf8e-d",
        "outputId": "5b6a1160-1c33-48ce-f5ec-92522a207d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of words 4\n",
            "1/1 [==============================] - 1s 599ms/step\n",
            "ذَهَبَ\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "عَلَيَّ\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "إلَى\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "الَّْشاِطِئ\n",
            "ذَهَبَ عَلَيَّ إلَى الَّْشاِطِئ \n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "# input = \"الذي علم بالقلم\"\n",
        "input= \"ذهب علي إلى الشاطئ\"\n",
        "model = load_model(\"diacritic_prediction_model.h5\")\n",
        "\n",
        "# input = \"قال الزركشي ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر\"\n",
        "input_word = input.split(' ')\n",
        "word_vectors = [word_embeddings_model.wv[word] for word in input_word if word in word_embeddings_model.wv]\n",
        "# print(word_vectors)\n",
        "outputString=\"\"\n",
        "print(\"number of words\",len(input_word))\n",
        "for word in input_word:\n",
        "  predicted_diacritics1 = predict_diacritics(model,word_tokenizer, diacritic_tokenizer,max_len, word)\n",
        "  #print(predicted_diacritics1.split(' '))\n",
        "  predictedDiacritics=(predicted_diacritics1.split('  '))\n",
        "  temp=list()\n",
        "  for diacrtic in predictedDiacritics:\n",
        "    # print(diacrtic)\n",
        "    # print(len(diacrtic))\n",
        "    if ' ' in diacrtic:\n",
        "      # print(\"after space removel\")\n",
        "      diacrtic=diacrtic.replace(' ','')\n",
        "      # print(diacrtic)\n",
        "      # print(len(diacrtic))\n",
        "      temp.append(diacrtic)\n",
        "      continue\n",
        "    temp.append(diacrtic)\n",
        "  predictedDiacritics=temp.copy()\n",
        "  tempString=\"\"\n",
        "  for i in range(len(word)):\n",
        "    tempString+=word[i]\n",
        "    if i<len(predictedDiacritics):\n",
        "      tempString+=predictedDiacritics[i]\n",
        "  print(tempString)\n",
        "  tempString+=\" \"\n",
        "  outputString+=tempString\n",
        "\n",
        "print(outputString)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TzBU3UGN8e-d"
      },
      "outputs": [],
      "source": [
        "def get_word_context_embedding(word):\n",
        "    try:\n",
        "        return word_embeddings_model.wv[word]\n",
        "    except KeyError:\n",
        "        return np.zeros(word_embeddings_model.vector_size)  # Return zero vector for out-of-vocabulary words\n",
        "\n",
        "word_context_embeddings = [get_word_context_embedding(word) for word in list_of_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3DUsB-z8e-d"
      },
      "outputs": [],
      "source": [
        "combined_embeddings = np.concatenate([word_context_embeddings, padded_word_sequences], axis=-1)\n",
        "max_len_diacritic = max_len + word_embeddings_model.vector_size  # Adjust to match the expected model input length\n",
        "\n",
        "padded_word_sequences = pad_sequences(word_sequences, maxlen=max_len, padding='post')\n",
        "padded_diacritic_sequences = pad_sequences(diacritic_sequences, maxlen=max_len_diacritic, padding='post')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
