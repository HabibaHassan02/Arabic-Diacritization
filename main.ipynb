{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Preprocessing :\n",
    "define characters accepted and tashkeel accepted and remove from the training set any tashkeel and unwanted characters (eg. brackets, numbers... etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pyarabic.araby as araby\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import load_model\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "# from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ْ', 'ّ', 'ٌ', 'ٍ', 'ِ', 'ً', 'َ', 'ُ', 'ى', 'ع', 'ظ', 'ح', 'ر', 'س', 'ي', 'ش', 'ض', 'ق', 'ث', 'ل', 'ص', 'ط', 'ك', 'آ', 'م', 'ا', 'إ', 'ه', 'ز', 'ء', 'أ', 'ف', 'ؤ', 'غ', 'ج', 'ئ', 'د', 'ة', 'خ', 'و', 'ب', 'ذ', 'ت', 'ن', '.', '،', ':', '؛', '-', '؟']\n",
      "['ى', 'ع', 'ظ', 'ح', 'ر', 'س', 'ي', 'ش', 'ض', 'ق', 'ث', 'ل', 'ص', 'ط', 'ك', 'آ', 'م', 'ا', 'إ', 'ه', 'ز', 'ء', 'أ', 'ف', 'ؤ', 'غ', 'ج', 'ئ', 'د', 'ة', 'خ', 'و', 'ب', 'ذ', 'ت', 'ن', '.', '،', ':', '؛', '-', '؟']\n"
     ]
    }
   ],
   "source": [
    "tashkeel = [\"ْ\", \"ّ\", \"ٌ\", \"ٍ\", \"ِ\", \"ً\", \"َ\", \"ُ\"]\n",
    "arabicCharacters = \"ىعظحرسيشضقثلصطكآماإهزءأفؤغجئدةخوبذتن\"\n",
    "arabicDictionary=['ى', 'ع', 'ظ', 'ح', 'ر', 'س', 'ي', 'ش', 'ض', 'ق', ' ', 'ث', 'ل', 'ص', 'ط', 'ك', 'آ', 'م', 'ا', 'إ', 'ه', 'ز', 'ء', 'أ', 'ف', 'ؤ', 'غ', 'ج', 'ئ', 'د', 'ة', 'خ', 'و', 'ب', 'ذ', 'ت', 'ن']\n",
    "punctuations = [\".\", \"،\", \":\", \"؛\", \"-\", \"؟\"]\n",
    "validCharacters= tashkeel + list(arabicCharacters) + punctuations\n",
    "charcters_without_tashkeel = list(arabicCharacters) + punctuations\n",
    "print(validCharacters)\n",
    "print(charcters_without_tashkeel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#reading the training dataset\n",
    "f = open(r\"train.txt\", \"r\",encoding=\"utf-8\").read()\n",
    "print(type(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18515271\n",
      "17997403\n",
      "ق\n"
     ]
    }
   ],
   "source": [
    "print(len(f))\n",
    "# arabic_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
    "#regex to keep arabic letters only and remove any other character (eg. brackets, numbers ...etc)\n",
    "characters_regex =r'[\\s\\.\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD]+'\n",
    "processedData = re.findall(characters_regex,f)\n",
    "processedData = \" \".join(processedData)\n",
    "processedData = re.sub(r\"\\s+\",\" \" ,processedData) #substitute many spaces with one space only\n",
    "# processedData = ' '.join(word for word in processedData.split() if word not in arabic_stopwords) # remove stopwors from text\n",
    "print(len(processedData))\n",
    "print(processedData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = open('processed.txt', 'w', encoding='utf-8')\n",
    "\n",
    "w.write(processedData)\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10711561\n"
     ]
    }
   ],
   "source": [
    "#remove diacritics\n",
    "without_diacritics= araby.strip_diacritics(processedData)\n",
    "print(len(without_diacritics))\n",
    "w2 = open('withoutDiacritics.txt', 'w', encoding='utf-8')\n",
    "w2.write(without_diacritics)\n",
    "w2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40836\n"
     ]
    }
   ],
   "source": [
    "#take the procrssed text and separate it into sentences\n",
    "fileRead=open('processed.txt', encoding='utf-8')\n",
    "lines=fileRead.readlines()\n",
    "longString1=' '.join(lines)\n",
    "longStringSplited=re.sub(r\"\\n\", ' ', longString1)\n",
    "without_diacritics_splitted=re.sub(r\"\\n\", ' ', without_diacritics)\n",
    "without_diacritics_splitted=without_diacritics_splitted.split('.')\n",
    "#the dot itself got removed in the process\n",
    "longStringSplited=longStringSplited.split('.')\n",
    "print(len(longStringSplited))\n",
    "w2 = open('withoutDiacritics.txt', 'w', encoding='utf-8')\n",
    "\n",
    "w2.write(without_diacritics)\n",
    "\n",
    "w3 = open('processed_sentences_separated.txt', 'w', encoding='utf-8')\n",
    "list_of_sentences=[]\n",
    "for line in longStringSplited:\n",
    " w3.write(line)\n",
    "\n",
    "for line in without_diacritics_splitted: #list of lists of sentences splitted in words without diacratics\n",
    " list_of_sentences.append(line.split(\" \"))\n",
    "\n",
    " #now longStringSplited is the list of procrssed text without practices and numbers and dots \n",
    " #the rest of the punctuation still there  \n",
    "# print(list_of_sentences[0:50]) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# now we want to create a list of words and a crossbonding list of punctuation for each word \n",
    "\n",
    "#first remove all puncturation \n",
    "\n",
    "fileRead=open('processed_sentences_separated.txt', encoding='utf-8')\n",
    "lines=fileRead.readlines()\n",
    "print(type(lines))\n",
    "longString=' '.join(lines)             \n",
    "longString=re.sub(r\"\\n\", ' ', longString)\n",
    "if longString == longString1:\n",
    "     print(\"true\")\n",
    "punctuations = [\"،\", \":\", \"؛\", \"-\", \"؟\"]\n",
    "for element in punctuations:\n",
    "     longString=re.sub(element, '', longString)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the varable called longString has a single string with all the processed words in it \n",
    "listOfwordsWith_Diacritics=list()\n",
    "listOfwordsWith_NoDiacritics=list()\n",
    "\n",
    "without_diacritics= araby.strip_diacritics(longString)\n",
    "\n",
    "\n",
    "without_diacritics=re.sub(r\"\\s+\", ' ', without_diacritics)\n",
    "\n",
    "listOfwordsWith_NoDiacritics=without_diacritics.split(\" \")\n",
    "\n",
    "\n",
    "listOfwordsWith_Diacritics=re.sub(r\"\\s+\", ' ', longString)\n",
    "listOfwordsWith_Diacritics=listOfwordsWith_Diacritics.split(\" \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قَوْلُهُ', 'أَوْ', 'قَطَعَ', 'الْأَوَّلُ', 'يَدَهُ', 'إلَخْ', 'قَالَ', 'الزَّرْكَشِيُّ', 'ابْنُ', 'عَرَفَةَ', 'قَوْلُهُ', 'بِلَفْظٍ', 'يَقْتَضِيه', 'كَإِنْكَارِ', 'غَيْرِ', 'حَدِيثٍ', 'بِالْإِسْلَامِ', 'وُجُوبَ', 'مَا', 'عُلِمَ', 'وُجُوبُهُ', 'مِنْ', 'الدِّينِ', 'ضَرُورَةً', 'كَإِلْقَاءِ', 'مُصْحَفٍ', 'بِقَذَرٍ', 'وَشَدِّ', 'زُنَّارٍ', 'ابْنُ', 'عَرَفَةَ', 'قَوْلُ', 'ابْنِ', 'شَاسٍ', 'أَوْ', 'بِفِعْلٍ', 'يَتَضَمَّنُهُ', 'هُوَ', 'كَلُبْسِ', 'الزُّنَّارِ', 'وَإِلْقَاءِ', 'الْمُصْحَفِ', 'فِي', 'صَرِيحِ', 'النَّجَاسَةِ', 'وَالسُّجُودِ', 'لِلصَّنَمِ', 'وَنَحْوِ', 'ذَلِكَ', 'وَسِحْرٍ', 'مُحَمَّدٌ', 'قَوْلُ', 'مَالِكٍ', 'وَأَصْحَابِهِ', 'أَنَّ', 'السَّاحِرَ', 'كَافِرٌ', 'بِاَللَّهِ', 'تَعَالَى', 'قَالَ', 'مَالِكٌ', 'هُوَ', 'كَالزِّنْدِيقِ', 'إذَا', 'عَمِلَ', 'السِّحْرَ', 'بِنَفْسِهِ', 'قُتِلَ', 'وَلَمْ', 'يُسْتَتَبْ', 'قَوْلُهُ', 'لِعَدَمِ', 'مَا', 'تَتَعَلَّقُ', 'إلَخْ', 'أَيْ', 'الْوَصِيَّةُ', 'قَوْلُهُ', 'مَا', 'مَرَّ', 'أَيْ', 'قُبَيْلَ', 'قَوْلِ', 'الْمَتْنِ', 'لَغَتْ', 'وَلَوْ', 'اقْتَصَرَ', 'عَلَى', 'أَوْصَيْت', 'لَهُ', 'بِشَاةٍ', 'أَوْ', 'أَعْطُوهُ', 'شَاةً', 'وَلَا', 'غَنَمَ', 'لَهُ', 'عِنْدَ', 'الْمَوْتِ', 'هَلْ']\n",
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة', 'قوله', 'بلفظ', 'يقتضيه', 'كإنكار', 'غير', 'حديث', 'بالإسلام', 'وجوب', 'ما', 'علم', 'وجوبه', 'من', 'الدين', 'ضرورة', 'كإلقاء', 'مصحف', 'بقذر', 'وشد', 'زنار', 'ابن', 'عرفة', 'قول', 'ابن', 'شاس', 'أو', 'بفعل', 'يتضمنه', 'هو', 'كلبس', 'الزنار', 'وإلقاء', 'المصحف', 'في', 'صريح', 'النجاسة', 'والسجود', 'للصنم', 'ونحو', 'ذلك', 'وسحر', 'محمد', 'قول', 'مالك', 'وأصحابه', 'أن', 'الساحر', 'كافر', 'بالله', 'تعالى', 'قال', 'مالك', 'هو', 'كالزنديق', 'إذا', 'عمل', 'السحر', 'بنفسه', 'قتل', 'ولم', 'يستتب', 'قوله', 'لعدم', 'ما', 'تتعلق', 'إلخ', 'أي', 'الوصية', 'قوله', 'ما', 'مر', 'أي', 'قبيل', 'قول', 'المتن', 'لغت', 'ولو', 'اقتصر', 'على', 'أوصيت', 'له', 'بشاة', 'أو', 'أعطوه', 'شاة', 'ولا', 'غنم', 'له', 'عند', 'الموت', 'هل']\n"
     ]
    }
   ],
   "source": [
    "#now that we have two separated lists we need to get the diacritics list \n",
    "\n",
    "\n",
    "listofDiacritrcs_ToWord=list()\n",
    "temp=list()\n",
    "counter=0\n",
    "flag=0\n",
    "for word in listOfwordsWith_Diacritics:\n",
    "    while counter<len(word):\n",
    "     if word[counter] in arabicDictionary: #checking if the character is a letter\n",
    "      if (counter+1)<len(word):\n",
    "        #checking if the next character is also a letter, then that means that the diacritics of the current letter is none so add empty string to the list\n",
    "        if word[counter +1] in arabicDictionary: \n",
    "          temp.append(\"\")\n",
    "          counter+=1\n",
    "          continue\n",
    "      counter+=1 #if it is the end of the word (no more letters) or the next character is a diacritics -> continue looping\n",
    "      continue\n",
    "     else:\n",
    "      if (counter+1)<len(word):\n",
    "        if word[(counter+1)] not in arabicDictionary: #if the current and the next characters are diacritics, add them together in the list\n",
    "          temp.append(word[counter]+word[counter+1])\n",
    "          counter+=2\n",
    "          continue\n",
    "      temp.append(word[counter]) #if the current character only is the diacritics add it to the list\n",
    "      counter+=1    \n",
    "    listofDiacritrcs_ToWord.append(temp.copy())     \n",
    "    temp.clear() \n",
    "    counter=0\n",
    "          \n",
    "          \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "print(listOfwordsWith_Diacritics[0:100])\n",
    "print(listOfwordsWith_NoDiacritics[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2102046\n",
      "الْأَوَّلُ\n",
      "الأول\n",
      "['', 'ْ', 'َ', 'َّ', 'ُ']\n",
      " ْ َ َّ ُ\n",
      "[['َ', 'ْ', 'ُ', 'ُ'], ['َ', 'ْ'], ['َ', 'َ', 'َ'], ['', 'ْ', 'َ', 'َّ', 'ُ'], ['َ', 'َ', 'ُ'], ['', 'َ', 'ْ'], ['َ', '', 'َ'], ['', '', 'َّ', 'ْ', 'َ', 'ِ', 'ُّ'], ['', 'ْ', 'ُ'], ['َ', 'َ', 'َ', 'َ'], ['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'ْ', 'ٍ'], ['َ', 'ْ', 'َ', 'ِ', ''], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['َ', 'ْ', 'ِ'], ['َ', 'ِ', '', 'ٍ'], ['ِ', '', 'ْ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['ُ', 'ُ', '', 'َ'], ['َ'], ['ُ', 'ِ', 'َ'], ['ُ', 'ُ', '', 'ُ', 'ُ'], ['ِ', 'ْ'], ['', '', 'ِّ', '', 'ِ'], ['َ', 'ُ', '', 'َ', 'ً'], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['ُ', 'ْ', 'َ', 'ٍ'], ['ِ', 'َ', 'َ', 'ٍ'], ['َ', 'َ', 'ِّ'], ['ُ', 'َّ', '', 'ٍ'], ['', 'ْ', 'ُ'], ['َ', 'َ', 'َ', 'َ'], ['َ', 'ْ', 'ُ'], ['', 'ْ', 'ِ'], ['َ', '', 'ٍ'], ['َ', 'ْ'], ['ِ', 'ِ', 'ْ', 'ٍ'], ['َ', 'َ', 'َ', 'َّ', 'ُ', 'ُ'], ['ُ', 'َ'], ['َ', 'ُ', 'ْ', 'ِ'], ['', '', 'ُّ', 'َّ', '', 'ِ'], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['', 'ْ', 'ُ', 'ْ', 'َ', 'ِ'], ['ِ'], ['َ', 'ِ', '', 'ِ'], ['', '', 'َّ', 'َ', '', 'َ', 'ِ'], ['َ', '', '', 'ُّ', 'ُ', '', 'ِ'], ['ِ', '', 'َّ', 'َ', 'ِ'], ['َ', 'َ', 'ْ', 'ِ'], ['َ', 'ِ', 'َ'], ['َ', 'ِ', 'ْ', 'ٍ'], ['ُ', 'َ', 'َّ', 'ٌ'], ['َ', 'ْ', 'ُ'], ['َ', '', 'ِ', 'ٍ'], ['َ', 'َ', 'ْ', 'َ', '', 'ِ', 'ِ'], ['َ', 'َّ'], ['', '', 'َّ', '', 'ِ', 'َ'], ['َ', '', 'ِ', 'ٌ'], ['ِ', 'َ', '', 'َّ', 'ِ'], ['َ', 'َ', '', 'َ'], ['َ', '', 'َ'], ['َ', '', 'ِ', 'ٌ'], ['ُ', 'َ'], ['َ', '', '', 'ِّ', 'ْ', 'ِ', '', 'ِ'], ['', 'َ'], ['َ', 'ِ', 'َ'], ['', '', 'ِّ', 'ْ', 'َ'], ['ِ', 'َ', 'ْ', 'ِ', 'ِ'], ['ُ', 'ِ', 'َ'], ['َ', 'َ', 'ْ'], ['ُ', 'ْ', 'َ', 'َ', 'ْ'], ['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'َ', 'ِ'], ['َ'], ['َ', 'َ', 'َ', 'َّ', 'ُ'], ['', 'َ', 'ْ'], ['َ', 'ْ'], ['', 'ْ', 'َ', 'ِ', 'َّ', 'ُ'], ['َ', 'ْ', 'ُ', 'ُ'], ['َ'], ['َ', 'َّ'], ['َ', 'ْ'], ['ُ', 'َ', 'ْ', 'َ'], ['َ', 'ْ', 'ِ'], ['', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', 'ْ'], ['َ', 'َ', 'ْ'], ['', 'ْ', 'َ', 'َ', 'َ'], ['َ', 'َ'], ['َ', 'ْ', 'َ', 'ْ'], ['َ', 'ُ'], ['ِ', 'َ', '', 'ٍ'], ['َ', 'ْ'], ['َ', 'ْ', 'ُ', '', 'ُ'], ['َ', '', 'ً'], ['َ', 'َ'], ['َ', 'َ', 'َ'], ['َ', 'ُ'], ['ِ', 'ْ', 'َ'], ['', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'ْ']]\n"
     ]
    }
   ],
   "source": [
    "print(len(listOfwordsWith_NoDiacritics))\n",
    "print(listOfwordsWith_Diacritics[3])\n",
    "print(listOfwordsWith_NoDiacritics[3])\n",
    "print(listofDiacritrcs_ToWord[3])\n",
    "print(\" \".join(listofDiacritrcs_ToWord[3]))\n",
    "print(listofDiacritrcs_ToWord[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18728263, 22684760)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Word2Vec model\n",
    "Word2Vec_model = Word2Vec(list_of_sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "Word2Vec_model.train(list_of_sentences,total_examples=len(listOfwordsWith_NoDiacritics),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {word: Word2Vec_model.wv[word] for word in Word2Vec_model.wv.index_to_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-Building the LSTM model for character level classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2102046)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(listofDiacritrcs_ToWord)):\n",
    "        listofDiacritrcs_ToWord[i] = \" \".join(listofDiacritrcs_ToWord[i])\n",
    "# print(listofDiacritrcs_ToWord[0:100])\n",
    "Y_train= np.array([listofDiacritrcs_ToWord],dtype=object).T\n",
    "X_train = np.array([listOfwordsWith_NoDiacritics],dtype=object)\n",
    "print(str(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "65689/65689 [==============================] - 1123s 17ms/step - loss: 0.1175 - accuracy: 0.9577\n",
      "Epoch 2/10\n",
      "65689/65689 [==============================] - 1141s 17ms/step - loss: 0.0916 - accuracy: 0.9656\n",
      "Epoch 3/10\n",
      "65689/65689 [==============================] - 1100s 17ms/step - loss: 0.0883 - accuracy: 0.9666\n",
      "Epoch 4/10\n",
      "65689/65689 [==============================] - 1088s 17ms/step - loss: 0.0867 - accuracy: 0.9671\n",
      "Epoch 5/10\n",
      "65689/65689 [==============================] - 1099s 17ms/step - loss: 0.0858 - accuracy: 0.9674\n",
      "Epoch 6/10\n",
      "65689/65689 [==============================] - 1236s 19ms/step - loss: 0.0852 - accuracy: 0.9676\n",
      "Epoch 7/10\n",
      "65689/65689 [==============================] - 1646s 25ms/step - loss: 0.0847 - accuracy: 0.9677\n",
      "Epoch 8/10\n",
      "65689/65689 [==============================] - 2616s 40ms/step - loss: 0.0843 - accuracy: 0.9678\n",
      "Epoch 9/10\n",
      "65689/65689 [==============================] - 1226s 19ms/step - loss: 0.0841 - accuracy: 0.9679\n",
      "Epoch 10/10\n",
      "65689/65689 [==============================] - 1137s 17ms/step - loss: 0.0838 - accuracy: 0.9680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input words and diacritics\n",
    "word_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters='')\n",
    "word_tokenizer.fit_on_texts(listOfwordsWith_NoDiacritics)\n",
    "word_sequences = word_tokenizer.texts_to_sequences(listOfwordsWith_NoDiacritics)\n",
    "\n",
    "diacritic_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters='')\n",
    "diacritic_tokenizer.fit_on_texts([''.join(d) for d in listofDiacritrcs_ToWord])\n",
    "diacritic_sequences = diacritic_tokenizer.texts_to_sequences([''.join(d) for d in listofDiacritrcs_ToWord])\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_len = max(max(len(seq) for seq in word_sequences), max(len(seq) for seq in diacritic_sequences))\n",
    "padded_word_sequences = pad_sequences(word_sequences, maxlen=max_len, padding='post')\n",
    "padded_diacritic_sequences = pad_sequences(diacritic_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=50, input_length=max_len))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dense(len(diacritic_tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_word_sequences, np.expand_dims(padded_diacritic_sequences, -1), epochs=10, batch_size=32)\n",
    "\n",
    "# Save the model for later use\n",
    "model.save(\"diacritic_prediction_model.h5\")\n",
    "\n",
    "# Now, you can use the trained model to predict diacritics for new Arabic words\n",
    "def predict_diacritics(model, word):\n",
    "    word_sequence = word_tokenizer.texts_to_sequences([word])\n",
    "    padded_word_sequence = pad_sequences(word_sequence, maxlen=max_len, padding='post')\n",
    "    predicted_diacritic_sequence = model.predict(padded_word_sequence)\n",
    "    predicted_diacritic_sequence = np.argmax(predicted_diacritic_sequence, axis=-1)\n",
    "    predicted_diacritic = diacritic_tokenizer.sequences_to_texts(predicted_diacritic_sequence)\n",
    "    return predicted_diacritic[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words 4\n",
      "1/1 [==============================] - 0s 363ms/step\n",
      "ذَهَبَ\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "عَلَيَّ\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "إلَى\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "الَّْشاِطِئ\n",
      "ذَهَبَ عَلَيَّ إلَى الَّْشاِطِئ \n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# input = \"الذي علم بالقلم\"\n",
    "input= \"ذهب علي إلى الشاطئ\"\n",
    "model = load_model(\"diacritic_prediction_model.h5\")\n",
    "\n",
    "# input = \"قال الزركشي ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر\"\n",
    "input_word = input.split(' ')\n",
    "word_vectors = [Word2Vec_model.wv[word] for word in input_word if word in Word2Vec_model.wv]\n",
    "# print(word_vectors)\n",
    "outputString=\"\"\n",
    "print(\"number of words\",len(input_word))\n",
    "for word in input_word: \n",
    "  predicted_diacritics1 = predict_diacritics(model, word)\n",
    "  #print(predicted_diacritics1.split(' '))\n",
    "  predictedDiacritics=(predicted_diacritics1.split('  '))\n",
    "  temp=list()\n",
    "  for diacrtic in predictedDiacritics:\n",
    "    # print(diacrtic)\n",
    "    # print(len(diacrtic))\n",
    "    if ' ' in diacrtic:\n",
    "      # print(\"after space removel\")\n",
    "      diacrtic=diacrtic.replace(' ','')\n",
    "      # print(diacrtic)\n",
    "      # print(len(diacrtic))\n",
    "      temp.append(diacrtic)\n",
    "      continue\n",
    "    temp.append(diacrtic)\n",
    "  predictedDiacritics=temp.copy()\n",
    "  tempString=\"\"\n",
    "  for i in range(len(word)):\n",
    "    tempString+=word[i]\n",
    "    if i<len(predictedDiacritics):\n",
    "      tempString+=predictedDiacritics[i]\n",
    "  print(tempString)\n",
    "  tempString+=\" \"\n",
    "  outputString+=tempString\n",
    "\n",
    "print(outputString)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
