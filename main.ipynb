{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Preprocessing :\n",
    "define characters accepted and tashkeel accepted and remove from the training set any tashkeel and unwanted characters (eg. brackets, numbers... etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pyarabic.araby as araby\n",
    "import nltk\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tashkeel = [\"ْ\", \"ّ\", \"ٌ\", \"ٍ\", \"ِ\", \"ً\", \"َ\", \"ُ\"]\n",
    "arabicCharacters = \"ىعظحرسيشضقثلصطكآماإهزءأفؤغجئدةخوبذتن\"\n",
    "arabicDictionary=['ى', 'ع', 'ظ', 'ح', 'ر', 'س', 'ي', 'ش', 'ض', 'ق', ' ', 'ث', 'ل', 'ص', 'ط', 'ك', 'آ', 'م', 'ا', 'إ', 'ه', 'ز', 'ء', 'أ', 'ف', 'ؤ', 'غ', 'ج', 'ئ', 'د', 'ة', 'خ', 'و', 'ب', 'ذ', 'ت', 'ن']\n",
    "punctuations = [\".\", \"،\", \":\", \"؛\", \"-\", \"؟\"]\n",
    "validCharacters= tashkeel + list(arabicCharacters) + punctuations\n",
    "charcters_without_tashkeel = list(arabicCharacters) + punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#reading the training dataset\n",
    "f = open(r\"train.txt\", \"r\",encoding=\"utf-8\").read()\n",
    "print(type(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18515271\n",
      "17997403\n"
     ]
    }
   ],
   "source": [
    "print(len(f))\n",
    "# arabic_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
    "#regex to keep arabic letters only and remove any other character (eg. brackets, numbers ...etc)\n",
    "characters_regex =r'[\\s\\.\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD]+'\n",
    "processedData = re.findall(characters_regex,f)\n",
    "processedData = \" \".join(processedData)\n",
    "processedData = re.sub(r\"\\s+\",\" \" ,processedData) #substitute many spaces with one space only\n",
    "# processedData = ' '.join(word for word in processedData.split() if word not in arabic_stopwords) # remove stopwors from text\n",
    "print(len(processedData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = open('processed.txt', 'w', encoding='utf-8')\n",
    "\n",
    "w.write(processedData)\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10711561\n"
     ]
    }
   ],
   "source": [
    "#remove diacritics\n",
    "without_diacritics= araby.strip_diacritics(processedData)\n",
    "print(len(without_diacritics))\n",
    "w2 = open('withoutDiacritics.txt', 'w', encoding='utf-8')\n",
    "\n",
    "w2.write(without_diacritics)\n",
    "w2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40836\n"
     ]
    }
   ],
   "source": [
    "#take the procrssed text and separate it into sentences\n",
    "fileRead=open('processed.txt', encoding='utf-8')\n",
    "lines=fileRead.readlines()\n",
    "longString=' '.join(lines)\n",
    "longStringSplited=re.sub(r\"\\n\", ' ', longString)\n",
    "#the dot itself got removed in the process\n",
    "longStringSplited=longStringSplited.split('.')\n",
    "print(len(longStringSplited))\n",
    "w2 = open('withoutDiacritics.txt', 'w', encoding='utf-8')\n",
    "\n",
    "w2.write(without_diacritics)\n",
    "\n",
    "w3 = open('processed_sentences_separated.txt', 'w', encoding='utf-8')\n",
    "for line in longStringSplited:\n",
    " w3.write(line)\n",
    "\n",
    " #now longStringSplited is the list of procrssed text without practices and numbers and dots \n",
    " #the rest of the punctuation still there  \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to create a list of words and a crossbonding list of punctuation for each word \n",
    "\n",
    "#first remove all puncturation \n",
    "\n",
    "fileRead=open('processed_sentences_separated.txt', encoding='utf-8')\n",
    "lines=fileRead.readlines()\n",
    "longString=' '.join(lines)\n",
    "longString=re.sub(r\"\\n\", ' ', longString)\n",
    "punctuations = [\"،\", \":\", \"؛\", \"-\", \"؟\"]\n",
    "for element in punctuations:\n",
    "     longString=re.sub(element, '', longString)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the varable called longString has a single string with all the processed words in it \n",
    "listOfwordsWith_Diacritics=list()\n",
    "listOfwordsWith_NoDiacritics=list()\n",
    "\n",
    "\n",
    "without_diacritics= araby.strip_diacritics(longString)\n",
    "\n",
    "\n",
    "without_diacritics=re.sub(r\"\\s+\", ' ', without_diacritics)\n",
    "listOfwordsWith_NoDiacritics=without_diacritics.split(\" \")\n",
    "\n",
    "\n",
    "\n",
    "listOfwordsWith_Diacritics=re.sub(r\"\\s+\", ' ', longString)\n",
    "listOfwordsWith_Diacritics=listOfwordsWith_Diacritics.split(\" \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قَوْلُهُ', 'أَوْ', 'قَطَعَ', 'الْأَوَّلُ', 'يَدَهُ', 'إلَخْ', 'قَالَ', 'الزَّرْكَشِيُّ', 'ابْنُ', 'عَرَفَةَ', 'قَوْلُهُ', 'بِلَفْظٍ', 'يَقْتَضِيه', 'كَإِنْكَارِ', 'غَيْرِ', 'حَدِيثٍ', 'بِالْإِسْلَامِ', 'وُجُوبَ', 'مَا', 'عُلِمَ', 'وُجُوبُهُ', 'مِنْ', 'الدِّينِ', 'ضَرُورَةً', 'كَإِلْقَاءِ', 'مُصْحَفٍ', 'بِقَذَرٍ', 'وَشَدِّ', 'زُنَّارٍ', 'ابْنُ', 'عَرَفَةَ', 'قَوْلُ', 'ابْنِ', 'شَاسٍ', 'أَوْ', 'بِفِعْلٍ', 'يَتَضَمَّنُهُ', 'هُوَ', 'كَلُبْسِ', 'الزُّنَّارِ', 'وَإِلْقَاءِ', 'الْمُصْحَفِ', 'فِي', 'صَرِيحِ', 'النَّجَاسَةِ', 'وَالسُّجُودِ', 'لِلصَّنَمِ', 'وَنَحْوِ', 'ذَلِكَ', 'وَسِحْرٍ', 'مُحَمَّدٌ', 'قَوْلُ', 'مَالِكٍ', 'وَأَصْحَابِهِ', 'أَنَّ', 'السَّاحِرَ', 'كَافِرٌ', 'بِاَللَّهِ', 'تَعَالَى', 'قَالَ', 'مَالِكٌ', 'هُوَ', 'كَالزِّنْدِيقِ', 'إذَا', 'عَمِلَ', 'السِّحْرَ', 'بِنَفْسِهِ', 'قُتِلَ', 'وَلَمْ', 'يُسْتَتَبْ', 'قَوْلُهُ', 'لِعَدَمِ', 'مَا', 'تَتَعَلَّقُ', 'إلَخْ', 'أَيْ', 'الْوَصِيَّةُ', 'قَوْلُهُ', 'مَا', 'مَرَّ', 'أَيْ', 'قُبَيْلَ', 'قَوْلِ', 'الْمَتْنِ', 'لَغَتْ', 'وَلَوْ', 'اقْتَصَرَ', 'عَلَى', 'أَوْصَيْت', 'لَهُ', 'بِشَاةٍ', 'أَوْ', 'أَعْطُوهُ', 'شَاةً', 'وَلَا', 'غَنَمَ', 'لَهُ', 'عِنْدَ', 'الْمَوْتِ', 'هَلْ']\n",
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة', 'قوله', 'بلفظ', 'يقتضيه', 'كإنكار', 'غير', 'حديث', 'بالإسلام', 'وجوب', 'ما', 'علم', 'وجوبه', 'من', 'الدين', 'ضرورة', 'كإلقاء', 'مصحف', 'بقذر', 'وشد', 'زنار', 'ابن', 'عرفة', 'قول', 'ابن', 'شاس', 'أو', 'بفعل', 'يتضمنه', 'هو', 'كلبس', 'الزنار', 'وإلقاء', 'المصحف', 'في', 'صريح', 'النجاسة', 'والسجود', 'للصنم', 'ونحو', 'ذلك', 'وسحر', 'محمد', 'قول', 'مالك', 'وأصحابه', 'أن', 'الساحر', 'كافر', 'بالله', 'تعالى', 'قال', 'مالك', 'هو', 'كالزنديق', 'إذا', 'عمل', 'السحر', 'بنفسه', 'قتل', 'ولم', 'يستتب', 'قوله', 'لعدم', 'ما', 'تتعلق', 'إلخ', 'أي', 'الوصية', 'قوله', 'ما', 'مر', 'أي', 'قبيل', 'قول', 'المتن', 'لغت', 'ولو', 'اقتصر', 'على', 'أوصيت', 'له', 'بشاة', 'أو', 'أعطوه', 'شاة', 'ولا', 'غنم', 'له', 'عند', 'الموت', 'هل']\n"
     ]
    }
   ],
   "source": [
    "#now that we have two separated lists we need to get the diacritics list \n",
    "\n",
    "\n",
    "listofDiacritrcs_ToWord=list()\n",
    "temp=list()\n",
    "counter=0\n",
    "flag=0\n",
    "for word in listOfwordsWith_Diacritics:\n",
    "    while counter<len(word):\n",
    "     if word[counter] in arabicDictionary: #checking if the character is a letter\n",
    "      if (counter+1)<len(word):\n",
    "        #checking if the next character is also a letter, then that means that the diacritics of the current letter is none so add empty string to the list\n",
    "        if word[counter +1] in arabicDictionary: \n",
    "          temp.append(\"\")\n",
    "          counter+=1\n",
    "          continue\n",
    "      counter+=1 #if it is the end of the word (no more letters) or the next character is a diacritics -> continue looping\n",
    "      continue\n",
    "     else:\n",
    "      if (counter+1)<len(word):\n",
    "        if word[(counter+1)] not in arabicDictionary: #if the current and the next characters are diacritics, add them together in the list\n",
    "          temp.append(word[counter]+word[counter+1])\n",
    "          counter+=2\n",
    "          continue\n",
    "      temp.append(word[counter]) #if the current character only is the diacritics add it to the list\n",
    "      counter+=1    \n",
    "    listofDiacritrcs_ToWord.append(temp.copy())     \n",
    "    temp.clear() \n",
    "    counter=0\n",
    "          \n",
    "          \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "print(listOfwordsWith_Diacritics[0:100])\n",
    "print(listOfwordsWith_NoDiacritics[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2102046\n",
      "الْأَوَّلُ\n",
      "الأول\n",
      "['', 'ْ', 'َ', 'َّ', 'ُ']\n",
      " ْ َ َّ ُ\n",
      "[['َ', 'ْ', 'ُ', 'ُ'], ['َ', 'ْ'], ['َ', 'َ', 'َ'], ['', 'ْ', 'َ', 'َّ', 'ُ'], ['َ', 'َ', 'ُ'], ['', 'َ', 'ْ'], ['َ', '', 'َ'], ['', '', 'َّ', 'ْ', 'َ', 'ِ', 'ُّ'], ['', 'ْ', 'ُ'], ['َ', 'َ', 'َ', 'َ'], ['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'ْ', 'ٍ'], ['َ', 'ْ', 'َ', 'ِ', ''], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['َ', 'ْ', 'ِ'], ['َ', 'ِ', '', 'ٍ'], ['ِ', '', 'ْ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['ُ', 'ُ', '', 'َ'], ['َ'], ['ُ', 'ِ', 'َ'], ['ُ', 'ُ', '', 'ُ', 'ُ'], ['ِ', 'ْ'], ['', '', 'ِّ', '', 'ِ'], ['َ', 'ُ', '', 'َ', 'ً'], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['ُ', 'ْ', 'َ', 'ٍ'], ['ِ', 'َ', 'َ', 'ٍ'], ['َ', 'َ', 'ِّ'], ['ُ', 'َّ', '', 'ٍ'], ['', 'ْ', 'ُ'], ['َ', 'َ', 'َ', 'َ'], ['َ', 'ْ', 'ُ'], ['', 'ْ', 'ِ'], ['َ', '', 'ٍ'], ['َ', 'ْ'], ['ِ', 'ِ', 'ْ', 'ٍ'], ['َ', 'َ', 'َ', 'َّ', 'ُ', 'ُ'], ['ُ', 'َ'], ['َ', 'ُ', 'ْ', 'ِ'], ['', '', 'ُّ', 'َّ', '', 'ِ'], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['', 'ْ', 'ُ', 'ْ', 'َ', 'ِ'], ['ِ'], ['َ', 'ِ', '', 'ِ'], ['', '', 'َّ', 'َ', '', 'َ', 'ِ'], ['َ', '', '', 'ُّ', 'ُ', '', 'ِ'], ['ِ', '', 'َّ', 'َ', 'ِ'], ['َ', 'َ', 'ْ', 'ِ'], ['َ', 'ِ', 'َ'], ['َ', 'ِ', 'ْ', 'ٍ'], ['ُ', 'َ', 'َّ', 'ٌ'], ['َ', 'ْ', 'ُ'], ['َ', '', 'ِ', 'ٍ'], ['َ', 'َ', 'ْ', 'َ', '', 'ِ', 'ِ'], ['َ', 'َّ'], ['', '', 'َّ', '', 'ِ', 'َ'], ['َ', '', 'ِ', 'ٌ'], ['ِ', 'َ', '', 'َّ', 'ِ'], ['َ', 'َ', '', 'َ'], ['َ', '', 'َ'], ['َ', '', 'ِ', 'ٌ'], ['ُ', 'َ'], ['َ', '', '', 'ِّ', 'ْ', 'ِ', '', 'ِ'], ['', 'َ'], ['َ', 'ِ', 'َ'], ['', '', 'ِّ', 'ْ', 'َ'], ['ِ', 'َ', 'ْ', 'ِ', 'ِ'], ['ُ', 'ِ', 'َ'], ['َ', 'َ', 'ْ'], ['ُ', 'ْ', 'َ', 'َ', 'ْ'], ['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'َ', 'ِ'], ['َ'], ['َ', 'َ', 'َ', 'َّ', 'ُ'], ['', 'َ', 'ْ'], ['َ', 'ْ'], ['', 'ْ', 'َ', 'ِ', 'َّ', 'ُ'], ['َ', 'ْ', 'ُ', 'ُ'], ['َ'], ['َ', 'َّ'], ['َ', 'ْ'], ['ُ', 'َ', 'ْ', 'َ'], ['َ', 'ْ', 'ِ'], ['', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', 'ْ'], ['َ', 'َ', 'ْ'], ['', 'ْ', 'َ', 'َ', 'َ'], ['َ', 'َ'], ['َ', 'ْ', 'َ', 'ْ'], ['َ', 'ُ'], ['ِ', 'َ', '', 'ٍ'], ['َ', 'ْ'], ['َ', 'ْ', 'ُ', '', 'ُ'], ['َ', '', 'ً'], ['َ', 'َ'], ['َ', 'َ', 'َ'], ['َ', 'ُ'], ['ِ', 'ْ', 'َ'], ['', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'ْ']]\n"
     ]
    }
   ],
   "source": [
    "print(len(listOfwordsWith_NoDiacritics))\n",
    "print(listOfwordsWith_Diacritics[3])\n",
    "print(listOfwordsWith_NoDiacritics[3])\n",
    "print(listofDiacritrcs_ToWord[3])\n",
    "print(\" \".join(listofDiacritrcs_ToWord[3]))\n",
    "print(listofDiacritrcs_ToWord[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-Building the LSTM model for character level classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2102046)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(listofDiacritrcs_ToWord)):\n",
    "        listofDiacritrcs_ToWord[i] = \" \".join(listofDiacritrcs_ToWord[i])\n",
    "# print(listofDiacritrcs_ToWord[0:100])\n",
    "Y_train= np.array([listofDiacritrcs_ToWord],dtype=object).T\n",
    "X_train = np.array([listOfwordsWith_NoDiacritics],dtype=object)\n",
    "print(str(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each sentence.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_28 (Embedding)    (None, 2102046, 100)      5000000   \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spati  (None, 2102046, 100)      0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " lstm_32 (LSTM)              (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5080804 (19.38 MB)\n",
      "Trainable params: 5080804 (19.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 5\n",
    "# batch_size = 64\n",
    "\n",
    "# history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding, LSTM, Dense\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Assuming you have a list of Arabic words and their diacritics\n",
    "# arabic_words =listOfwordsWith_NoDiacritics [0:50000] # List of Arabic words\n",
    "# diacritics = listofDiacritrcs_ToWord[0:50000] # List of corresponding diacritics\n",
    "\n",
    "# # Tokenization\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(arabic_words)\n",
    "# word_sequences = tokenizer.texts_to_sequences(arabic_words)\n",
    "# diacritic_sequences = tokenizer.texts_to_sequences(diacritics)\n",
    "\n",
    "# # Padding\n",
    "# max_sequence_length = max(len(seq) for seq in word_sequences)\n",
    "# word_sequences = pad_sequences(word_sequences, maxlen=max_sequence_length)\n",
    "# diacritic_sequences = pad_sequences(diacritic_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# # Model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=50, input_length=max_sequence_length))\n",
    "# model.add(LSTM(100, return_sequences=True))\n",
    "# model.add(Dense(len(tokenizer.word_index)+1, activation='softmax'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Training\n",
    "# model.fit(word_sequences, diacritic_sequences, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inference\n",
    "# new_word = \"العب\"\n",
    "# input_sequence = tokenizer.texts_to_sequences([new_word])\n",
    "# padded_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length)\n",
    "\n",
    "# # Predict diacritics\n",
    "# predicted_probabilities = model.predict(padded_sequence)\n",
    "# predicted_diacritics = predicted_probabilities.argmax(axis=-1)\n",
    "\n",
    "# # Decode diacritics and combine with the input word\n",
    "# # Decode diacritics and combine with the input word\n",
    "# decoded_diacritics = ''.join([tokenizer.index_word.get(int(idx), '') for idx in predicted_diacritics])\n",
    "# word_with_diacritics = new_word + decoded_diacritics\n",
    "# print(\"Word with Diacritics:\", word_with_diacritics)\n",
    "# # Print raw predicted probabilities\n",
    "# print(\"Raw Predicted Probabilities:\", predicted_probabilities)\n",
    "# # Print tokenizer's index_word mapping\n",
    "# print(\"Tokenizer Index to Word Mapping:\", tokenizer.index_word)\n",
    "# decoded_diacritics = ' '.join([tokenizer.index_word.get(int(idx), '') for idx in predicted_diacritics[0]])\n",
    "# print(\"Diacritics:\", decoded_diacritics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.4741 - accuracy: 0.8463\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.2503 - accuracy: 0.9140\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2073 - accuracy: 0.9291\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1856 - accuracy: 0.9354\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1714 - accuracy: 0.9394\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.1618 - accuracy: 0.9422\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1540 - accuracy: 0.9444\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1477 - accuracy: 0.9464\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1426 - accuracy: 0.9481\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.1382 - accuracy: 0.9494\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the input words and diacritics\n",
    "word_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters='')\n",
    "word_tokenizer.fit_on_texts(listOfwordsWith_NoDiacritics[0:50000])\n",
    "word_sequences = word_tokenizer.texts_to_sequences(listOfwordsWith_NoDiacritics[0:50000])\n",
    "\n",
    "diacritic_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters='')\n",
    "diacritic_tokenizer.fit_on_texts([''.join(d) for d in listofDiacritrcs_ToWord[0:50000]])\n",
    "diacritic_sequences = diacritic_tokenizer.texts_to_sequences([''.join(d) for d in listofDiacritrcs_ToWord[0:50000]])\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_len = max(max(len(seq) for seq in word_sequences), max(len(seq) for seq in diacritic_sequences))\n",
    "padded_word_sequences = pad_sequences(word_sequences, maxlen=max_len, padding='post')\n",
    "padded_diacritic_sequences = pad_sequences(diacritic_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=50, input_length=max_len))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dense(len(diacritic_tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_word_sequences, np.expand_dims(padded_diacritic_sequences, -1), epochs=10, batch_size=32)\n",
    "\n",
    "# Save the model for later use\n",
    "model.save(\"diacritic_prediction_model.h5\")\n",
    "\n",
    "# Now, you can use the trained model to predict diacritics for new Arabic words\n",
    "def predict_diacritics(model, word):\n",
    "    word_sequence = word_tokenizer.texts_to_sequences([word])\n",
    "    padded_word_sequence = pad_sequences(word_sequence, maxlen=max_len, padding='post')\n",
    "    predicted_diacritic_sequence = model.predict(padded_word_sequence)\n",
    "    predicted_diacritic_sequence = np.argmax(predicted_diacritic_sequence, axis=-1)\n",
    "    predicted_diacritic = diacritic_tokenizer.sequences_to_texts(predicted_diacritic_sequence)\n",
    "    return predicted_diacritic[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "Input word: ذهب\n",
      "Predicted diacritics: َ   َ   َ\n",
      "َ\n",
      "َ\n",
      "َ\n",
      "ذَهَبَ\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Input word: علي\n",
      "Predicted diacritics: َ   َ   ّ َ\n",
      "َ\n",
      "َ\n",
      "ّ\n",
      "here\n",
      "عَلَيّ \n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Input word: إلى\n",
      "Predicted diacritics:   َ\n",
      "َ\n",
      "إَ\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Input word: الشاطئ\n",
      "Predicted diacritics:   ْ ّ َ     ِ   ِ\n",
      "ْ\n",
      "ّ\n",
      "here\n",
      "ِ\n",
      "ِ\n",
      "اْلّ شِاِ\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_word = \"ذهب علي إلى الشاطئ\"\n",
    "input_word=input_word.split(' ')\n",
    "\n",
    "for word in input_word: \n",
    "  predicted_diacritics1 = predict_diacritics(model, word)\n",
    "  print(f\"Input word: {word}\")\n",
    "  print(f\"Predicted diacritics: {predicted_diacritics1}\")\n",
    "  tempString=str()\n",
    "  counterWord=0\n",
    "  counterDiacrtic=0\n",
    "  while counterDiacrtic<(len(predicted_diacritics1)):\n",
    "    if predicted_diacritics1[counterDiacrtic]!=' ':\n",
    "      tempString=tempString+word[counterWord]\n",
    "      tempString=tempString+predicted_diacritics1[counterDiacrtic]\n",
    "      print(predicted_diacritics1[counterDiacrtic])\n",
    "      if predicted_diacritics1[counterDiacrtic] == 'ّ':\n",
    "        print(\"here\")\n",
    "        tempString=tempString+predicted_diacritics1[counterDiacrtic+2]\n",
    "        counterDiacrtic=counterDiacrtic+2\n",
    "      counterWord+=1\n",
    "    counterDiacrtic+=1\n",
    "\n",
    "  print(tempString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
