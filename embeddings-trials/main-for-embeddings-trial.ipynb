{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Preprocessing :\n",
    "define characters accepted and tashkeel accepted and remove from the training set any tashkeel and unwanted characters (eg. brackets, numbers... etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\MahyDolphin\\College_Stuff\\senior2\\NLP\\project\\Code\\Arabic-Diacritization\\nlp-env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pyarabic.araby as araby\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from gensim.models import Word2Vec\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "# from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "18515271\n",
      "ق\n"
     ]
    }
   ],
   "source": [
    "arabicDictionary=['ى', 'ع', 'ظ', 'ح', 'ر', 'س', 'ي', 'ش', 'ض', 'ق', ' ', 'ث', 'ل', 'ص', 'ط', 'ك', 'آ', 'م', 'ا', 'إ', 'ه', 'ز', 'ء', 'أ', 'ف', 'ؤ', 'غ', 'ج', 'ئ', 'د', 'ة', 'خ', 'و', 'ب', 'ذ', 'ت', 'ن']\n",
    "punctuations = [\"،\", \":\", \"؛\", \"-\", \"؟\"]\n",
    "#reading the training dataset\n",
    "f = open(r\"train.txt\", \"r\",encoding=\"utf-8\").read()\n",
    "print(type(f))\n",
    "print(len(f))\n",
    "print(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataFromNonArabicLetters(data):\n",
    "    #regex to keep arabic letters only and remove any other character (eg. brackets, numbers ...etc)\n",
    "    characters_regex =r'[\\s\\.\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD]+'\n",
    "    processedData = re.findall(characters_regex,data)\n",
    "    processedData = \" \".join(processedData)\n",
    "    processedData = re.sub(r\"\\s+\",\" \" ,processedData) #substitute many spaces with one space only\n",
    "    return processedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17997403\n",
      "َ\n"
     ]
    }
   ],
   "source": [
    "processedData=cleanDataFromNonArabicLetters(f)\n",
    "print(len(processedData))\n",
    "print(processedData[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDiacratics(processedData):\n",
    "    without_diacritics= araby.strip_diacritics(processedData)\n",
    "    return without_diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10711561\n",
      "و\n"
     ]
    }
   ],
   "source": [
    "without_diacritics = removeDiacratics(processedData)\n",
    "print(len(without_diacritics))\n",
    "print(without_diacritics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the procrssed text and separate it into sentences\n",
    "def generateListsWithDiacraticsandWithout(with_diacratics, without_diacritics):\n",
    "    #---------------------Preprocessing of words with diacratics------------\n",
    "\n",
    "    #generate longStringSplited which is the list of procrssed text without brackets and numbers and dots \n",
    "    #the rest of the punctuation still there\n",
    "    longStringWithDecimalPoint=re.sub(r\"\\n\", ' ', with_diacratics) #string of data with diacratics removed endlines from it\n",
    "    for element in punctuations:\n",
    "        longStringWithDecimalPoint=re.sub(element, '', longStringWithDecimalPoint) #remove punctuations from the string\n",
    "\n",
    "    longStringSplited=longStringWithDecimalPoint.split('.') #split the string by dots (segment sentences) and make list of them\n",
    "    longString=' '.join(longStringSplited)  #long string without decimal points\n",
    "\n",
    "    #-------------------------------------\n",
    "\n",
    "    #---------------------Preprocessing of words without diacratics------------\n",
    "    without_diacratics_longStringWithDecimals=re.sub(r\"\\n\", ' ', without_diacritics) #string of data without diacratics removed endlines from it\n",
    "    for element in punctuations:\n",
    "        without_diacratics_longStringWithDecimals=re.sub(element, '', without_diacratics_longStringWithDecimals) #remove punctuations from the string\n",
    "\n",
    "    longStringSplited_withoutDiacratics=without_diacratics_longStringWithDecimals.split('.') #split the string by dots (segment sentences) and make list of them\n",
    "    longString_withoutDiacratics=' '.join(longStringSplited_withoutDiacratics)  #long string without decimal points\n",
    "\n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "    list_of_sentences=[]\n",
    "    for line in longStringSplited_withoutDiacratics: #list of lists of sentences splitted in words without diacratics (used in embeddings)\n",
    "        list_of_sentences.append(line.split(\" \"))\n",
    "    \n",
    "\n",
    "    # now the variable called longString has a single string with all the processed words in it \n",
    "    listOfwordsWith_Diacritics=list()\n",
    "    listOfwordsWith_NoDiacritics=list()\n",
    "\n",
    "    listOfwordsWith_Diacritics=re.sub(r\"\\s+\", ' ', longString)\n",
    "    listOfwordsWith_Diacritics=listOfwordsWith_Diacritics.split(\" \")\n",
    "\n",
    "    listOfwordsWith_NoDiacritics=re.sub(r\"\\s+\", ' ', longString_withoutDiacratics)\n",
    "    listOfwordsWith_NoDiacritics=listOfwordsWith_NoDiacritics.split(\" \")\n",
    "\n",
    "    return list_of_sentences,listOfwordsWith_Diacritics,listOfwordsWith_NoDiacritics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences,listOfwordsWith_Diacritics,listOfwordsWith_NoDiacritics= generateListsWithDiacraticsandWithout(processedData, without_diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40836\n",
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة', 'قوله', 'بلفظ', 'يقتضيه', 'كإنكار', 'غير', 'حديث', 'بالإسلام', 'وجوب', 'ما', 'علم', 'وجوبه', 'من', 'الدين', 'ضرورة', 'كإلقاء', 'مصحف', 'بقذر', 'وشد', 'زنار', 'ابن', 'عرفة', 'قول', 'ابن', 'شاس', 'أو', 'بفعل', 'يتضمنه', 'هو', 'كلبس', 'الزنار', 'وإلقاء', 'المصحف', 'في', 'صريح', 'النجاسة', 'والسجود', 'للصنم', 'ونحو', 'ذلك', 'وسحر', 'محمد', 'قول', 'مالك', 'وأصحابه', 'أن', 'الساحر', 'كافر', 'بالله', 'تعالى', 'قال', 'مالك', 'هو', 'كالزنديق', 'إذا', 'عمل', 'السحر', 'بنفسه', 'قتل', 'ولم', 'يستتب', '']\n",
      "2102054\n",
      "2102054\n",
      "['أَوْ', 'سَيِّدُهَا', 'فِي', 'ذَلِكَ', 'جَازَ', 'لِأَنَّ', 'لَهُ', 'غَرَضًا', 'فِي', 'تَزَيُّنِهَا', 'لَهُ', 'كَمَا', 'فِي', 'الرَّوْضَةِ', 'وَهُوَ', 'الْأَوْجَهُ', 'وَإِنْ', 'جَرَى', 'فِي', 'التَّحْقِيقِ', 'عَلَى', 'خِلَافِ', 'ذَلِكَ', 'فِي', 'الْوَصْلِ', 'وَالْوَشْرِ', 'فَأَلْحَقَهُمَا', 'بِالْوَشْمِ', 'فِي', 'الْمَنْعِ', 'مُطْلَقًا', 'وَيُكْرَهُ', 'أَنْ', 'يَنْتِفَ', 'الشَّيْبَ', 'مِنْ', 'الْمَحَلِّ', 'الَّذِي', 'لَا', 'يُطْلَبُ', 'مِنْهُ', 'إزَالَةُ', 'شَعْرِهِ', 'وَيُسَنُّ', 'خَضْبُهُ', 'بِالْحِنَّاءِ', 'وَنَحْوِهِ', 'وَيُسَنُّ', 'لِلْمَرْأَةِ', 'الْمُزَوَّجَةِ', 'وَالْمَمْلُوكَةِ', 'خَضْبُ', 'كَفِّهَا', 'وَقَدَمِهَا', 'بِذَلِكَ', 'تَعْمِيمًا', 'لِأَنَّهُ', 'زِينَةٌ', 'وَهِيَ', 'مَطْلُوبَةٌ', 'مِنْهَا', 'لِحَلِيلِهَا', 'أَمَّا', 'النَّقْشُ', 'وَالتَّطْرِيفُ', 'فَلَا', 'يُسَنُّ', 'وَخَرَجَ', 'بِالْمُزَوَّجَةِ', 'وَالْمَمْلُوكَةِ', 'غَيْرُهُمَا', 'فَيُكْرَهُ', 'لَهُ', 'وَبِالْمَرْأَةِ', 'الرَّجُلُ', 'وَالْخُنْثَى', 'فَيَحْرُمُ', 'الْخِضَابُ', 'عَلَيْهِمَا', 'إلَّا', 'لِعُذْرٍ', 'نِهَايَةٌ', 'وَمُغْنِي', 'قَالَ', 'ع', 'ش', 'قَوْلُهُ', 'م', 'ر', 'وَيَحْرُمُ', 'عَلَى', 'الْمَرْأَةِ', 'خَرَجَ', 'بِالْمَرْأَةِ', 'غَيْرُهَا', 'مِنْ', 'ذَكَرٍ', 'وَأُنْثَى', 'صَغِيرَيْنِ', 'فَيَجُوزُ']\n",
      "['أو', 'سيدها', 'في', 'ذلك', 'جاز', 'لأن', 'له', 'غرضا', 'في', 'تزينها', 'له', 'كما', 'في', 'الروضة', 'وهو', 'الأوجه', 'وإن', 'جرى', 'في', 'التحقيق', 'على', 'خلاف', 'ذلك', 'في', 'الوصل', 'والوشر', 'فألحقهما', 'بالوشم', 'في', 'المنع', 'مطلقا', 'ويكره', 'أن', 'ينتف', 'الشيب', 'من', 'المحل', 'الذي', 'لا', 'يطلب', 'منه', 'إزالة', 'شعره', 'ويسن', 'خضبه', 'بالحناء', 'ونحوه', 'ويسن', 'للمرأة', 'المزوجة', 'والمملوكة', 'خضب', 'كفها', 'وقدمها', 'بذلك', 'تعميما', 'لأنه', 'زينة', 'وهي', 'مطلوبة', 'منها', 'لحليلها', 'أما', 'النقش', 'والتطريف', 'فلا', 'يسن', 'وخرج', 'بالمزوجة', 'والمملوكة', 'غيرهما', 'فيكره', 'له', 'وبالمرأة', 'الرجل', 'والخنثى', 'فيحرم', 'الخضاب', 'عليهما', 'إلا', 'لعذر', 'نهاية', 'ومغني', 'قال', 'ع', 'ش', 'قوله', 'م', 'ر', 'ويحرم', 'على', 'المرأة', 'خرج', 'بالمرأة', 'غيرها', 'من', 'ذكر', 'وأنثى', 'صغيرين', 'فيجوز']\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_sentences))\n",
    "print(list_of_sentences[0])\n",
    "print(len(listOfwordsWith_NoDiacritics))\n",
    "print(len(listOfwordsWith_Diacritics))\n",
    "print(listOfwordsWith_Diacritics[2101000:2101100])\n",
    "print(listOfwordsWith_NoDiacritics[2101000:2101100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDiacraticsList(listOfwordsWith_Diacritics):\n",
    "    #now that we have two separated lists we need to get the diacritics list \n",
    "\n",
    "\n",
    "    listofDiacritrcs_ToWord=list()\n",
    "    temp=list()\n",
    "    counter=0\n",
    "    for word in listOfwordsWith_Diacritics:\n",
    "        while counter<len(word):\n",
    "            if word[counter] in arabicDictionary: #checking if the character is a letter\n",
    "                if (counter+1)<len(word):\n",
    "                    #checking if the next character is also a letter, then that means that the diacritics of the current letter is none so add empty string to the list\n",
    "                    if word[counter +1] in arabicDictionary: \n",
    "                        temp.append(\"\")\n",
    "                        counter+=1\n",
    "                        continue\n",
    "                counter+=1 #if it is the end of the word (no more letters) or the next character is a diacritics -> continue looping\n",
    "                continue\n",
    "            else:\n",
    "                if (counter+1)<len(word):\n",
    "                    if word[(counter+1)] not in arabicDictionary: #if the current and the next characters are diacritics, add them together in the list\n",
    "                        temp.append(word[counter]+word[counter+1])\n",
    "                        counter+=2\n",
    "                        continue\n",
    "                temp.append(word[counter]) #if the current character only is the diacritics add it to the list\n",
    "                counter+=1    \n",
    "        listofDiacritrcs_ToWord.append(temp.copy())     \n",
    "        temp.clear() \n",
    "        counter=0\n",
    "    return listofDiacritrcs_ToWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofDiacritrcs_ToWord= generateDiacraticsList(listOfwordsWith_Diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2102054\n",
      "الْأَوَّلُ\n",
      "الأول\n",
      "['', 'ْ', 'َ', 'َّ', 'ُ']\n",
      " ْ َ َّ ُ\n",
      "[['َ', 'ْ', 'ُ', 'ُ'], ['َ', 'ْ'], ['َ', 'َ', 'َ'], ['', 'ْ', 'َ', 'َّ', 'ُ'], ['َ', 'َ', 'ُ'], ['', 'َ', 'ْ'], ['َ', '', 'َ'], ['', '', 'َّ', 'ْ', 'َ', 'ِ', 'ُّ'], ['', 'ْ', 'ُ'], ['َ', 'َ', 'َ', 'َ'], ['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'ْ', 'ٍ'], ['َ', 'ْ', 'َ', 'ِ', ''], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['َ', 'ْ', 'ِ'], ['َ', 'ِ', '', 'ٍ'], ['ِ', '', 'ْ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['ُ', 'ُ', '', 'َ'], ['َ'], ['ُ', 'ِ', 'َ'], ['ُ', 'ُ', '', 'ُ', 'ُ'], ['ِ', 'ْ'], ['', '', 'ِّ', '', 'ِ'], ['َ', 'ُ', '', 'َ', 'ً'], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['ُ', 'ْ', 'َ', 'ٍ'], ['ِ', 'َ', 'َ', 'ٍ'], ['َ', 'َ', 'ِّ'], ['ُ', 'َّ', '', 'ٍ'], ['', 'ْ', 'ُ'], ['َ', 'َ', 'َ', 'َ'], ['َ', 'ْ', 'ُ'], ['', 'ْ', 'ِ'], ['َ', '', 'ٍ'], ['َ', 'ْ'], ['ِ', 'ِ', 'ْ', 'ٍ'], ['َ', 'َ', 'َ', 'َّ', 'ُ', 'ُ'], ['ُ', 'َ'], ['َ', 'ُ', 'ْ', 'ِ'], ['', '', 'ُّ', 'َّ', '', 'ِ'], ['َ', 'ِ', 'ْ', 'َ', '', 'ِ'], ['', 'ْ', 'ُ', 'ْ', 'َ', 'ِ'], ['ِ'], ['َ', 'ِ', '', 'ِ'], ['', '', 'َّ', 'َ', '', 'َ', 'ِ'], ['َ', '', '', 'ُّ', 'ُ', '', 'ِ'], ['ِ', '', 'َّ', 'َ', 'ِ'], ['َ', 'َ', 'ْ', 'ِ'], ['َ', 'ِ', 'َ'], ['َ', 'ِ', 'ْ', 'ٍ'], ['ُ', 'َ', 'َّ', 'ٌ'], ['َ', 'ْ', 'ُ'], ['َ', '', 'ِ', 'ٍ'], ['َ', 'َ', 'ْ', 'َ', '', 'ِ', 'ِ'], ['َ', 'َّ'], ['', '', 'َّ', '', 'ِ', 'َ'], ['َ', '', 'ِ', 'ٌ'], ['ِ', 'َ', '', 'َّ', 'ِ'], ['َ', 'َ', '', 'َ'], ['َ', '', 'َ'], ['َ', '', 'ِ', 'ٌ'], ['ُ', 'َ'], ['َ', '', '', 'ِّ', 'ْ', 'ِ', '', 'ِ'], ['', 'َ'], ['َ', 'ِ', 'َ'], ['', '', 'ِّ', 'ْ', 'َ'], ['ِ', 'َ', 'ْ', 'ِ', 'ِ'], ['ُ', 'ِ', 'َ'], ['َ', 'َ', 'ْ'], ['ُ', 'ْ', 'َ', 'َ', 'ْ'], ['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'َ', 'ِ'], ['َ'], ['َ', 'َ', 'َ', 'َّ', 'ُ'], ['', 'َ', 'ْ'], ['َ', 'ْ'], ['', 'ْ', 'َ', 'ِ', 'َّ', 'ُ'], ['َ', 'ْ', 'ُ', 'ُ'], ['َ'], ['َ', 'َّ'], ['َ', 'ْ'], ['ُ', 'َ', 'ْ', 'َ'], ['َ', 'ْ', 'ِ'], ['', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', 'ْ'], ['َ', 'َ', 'ْ'], ['', 'ْ', 'َ', 'َ', 'َ'], ['َ', 'َ'], ['َ', 'ْ', 'َ', 'ْ'], ['َ', 'ُ'], ['ِ', 'َ', '', 'ٍ'], ['َ', 'ْ'], ['َ', 'ْ', 'ُ', '', 'ُ'], ['َ', '', 'ً'], ['َ', 'َ'], ['َ', 'َ', 'َ'], ['َ', 'ُ'], ['ِ', 'ْ', 'َ'], ['', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'ْ']]\n"
     ]
    }
   ],
   "source": [
    "print(len(listofDiacritrcs_ToWord))\n",
    "print(listOfwordsWith_Diacritics[3])\n",
    "print(listOfwordsWith_NoDiacritics[3])\n",
    "print(listofDiacritrcs_ToWord[3])\n",
    "print(\" \".join(listofDiacritrcs_ToWord[3]))\n",
    "print(listofDiacritrcs_ToWord[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  TFBertModel, BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قوله أو قطع الأول يده إلخ قال الزركشي ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر وشد زنار ابن عرفة قول ابن شاس أو بفعل يتضمنه هو كلبس الزنار وإلقاء المصحف في صريح النجاسة والسجود للصنم ونحو ذلك وسحر محمد قول مالك وأصحابه أن الساحر كافر بالله تعالى قال مالك هو كالزنديق إذا عمل السحر بنفسه قتل ولم يستتب \n",
      " قوله لعدم ما تتعلق إلخ أي الوصية قوله ما مر أي قبيل قول المتن لغت ولو اقتصر على أوصيت له بشاة أو أعطوه شاة ولا غنم له عند الموت هل تبطل الوصية أو يشترى له شاة ويؤخذ من قوله الآتي كما لو لم يقل من مالي ولا من غنمي أنها لا تبطل  وعبارة الكنز ولو لم يقل من مالي ولا من غنمي لم يتعين غنمه إن كانت انتهت ا ه سم قوله فيعطى واحدة منها إلخ كما لو كانت موجودة عند الوصية والموت  ولا يجوز أن يعطى واحدة من غير غنمه في الصورتين وإن تراضيا  لأنه صلح على مجهول مغني ونهاية قال ع ش قوله واحدة منها أي كاملة  ولا يجوز أن يعطى نصفين من شاتين  لأنه لا يسمى شاة وقوله ولا يجوز أن يعطى واحدة من غير غنمه وينبغي أن يقال مثل ذلك في الأرقاء ا ه \n"
     ]
    }
   ],
   "source": [
    "list_of_string_sentences=[]\n",
    "for i,sentence in enumerate(list_of_sentences):\n",
    "    list_of_words = list_of_sentences[i]\n",
    "    sentence_as_string = \" \".join(list_of_words)\n",
    "    list_of_string_sentences.append(sentence_as_string)\n",
    "print(list_of_string_sentences[0])\n",
    "print(list_of_string_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#we should get sentence [each sentence is a sequence, should have max of 512 tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ق', '##و', '##ل', '##ه', 'ا', '##و', 'ق', '##ط', '##ع', 'ا', '##ل', '##ا', '##و', '##ل', 'ي', '##د', '##ه', 'ا', '##ل', '##خ', 'ق', '##ا', '##ل', 'ا', '##ل', '##ز', '##ر', '##ك', '##ش', '##ي', 'ا', '##ب', '##ن', 'ع', '##ر', '##ف', '##ة', 'ق', '##و', '##ل', '##ه', 'ب', '##ل', '##ف', '##ظ', 'ي', '##ق', '##ت', '##ض', '##ي', '##ه', 'ك', '##ان', '##ك', '##ا', '##ر', 'غ', '##ي', '##ر', 'ح', '##د', '##ي', '##ث', 'ب', '##ا', '##ل', '##ا', '##س', '##ل', '##ا', '##م', 'و', '##ج', '##و', '##ب', 'م', '##ا', 'ع', '##ل', '##م', 'و', '##ج', '##و', '##ب', '##ه', 'م', '##ن', 'ا', '##ل', '##د', '##ي', '##ن', 'ض', '##ر', '##و', '##ر', '##ة', 'ك', '##ا', '##ل', '##ق', '##ا', '##ء', 'م', '##ص', '##ح', '##ف', 'ب', '##ق', '##ذ', '##ر', 'و', '##ش', '##د', 'ز', '##ن', '##ا', '##ر', 'ا', '##ب', '##ن', 'ع', '##ر', '##ف', '##ة', 'ق', '##و', '##ل', 'ا', '##ب', '##ن', 'ش', '##ا', '##س', 'ا', '##و', 'ب', '##ف', '##ع', '##ل', 'ي', '##ت', '##ض', '##م', '##ن', '##ه', 'ه', '##و', 'ك', '##ل', '##ب', '##س', 'ا', '##ل', '##ز', '##ن', '##ا', '##ر', 'و', '##ا', '##ل', '##ق', '##ا', '##ء', 'ا', '##ل', '##م', '##ص', '##ح', '##ف', 'ف', '##ي', 'ص', '##ر', '##ي', '##ح', 'ا', '##ل', '##ن', '##ج', '##ا', '##س', '##ة', 'و', '##ا', '##ل', '##س', '##ج', '##و', '##د', 'ل', '##ل', '##ص', '##ن', '##م', 'و', '##ن', '##ح', '##و', 'ذ', '##ل', '##ك', 'و', '##س', '##ح', '##ر', 'م', '##ح', '##م', '##د', 'ق', '##و', '##ل', 'م', '##ا', '##ل', '##ك', 'و', '##ا', '##ص', '##ح', '##ا', '##ب', '##ه', 'ا', '##ن', 'ا', '##ل', '##س', '##ا', '##ح', '##ر', 'ك', '##ا', '##ف', '##ر', 'ب', '##ا', '##ل', '##ل', '##ه', 'ت', '##ع', '##ا', '##ل', '##ى', 'ق', '##ا', '##ل', 'م', '##ا', '##ل', '##ك', 'ه', '##و', 'ك', '##ا', '##ل', '##ز', '##ن', '##د', '##ي', '##ق', 'ا', '##ذ', '##ا', 'ع', '##م', '##ل', 'ا', '##ل', '##س', '##ح', '##ر', 'ب', '##ن', '##ف', '##س', '##ه', 'ق', '##ت', '##ل', 'و', '##ل', '##م', 'ي', '##س', '##ت', '##ت', '##ب', '[SEP]']\n",
      "['[CLS]', 'ق', '##و', '##ل', '##ه', 'ل', '##ع', '##د', '##م', 'م', '##ا', 'ت', '##ت', '##ع', '##ل', '##ق', 'ا', '##ل', '##خ', 'ا', '##ي', 'ا', '##ل', '##و', '##ص', '##ي', '##ة', 'ق', '##و', '##ل', '##ه', 'م', '##ا', 'م', '##ر', 'ا', '##ي', 'ق', '##ب', '##ي', '##ل', 'ق', '##و', '##ل', 'ا', '##ل', '##م', '##ت', '##ن', 'ل', '##غ', '##ت', 'و', '##ل', '##و', 'ا', '##ق', '##ت', '##ص', '##ر', 'ع', '##ل', '##ى', 'ا', '##و', '##ص', '##ي', '##ت', 'ل', '##ه', 'ب', '##ش', '##ا', '##ة', 'ا', '##و', 'ا', '##ع', '##ط', '##و', '##ه', 'ش', '##ا', '##ة', 'و', '##ل', '##ا', 'غ', '##ن', '##م', 'ل', '##ه', 'ع', '##ن', '##د', 'ا', '##ل', '##م', '##و', '##ت', 'ه', '##ل', 'ت', '##ب', '##ط', '##ل', 'ا', '##ل', '##و', '##ص', '##ي', '##ة', 'ا', '##و', 'ي', '##ش', '##ت', '##ر', '##ى', 'ل', '##ه', 'ش', '##ا', '##ة', 'و', '##ي', '##و', '##خ', '##ذ', 'م', '##ن', 'ق', '##و', '##ل', '##ه', 'ا', '##ل', '##ا', '##ت', '##ي', 'ك', '##م', '##ا', 'ل', '##و', 'ل', '##م', 'ي', '##ق', '##ل', 'م', '##ن', 'م', '##ا', '##ل', '##ي', 'و', '##ل', '##ا', 'م', '##ن', 'غ', '##ن', '##م', '##ي', 'ا', '##ن', '##ه', '##ا', 'ل', '##ا', 'ت', '##ب', '##ط', '##ل', 'و', '##ع', '##ب', '##ا', '##ر', '##ة', 'ا', '##ل', '##ك', '##ن', '##ز', 'و', '##ل', '##و', 'ل', '##م', 'ي', '##ق', '##ل', 'م', '##ن', 'م', '##ا', '##ل', '##ي', 'و', '##ل', '##ا', 'م', '##ن', 'غ', '##ن', '##م', '##ي', 'ل', '##م', 'ي', '##ت', '##ع', '##ي', '##ن', 'غ', '##ن', '##م', '##ه', 'ا', '##ن', 'ك', '##ان', '##ت', 'ا', '##ن', '##ت', '##ه', '##ت', 'ا', 'ه', 'س', '##م', 'ق', '##و', '##ل', '##ه', 'ف', '##ي', '##ع', '##ط', '##ى', 'و', '##ا', '##ح', '##د', '##ة', 'م', '##ن', '##ه', '##ا', 'ا', '##ل', '##خ', 'ك', '##م', '##ا', 'ل', '##و', 'ك', '##ان', '##ت', 'م', '##و', '##ج', '##و', '##د', '##ة', 'ع', '##ن', '##د', 'ا', '##ل', '##و', '##ص', '##ي', '##ة', 'و', '##ا', '##ل', '##م', '##و', '##ت', 'و', '##ل', '##ا', 'ي', '##ج', '##و', '##ز', 'ا', '##ن', 'ي', '##ع', '##ط', '##ى', 'و', '##ا', '##ح', '##د', '##ة', 'م', '##ن', 'غ', '##ي', '##ر', 'غ', '##ن', '##م', '##ه', 'ف', '##ي', 'ا', '##ل', '##ص', '##و', '##ر', '##ت', '##ي', '##ن', 'و', '##ان', 'ت', '##ر', '##ا', '##ض', '##ي', '##ا', 'ل', '##ان', '##ه', 'ص', '##ل', '##ح', 'ع', '##ل', '##ى', 'م', '##ج', '##ه', '##و', '##ل', 'م', '##غ', '##ن', '##ي', 'و', '##ن', '##ه', '##ا', '##ي', '##ة', 'ق', '##ا', '##ل', 'ع', 'ش', 'ق', '##و', '##ل', '##ه', 'و', '##ا', '##ح', '##د', '##ة', 'م', '##ن', '##ه', '##ا', 'ا', '##ي', 'ك', '##ا', '##م', '##ل', '##ة', 'و', '##ل', '##ا', 'ي', '##ج', '##و', '##ز', 'ا', '##ن', 'ي', '##ع', '##ط', '##ى', 'ن', '##ص', '##ف', '##ي', '##ن', 'م', '##ن', 'ش', '##ا', '##ت', '##ي', '##ن', 'ل', '##ان', '##ه', 'ل', '##ا', 'ي', '##س', '##م', '##ى', 'ش', '##ا', '##ة', 'و', '##ق', '##و', '##ل', '##ه', 'و', '##ل', '##ا', 'ي', '##ج', '##و', '##ز', 'ا', '##ن', 'ي', '##ع', '##ط', '##ى', 'و', '##ا', '##ح', '##د', '##ة', 'م', '##ن', 'غ', '##ي', '##ر', 'غ', '##ن', '##م', '##ه', 'و', '##ي', '##ن', '##ب', '##غ', '##ي', 'ا', '##ن', 'ي', '##ق', '##ا', '##ل', 'م', '##ث', '##ل', 'ذ', '##ل', '##ك', 'ف', '##ي', 'ا', '##ل', '##ا', '##ر', '##ق', '##ا', '##ء', 'ا', 'ه', '[SEP]']\n",
      "2736\n"
     ]
    }
   ],
   "source": [
    "tokens_for_all_sentences=[]\n",
    "max_token_len=0\n",
    "for sent in list_of_string_sentences:\n",
    "    tokens_in_sent=tokenizer.tokenize(sent)\n",
    "    tokens_in_sent = ['[CLS]'] + tokens_in_sent + ['[SEP]']\n",
    "    # print(tokens_in_sent)\n",
    "    # print(len(tokens_in_sent))\n",
    "    max_token_len=max(len(tokens_in_sent),max_token_len)\n",
    "    tokens_for_all_sentences.append(tokens_in_sent)\n",
    "\n",
    "print(tokens_for_all_sentences[0])\n",
    "print(tokens_for_all_sentences[1])\n",
    "print(max_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n"
     ]
    }
   ],
   "source": [
    "tokens = tokens + ['[PAD]'] + ['[PAD]']   # to make a fixed size -- dk yet\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1292, 29836, 23673, 14157, 1270, 29836, 1292, 29828, 29830, 1270, 23673, 25573, 29836, 23673, 1300, 15394, 14157, 1270, 23673, 29821, 1292, 25573, 23673, 1270, 23673, 29823, 17149, 29835, 29825, 14498, 1270, 29816, 15915, 1288, 17149, 29833, 19433, 1292, 29836, 23673, 14157, 1271, 23673, 29833, 29829, 1300, 29834, 29817, 29827, 14498, 14157, 1293, 18511, 29835, 25573, 17149, 1289, 14498, 17149, 1276, 15394, 14498, 29818, 1271, 25573, 23673, 25573, 29824, 23673, 25573, 22192, 1298, 29819, 29836, 29816, 1295, 25573, 1288, 23673, 22192, 1298, 29819, 29836, 29816, 14157, 1295, 15915, 1270, 23673, 15394, 14498, 15915, 1285, 17149, 29836, 17149, 19433, 1293, 25573, 23673, 29834, 25573, 29815, 1295, 29826, 29820, 29833, 1271, 29834, 29822, 17149, 1298, 29825, 15394, 1281, 15915, 25573, 17149, 1270, 29816, 15915, 1288, 17149, 29833, 19433, 1292, 29836, 23673, 1270, 29816, 15915, 1283, 25573, 29824, 1270, 29836, 1271, 29833, 29830, 23673, 1300, 29817, 29827, 22192, 15915, 14157, 1297, 29836, 1293, 23673, 29816, 29824, 1270, 23673, 29823, 15915, 25573, 17149, 1298, 25573, 23673, 29834, 25573, 29815, 1270, 23673, 22192, 29826, 29820, 29833, 1291, 14498, 1284, 17149, 14498, 29820, 1270, 23673, 15915, 29819, 25573, 29824, 19433, 1298, 25573, 23673, 29824, 29819, 29836, 15394, 1294, 23673, 29826, 15915, 22192, 1298, 15915, 29820, 29836, 1279, 23673, 29835, 1298, 29824, 29820, 17149, 1295, 29820, 22192, 15394, 1292, 29836, 23673, 1295, 25573, 23673, 29835, 1298, 25573, 29826, 29820, 25573, 29816, 14157, 1270, 15915, 1270, 23673, 29824, 25573, 29820, 17149, 1293, 25573, 29833, 17149, 1271, 25573, 23673, 23673, 14157, 1273, 29830, 25573, 23673, 29837, 1292, 25573, 23673, 1295, 25573, 23673, 29835, 1297, 29836, 1293, 25573, 23673, 29823, 15915, 15394, 14498, 29834, 1270, 29822, 25573, 1288, 22192, 23673, 1270, 23673, 29824, 29820, 17149, 1271, 15915, 29833, 29824, 14157, 1292, 29817, 23673, 1298, 23673, 22192, 1300, 29824, 29817, 29817, 29816, 102, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "\n",
    "output = model(token_ids, attention_mask = attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-2.0725e-01,  2.0815e-01, -3.2501e-01,  ..., -1.9035e-01,\n",
       "           7.6704e-01, -1.0966e-01],\n",
       "         [-1.4711e+00,  3.0560e-01,  3.1752e-01,  ...,  4.8062e-02,\n",
       "           6.0590e-01,  8.6927e-01],\n",
       "         [-4.6436e-01,  4.7697e-01, -1.0390e-01,  ..., -4.6546e-01,\n",
       "          -1.1156e-03, -7.7404e-01],\n",
       "         ...,\n",
       "         [ 3.8932e-01,  1.0696e-01, -7.2007e-01,  ..., -3.5734e-02,\n",
       "           3.4067e-02, -4.1209e-01],\n",
       "         [ 2.0424e-01,  2.5021e-01,  4.6779e-02,  ..., -1.7390e-01,\n",
       "           7.1936e-01, -4.0982e-01],\n",
       "         [ 6.9119e-02, -9.3067e-02,  5.2186e-02,  ..., -2.0024e-02,\n",
       "           5.4998e-01, -1.3584e-01]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-6.9516e-01, -3.4412e-01, -1.8805e-01,  5.5672e-01,  1.8127e-01,\n",
       "         -1.8612e-01,  2.6372e-01,  2.4826e-01,  2.0408e-01, -9.9983e-01,\n",
       "          1.1710e-01,  1.6371e-01,  9.5294e-01,  1.1354e-01,  6.3352e-01,\n",
       "         -2.2570e-01,  5.0741e-01, -4.6541e-01,  3.0480e-01,  1.3781e-01,\n",
       "          5.6942e-01,  9.9783e-01,  3.5280e-01,  3.0019e-01,  4.1953e-01,\n",
       "          4.9749e-02, -2.6996e-01,  7.3881e-01,  8.6652e-01,  5.5908e-01,\n",
       "         -4.6165e-01,  3.3628e-01, -9.4830e-01, -2.4534e-01, -3.9551e-01,\n",
       "         -9.6899e-01,  2.6481e-01, -4.3742e-01, -2.8761e-01, -7.1357e-02,\n",
       "         -6.4206e-01,  2.5853e-01,  9.9770e-01,  1.3951e-01,  2.0559e-01,\n",
       "         -2.3651e-01, -9.9940e-01,  2.5064e-01, -6.2953e-01, -4.6210e-02,\n",
       "         -9.5329e-02,  2.3240e-01,  1.5349e-01,  3.5468e-01,  3.5930e-01,\n",
       "         -1.3527e-01,  8.7891e-02,  2.4577e-01, -2.8002e-01, -4.3183e-01,\n",
       "         -4.4720e-01,  4.2588e-01, -4.6326e-01, -7.4241e-01, -4.9731e-01,\n",
       "         -3.6177e-03, -2.4430e-02, -2.6645e-01, -9.7254e-02,  6.9228e-02,\n",
       "          4.1337e-01,  3.0889e-01, -2.0964e-01, -5.6621e-01, -1.7198e-01,\n",
       "          2.0186e-01, -4.6977e-01,  1.0000e+00,  1.4118e-01, -9.2995e-01,\n",
       "          6.1408e-02,  1.0156e-02,  3.3132e-01,  2.4482e-01,  8.1956e-02,\n",
       "         -9.9997e-01,  4.3009e-01, -2.3770e-01, -9.5340e-01,  3.1805e-01,\n",
       "          1.7862e-01, -8.9102e-02,  2.9285e-01,  4.4531e-01, -2.0696e-01,\n",
       "         -3.4277e-01, -1.5136e-01, -3.9184e-01, -3.2205e-01, -3.6297e-01,\n",
       "          2.0869e-01, -3.1430e-01, -2.3993e-01, -2.2056e-01,  2.2409e-01,\n",
       "         -4.0951e-01,  1.8302e-02,  5.3246e-01, -2.2519e-01,  6.3582e-01,\n",
       "          4.3649e-01, -4.1667e-01,  3.4881e-01, -8.4107e-01,  3.8687e-01,\n",
       "         -3.1763e-01, -9.2457e-01, -4.2765e-01, -9.5222e-01,  5.1218e-01,\n",
       "         -1.0115e-01, -2.0678e-01,  7.4263e-01, -7.7537e-02,  2.9930e-01,\n",
       "         -2.3257e-01,  1.1400e-01, -1.0000e+00,  4.5232e-04, -4.5346e-01,\n",
       "         -7.6216e-02, -2.1191e-01, -9.2451e-01, -8.8157e-01,  5.3842e-01,\n",
       "          7.4555e-01,  1.5239e-01,  9.9529e-01, -2.6450e-01,  8.0363e-01,\n",
       "          4.2663e-01, -3.6901e-01, -2.5601e-01, -4.0773e-01,  6.1065e-01,\n",
       "         -2.7842e-01, -3.5634e-01,  1.6911e-01, -1.9595e-01, -8.3826e-02,\n",
       "          5.0202e-02, -2.4921e-01,  1.4338e-01, -6.3158e-01, -5.0180e-01,\n",
       "          8.0806e-01, -1.5317e-01,  5.0984e-02,  4.8065e-01, -3.5916e-01,\n",
       "         -1.7388e-01,  6.4524e-01,  3.0459e-01,  2.7864e-01,  4.6851e-03,\n",
       "          4.3758e-01,  5.7534e-02,  3.0895e-01, -6.9170e-01,  2.1317e-01,\n",
       "          4.3257e-01, -3.1067e-01, -4.1194e-01, -8.7827e-01, -3.0691e-01,\n",
       "          3.0335e-01,  9.0709e-01,  6.9557e-01,  2.7541e-01, -2.5345e-01,\n",
       "         -2.7430e-01,  1.6970e-01, -8.7759e-01,  9.1343e-01, -3.1182e-01,\n",
       "          2.1675e-01, -3.9580e-02,  4.9295e-01, -5.3289e-01, -9.9036e-02,\n",
       "          4.5373e-01, -3.0104e-04, -6.4698e-01, -6.9863e-02, -5.0627e-01,\n",
       "         -5.0834e-01, -5.3492e-01,  2.8910e-01, -2.3338e-01, -3.2538e-01,\n",
       "         -1.4647e-01,  7.5717e-01,  7.6581e-01,  3.4943e-01, -5.0154e-01,\n",
       "          4.0116e-01, -6.8067e-01, -2.5280e-01,  1.3932e-01,  4.1314e-01,\n",
       "          1.3691e-01,  9.2326e-01, -6.0035e-01, -5.7962e-02, -6.0859e-01,\n",
       "         -9.3064e-01,  2.4687e-01, -6.2993e-01,  5.2492e-02, -4.7101e-01,\n",
       "          3.7054e-01,  3.0611e-02, -2.3183e-01,  2.4239e-01, -6.3805e-01,\n",
       "         -5.9812e-01,  1.5018e-01, -4.1456e-01,  3.3728e-01, -2.5128e-01,\n",
       "          4.7550e-01,  5.7292e-01, -2.7913e-01, -1.9957e-01,  8.8127e-01,\n",
       "         -4.3125e-01, -4.2814e-01,  4.8387e-01, -3.1420e-01,  6.2156e-01,\n",
       "         -4.9096e-01,  9.6475e-01,  3.0292e-01, -4.3407e-02, -8.2198e-01,\n",
       "         -3.4124e-01, -4.1970e-01,  1.6086e-01, -1.5065e-01, -7.3881e-02,\n",
       "         -1.0350e-01,  4.4644e-01,  2.3497e-01,  3.8082e-01, -2.9259e-01,\n",
       "          8.1833e-01, -6.8480e-02, -8.7035e-01, -1.8300e-01, -1.4179e-01,\n",
       "         -9.5913e-01,  1.2336e-01,  3.7557e-01, -2.6612e-01, -3.4970e-01,\n",
       "         -3.1722e-01, -7.5252e-01,  4.9212e-01,  2.9715e-01,  8.5558e-01,\n",
       "         -3.0376e-01, -5.4293e-01, -3.9310e-01, -7.5314e-01, -3.8758e-02,\n",
       "         -2.7657e-02,  4.8169e-01,  6.1004e-02, -7.0239e-01,  5.0393e-01,\n",
       "          3.4157e-01,  3.5219e-01,  2.3132e-03,  9.2455e-01,  9.9978e-01,\n",
       "          9.1160e-01,  6.2872e-01,  3.5052e-01, -9.8381e-01, -2.5913e-01,\n",
       "          9.9983e-01, -5.7446e-01, -9.9995e-01, -7.4515e-01, -3.0463e-01,\n",
       "          2.8862e-01, -1.0000e+00, -2.6031e-01, -1.8131e-01, -7.1455e-01,\n",
       "         -1.2598e-01,  8.9316e-01,  7.6719e-01, -9.9999e-01,  6.9840e-01,\n",
       "          6.9927e-01, -4.6691e-01, -1.2173e-01, -3.4718e-01,  9.1875e-01,\n",
       "          3.8552e-01,  3.8563e-01, -1.8501e-01,  4.8356e-01, -3.3183e-01,\n",
       "         -7.2642e-01,  2.3306e-01, -2.0993e-01,  9.1611e-01,  1.1712e-01,\n",
       "         -7.0301e-01, -7.8137e-01,  4.2161e-02, -1.5756e-01, -1.1982e-01,\n",
       "         -8.2017e-01, -2.5968e-01, -2.4929e-01,  4.9109e-01,  9.1584e-02,\n",
       "          2.3764e-01, -4.7578e-01,  1.5804e-01,  8.0689e-02,  1.0034e-01,\n",
       "          4.9644e-01, -8.8622e-01, -3.2682e-01, -3.9635e-01, -3.3400e-01,\n",
       "         -7.6316e-02, -9.0793e-01,  8.7566e-01, -3.0564e-01,  3.6572e-02,\n",
       "          1.0000e+00,  2.9214e-01, -5.4641e-01,  2.9805e-01,  1.2760e-01,\n",
       "         -5.7895e-01,  9.9998e-01,  6.7521e-01, -9.2622e-01, -4.1622e-01,\n",
       "          6.3289e-01, -3.7572e-01, -3.8856e-01,  9.9549e-01, -2.4266e-03,\n",
       "          2.7702e-01,  4.3137e-01,  9.4548e-01, -9.6772e-01,  9.0252e-01,\n",
       "         -7.2667e-01, -8.4632e-01,  8.2414e-01,  7.4315e-01, -2.3164e-01,\n",
       "         -6.8039e-01,  2.3451e-01, -5.7236e-01,  3.0705e-01, -6.2633e-01,\n",
       "          1.9439e-01,  2.0443e-01, -9.6505e-02,  6.8551e-01, -1.3762e-01,\n",
       "         -4.1316e-01,  2.6958e-01, -1.4214e-01,  6.5021e-02,  6.6281e-01,\n",
       "          4.0951e-01, -1.2291e-01, -1.3927e-02, -2.0150e-01, -1.1360e-01,\n",
       "         -8.3700e-01,  2.7177e-01,  9.9999e-01, -1.2443e-01, -6.2313e-02,\n",
       "         -5.8421e-03, -9.1965e-02, -1.1582e-01,  4.0970e-01,  3.2443e-01,\n",
       "         -3.7737e-01, -7.1932e-01,  3.1446e-01, -2.0007e-01, -9.6470e-01,\n",
       "          3.1241e-01,  2.5029e-01, -2.2774e-01,  9.9479e-01,  1.3880e-01,\n",
       "          2.4757e-01, -8.0542e-02,  1.3385e-01,  2.7184e-01,  8.7699e-03,\n",
       "         -2.2292e-01,  8.7735e-01, -2.5952e-01,  3.7903e-01,  4.4399e-01,\n",
       "          9.2926e-02, -3.0385e-01, -4.9565e-01,  1.8427e-01, -8.0926e-01,\n",
       "         -1.3751e-01, -7.9139e-01,  8.2584e-01,  4.0201e-01,  3.6123e-01,\n",
       "          2.1329e-01,  4.0447e-01,  1.0000e+00, -2.9973e-01,  4.3596e-01,\n",
       "          4.7542e-01,  3.9568e-01, -9.8353e-01, -1.6170e-01, -2.8529e-01,\n",
       "         -7.3011e-02,  3.0085e-02, -2.0985e-01,  2.8388e-01, -8.9321e-01,\n",
       "         -2.0745e-01,  2.8025e-01, -6.3858e-01, -9.4517e-01,  2.9889e-03,\n",
       "          1.4767e-01,  1.6560e-01, -5.3691e-01, -1.1184e-01, -4.0384e-01,\n",
       "         -8.1787e-02, -3.5436e-01, -6.6763e-01,  5.9943e-01, -1.8281e-01,\n",
       "          3.2836e-01, -3.0174e-01,  3.4205e-01,  1.0016e-01,  4.9954e-01,\n",
       "         -2.5426e-01, -1.1948e-05, -7.0333e-02, -4.4502e-01,  9.8578e-02,\n",
       "         -5.7675e-01, -8.2967e-02, -2.5755e-01,  1.0000e+00, -5.4241e-01,\n",
       "          3.2199e-01,  5.3563e-01,  2.1774e-01, -3.2821e-01,  3.6702e-01,\n",
       "          6.4818e-01,  2.1677e-01, -6.0840e-02,  2.5205e-01,  4.5351e-01,\n",
       "         -3.3489e-01,  4.3239e-01,  1.9163e-02,  3.6325e-01,  4.2502e-01,\n",
       "          2.7693e-01,  1.3581e-01,  1.3025e-02,  1.8044e-01,  8.8768e-01,\n",
       "         -3.1400e-01, -1.4599e-01, -2.0348e-01, -3.0165e-01, -3.7466e-01,\n",
       "          3.6796e-01,  9.9997e-01,  3.3142e-01,  2.0621e-01, -9.6851e-01,\n",
       "          3.8435e-02, -7.6732e-01,  9.9952e-01,  5.3741e-01, -6.8771e-01,\n",
       "          4.7410e-01,  3.3344e-01, -3.1557e-01,  9.4890e-03, -2.7307e-01,\n",
       "         -1.3294e-01,  2.6248e-01,  1.3283e-01,  8.9715e-01, -2.3595e-01,\n",
       "         -8.7970e-01, -4.8875e-01,  2.3571e-01, -7.2256e-01,  9.9268e-01,\n",
       "         -4.6878e-01, -1.9901e-01, -1.4017e-01, -2.1075e-01, -3.4864e-01,\n",
       "          1.4086e-01, -9.0155e-01, -3.0540e-01, -3.0623e-03,  8.9167e-01,\n",
       "          2.5483e-01, -4.1758e-01, -7.8908e-01,  7.1500e-02, -7.9307e-03,\n",
       "          3.0350e-01, -8.0742e-01,  9.1328e-01, -8.7193e-01,  3.0378e-01,\n",
       "          9.9990e-01,  2.6268e-01, -2.5083e-01,  9.4748e-02, -2.7173e-01,\n",
       "          4.5076e-02, -8.5334e-02,  6.8733e-02, -7.5995e-01, -3.1170e-01,\n",
       "         -2.2881e-01,  2.6182e-01, -2.5522e-01, -4.6445e-01,  4.9877e-01,\n",
       "          1.6908e-01, -3.4916e-01, -3.4603e-01, -1.7012e-01,  3.1052e-01,\n",
       "          5.9562e-01, -3.4691e-01, -5.9842e-02,  1.0741e-01, -4.8610e-02,\n",
       "         -7.5847e-01, -4.3331e-01, -2.4506e-01, -9.9625e-01,  3.0087e-01,\n",
       "         -1.0000e+00,  4.9989e-01, -1.6673e-01, -3.2505e-01,  5.5512e-01,\n",
       "          2.6452e-02,  3.4702e-01, -5.3269e-01,  2.7672e-01,  7.0937e-01,\n",
       "          4.8261e-01, -1.6270e-01,  2.3572e-01, -5.2691e-01,  2.1588e-01,\n",
       "         -8.7194e-02,  2.5719e-01, -2.5134e-02,  5.1129e-01, -2.1928e-02,\n",
       "          1.0000e+00,  1.0332e-01, -4.2701e-02, -6.0480e-01,  4.0570e-01,\n",
       "         -2.3765e-01,  9.9971e-01, -4.4810e-01, -8.4475e-01,  3.8913e-01,\n",
       "         -4.9875e-01, -6.3778e-01,  1.0830e-01,  3.9690e-02, -4.4304e-01,\n",
       "         -1.2613e-01,  6.9148e-01,  1.4959e-02, -3.7908e-01,  4.6355e-01,\n",
       "         -2.9587e-01, -2.7581e-01,  1.8255e-01,  1.0537e-01,  9.4074e-01,\n",
       "          3.2786e-01,  6.3734e-01,  1.4471e-01, -2.6129e-01,  8.2239e-01,\n",
       "          3.9720e-01,  3.9729e-01,  2.4448e-01,  9.9996e-01,  3.1531e-01,\n",
       "         -7.8244e-01,  9.5511e-02, -7.9753e-01, -1.7112e-01, -7.9019e-01,\n",
       "          6.3697e-02,  1.3466e-01,  6.2620e-01, -1.7329e-01,  8.6247e-01,\n",
       "          2.9178e-01,  8.1913e-02,  5.0742e-02,  2.6546e-01,  3.4826e-01,\n",
       "         -7.1566e-01, -9.4736e-01, -9.3897e-01,  1.2339e-01, -3.6697e-01,\n",
       "         -2.4628e-01,  3.0858e-01,  3.5139e-01,  2.2780e-01,  2.9225e-01,\n",
       "         -9.9998e-01,  7.8225e-01,  4.5668e-01,  6.4267e-02,  8.8120e-01,\n",
       "          3.5938e-01,  3.2793e-01,  3.1590e-01, -9.3742e-01, -4.1005e-01,\n",
       "         -2.9366e-01, -2.3977e-01,  5.8336e-01,  5.3798e-01,  4.7785e-01,\n",
       "          2.5825e-01, -4.1539e-01, -1.8613e-01,  2.6236e-01,  4.1960e-03,\n",
       "         -9.5419e-01,  4.7050e-01,  3.8065e-01, -6.0431e-01,  8.2717e-01,\n",
       "         -9.6212e-02, -2.7167e-01,  3.3674e-01, -4.0777e-01,  5.0443e-01,\n",
       "          6.3331e-01,  1.9679e-01,  1.0756e-01,  4.9206e-01,  5.8141e-01,\n",
       "          7.2119e-01,  9.7744e-01, -2.4270e-01,  4.1966e-01, -1.0608e-02,\n",
       "          3.5561e-01,  5.3827e-01, -7.8987e-01,  1.3785e-01,  1.8490e-01,\n",
       "         -4.1933e-01,  4.0193e-01, -3.2292e-01, -6.3387e-01,  6.9551e-01,\n",
       "         -2.3829e-01,  3.3861e-01, -4.2030e-01, -9.9620e-02, -4.8487e-01,\n",
       "         -3.0650e-01, -4.8698e-01, -2.9841e-01,  4.2630e-01,  1.3474e-01,\n",
       "          7.1716e-01,  5.4239e-01, -5.7158e-02, -3.6580e-01, -2.2661e-01,\n",
       "         -5.3609e-02, -7.8541e-01,  4.1950e-01, -1.0925e-01, -1.7768e-01,\n",
       "          2.5438e-01,  4.4121e-03,  7.2293e-01, -1.9698e-01, -3.1544e-01,\n",
       "         -1.5805e-01, -7.1487e-01,  5.2400e-01, -2.8021e-01, -4.4534e-01,\n",
       "         -3.6011e-01,  4.9766e-01,  2.1894e-01,  9.9508e-01, -4.6246e-02,\n",
       "          1.1101e-01, -2.4652e-01, -2.6557e-01,  3.6895e-01, -1.9383e-01,\n",
       "         -9.9996e-01,  3.1248e-01, -1.7625e-01,  2.1121e-01,  1.8467e-01,\n",
       "          3.0469e-01, -7.3314e-02, -7.9243e-01, -2.2219e-01,  1.4345e-01,\n",
       "         -1.7440e-01, -4.0654e-01, -3.0393e-01,  4.2545e-01, -1.3141e-01,\n",
       "          1.5848e-01,  6.1131e-01, -2.7391e-01,  4.6790e-01,  4.5205e-01,\n",
       "         -5.4204e-01, -5.3873e-01,  5.4713e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying AraBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33806b8f64554f14b3fd3c46e613f143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/381 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d132aa5bd3b428fa2e4f7c2ae8e14a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd5574d2e2441ad9454574ef491c29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/825k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1765cee4434c4af4a3bd74bc4b9689ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.64M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe392740fcf4ef88d3bedaac0a9e4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4499f4d9d7948148e7c841d9507acd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Token Limit: 512\n"
     ]
    }
   ],
   "source": [
    "max_token_limit = tokenizer.model_max_length\n",
    "print(\"Max Token Limit:\", max_token_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40836\n",
      "361\n",
      "قوله أو قطع الأول يده إلخ قال الزركشي ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر وشد زنار ابن عرفة قول ابن شاس أو بفعل يتضمنه هو كلبس الزنار وإلقاء المصحف في صريح النجاسة والسجود للصنم ونحو ذلك وسحر محمد قول مالك وأصحابه أن الساحر كافر بالله تعالى قال مالك هو كالزنديق إذا عمل السحر بنفسه قتل ولم يستتب \n",
      "40836\n",
      "93\n",
      "['[CLS]', 'قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزر', '##كش', '##ي', 'ابن', 'عرفة', 'قوله', 'بلف', '##ظ', 'يقتضي', '##ه', 'كإ', '##نك', '##ار', 'غير', 'حديث', 'بالإسلام', 'وجوب', 'ما', 'علم', 'وجوب', '##ه', 'من', 'الدين', 'ضرورة', 'كإ', '##لقاء', 'مصح', '##ف', 'بق', '##ذر', 'وشد', 'زن', '##ار', 'ابن', 'عرفة', 'قول', 'ابن', 'شاس', 'أو', 'بفعل', 'يتضمنه', 'هو', 'كلب', '##س', 'الزنا', '##ر', 'وإلقاء', 'المصحف', 'في', 'صريح', 'النجا', '##سة', 'والس', '##جو', '##د', 'للص', '##نم', 'ونحو', 'ذلك', 'وسحر', 'محمد', 'قول', 'مالك', 'وأصحاب', '##ه', 'أن', 'الساحر', 'كافر', 'بالله', 'تعالى', 'قال', 'مالك', 'هو', 'كال', '##زن', '##ديق', 'إذا', 'عمل', 'السحر', 'بنفسه', 'قتل', 'ولم', 'يست', '##تب', '[SEP]']\n",
      "835\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# print(len(list_of_string_sentences))\n",
    "# print(len(list_of_string_sentences[0]))\n",
    "# print(list_of_string_sentences[0])\n",
    "# tokens_for_all_sentences=[]\n",
    "# max_token_len=0\n",
    "# min_tokens_len=1000\n",
    "# for sentence in list_of_string_sentences:\n",
    "#     tokens_in_sent=tokenizer.tokenize(sentence)\n",
    "#     tokens_in_sent = ['[CLS]'] + tokens_in_sent + ['[SEP]']\n",
    "#     max_token_len=max(len(tokens_in_sent),max_token_len)\n",
    "#     min_tokens_len=min(len(tokens_in_sent), min_tokens_len)\n",
    "#     tokens_for_all_sentences.append(tokens_in_sent)\n",
    "\n",
    "# print(len(tokens_for_all_sentences))\n",
    "# print(len(tokens_for_all_sentences[0]))\n",
    "# print(tokens_for_all_sentences[0])\n",
    "# print(max_token_len)\n",
    "# print(min_tokens_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41494\n",
      "93\n",
      "['[CLS]', 'قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزر', '##كش', '##ي', 'ابن', 'عرفة', 'قوله', 'بلف', '##ظ', 'يقتضي', '##ه', 'كإ', '##نك', '##ار', 'غير', 'حديث', 'بالإسلام', 'وجوب', 'ما', 'علم', 'وجوب', '##ه', 'من', 'الدين', 'ضرورة', 'كإ', '##لقاء', 'مصح', '##ف', 'بق', '##ذر', 'وشد', 'زن', '##ار', 'ابن', 'عرفة', 'قول', 'ابن', 'شاس', 'أو', 'بفعل', 'يتضمنه', 'هو', 'كلب', '##س', 'الزنا', '##ر', 'وإلقاء', 'المصحف', 'في', 'صريح', 'النجا', '##سة', 'والس', '##جو', '##د', 'للص', '##نم', 'ونحو', 'ذلك', 'وسحر', 'محمد', 'قول', 'مالك', 'وأصحاب', '##ه', 'أن', 'الساحر', 'كافر', 'بالله', 'تعالى', 'قال', 'مالك', 'هو', 'كال', '##زن', '##ديق', 'إذا', 'عمل', 'السحر', 'بنفسه', 'قتل', 'ولم', 'يست', '##تب', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# max_token_limit = 512  # or any other desired limit\n",
    "# stride = 256  # or any other desired overlap\n",
    "\n",
    "# tokens_for_all_sentences = []\n",
    "\n",
    "# for sentence in list_of_string_sentences:\n",
    "#     tokens_in_sent = tokenizer.tokenize(sentence)\n",
    "#     segments = []\n",
    "\n",
    "#     # Create overlapping segments\n",
    "#     for i in range(0, len(tokens_in_sent), stride):\n",
    "#         segment = ['[CLS]'] + tokens_in_sent[i:i + stride - 2] + ['[SEP]']\n",
    "#         segments.append(segment)\n",
    "\n",
    "#     tokens_for_all_sentences.extend(segments)\n",
    "\n",
    "\n",
    "# print(len(tokens_for_all_sentences))\n",
    "# print(len(tokens_for_all_sentences[0]))\n",
    "# print(tokens_for_all_sentences[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40836\n",
      "256\n",
      "['[CLS]', 'قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزر', '##كش', '##ي', 'ابن', 'عرفة', 'قوله', 'بلف', '##ظ', 'يقتضي', '##ه', 'كإ', '##نك', '##ار', 'غير', 'حديث', 'بالإسلام', 'وجوب', 'ما', 'علم', 'وجوب', '##ه', 'من', 'الدين', 'ضرورة', 'كإ', '##لقاء', 'مصح', '##ف', 'بق', '##ذر', 'وشد', 'زن', '##ار', 'ابن', 'عرفة', 'قول', 'ابن', 'شاس', 'أو', 'بفعل', 'يتضمنه', 'هو', 'كلب', '##س', 'الزنا', '##ر', 'وإلقاء', 'المصحف', 'في', 'صريح', 'النجا', '##سة', 'والس', '##جو', '##د', 'للص', '##نم', 'ونحو', 'ذلك', 'وسحر', 'محمد', 'قول', 'مالك', 'وأصحاب', '##ه', 'أن', 'الساحر', 'كافر', 'بالله', 'تعالى', 'قال', 'مالك', 'هو', 'كال', '##زن', '##ديق', 'إذا', 'عمل', 'السحر', 'بنفسه', 'قتل', 'ولم', 'يست', '##تب', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# constant_token_len = 256  # Set the desired maximum token length\n",
    "\n",
    "# tokens_for_all_sentences = []\n",
    "\n",
    "# for sentence in list_of_string_sentences:\n",
    "#     tokens_in_sent = tokenizer.tokenize(sentence)\n",
    "    \n",
    "#     # Truncate or pad to the desired length\n",
    "#     tokens_in_sent = ['[CLS]'] + tokens_in_sent[:constant_token_len - 2] + ['[SEP]']\n",
    "#     padding_length = max_token_len - len(tokens_in_sent)\n",
    "#     tokens_in_sent += ['[PAD]'] * padding_length\n",
    "    \n",
    "#     tokens_for_all_sentences.append(tokens_in_sent)\n",
    "\n",
    "# print(len(tokens_for_all_sentences))\n",
    "# print(len(tokens_for_all_sentences[0]))\n",
    "# print(tokens_for_all_sentences[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42115\n",
      "256\n",
      "['[CLS]', 'قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزر', '##كش', '##ي', 'ابن', 'عرفة', 'قوله', 'بلف', '##ظ', 'يقتضي', '##ه', 'كإ', '##نك', '##ار', 'غير', 'حديث', 'بالإسلام', 'وجوب', 'ما', 'علم', 'وجوب', '##ه', 'من', 'الدين', 'ضرورة', 'كإ', '##لقاء', 'مصح', '##ف', 'بق', '##ذر', 'وشد', 'زن', '##ار', 'ابن', 'عرفة', 'قول', 'ابن', 'شاس', 'أو', 'بفعل', 'يتضمنه', 'هو', 'كلب', '##س', 'الزنا', '##ر', 'وإلقاء', 'المصحف', 'في', 'صريح', 'النجا', '##سة', 'والس', '##جو', '##د', 'للص', '##نم', 'ونحو', 'ذلك', 'وسحر', 'محمد', 'قول', 'مالك', 'وأصحاب', '##ه', 'أن', 'الساحر', 'كافر', 'بالله', 'تعالى', 'قال', 'مالك', 'هو', 'كال', '##زن', '##ديق', 'إذا', 'عمل', 'السحر', 'بنفسه', 'قتل', 'ولم', 'يست', '##تب', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "256\n",
      "['[CLS]', 'قوله', 'تأمين', 'حربي', 'ينزل', 'لأمر', 'ينصرف', 'بان', '##قض', '##ائه', 'أي', 'إعطائه', 'الأمان', 'إلا', 'أن', 'هذا', 'أمان', 'خاص', 'لأنه', 'ينزل', 'لأمر', 'بمعنى', 'أنه', 'يؤمن', 'لنز', '##وله', 'لأرض', 'الإسلام', 'لشراء', 'ونحو', '##ه', 'فإذا', 'فرغ', 'سببه', 'انصرف', 'الأمان', 'وهذا', 'القيد', 'أخرج', 'به', 'المها', '##دن', '##ة', 'وغيرها', 'كما', 'ذكره', 'في', 'ك', 'ولا', 'يخفى', 'أنه', 'لا', 'يشمل', 'صور', 'الاستئ', '##مان', 'كلها', 'فإنه', 'لا', 'يشمل', 'ما', 'إذا', 'دخل', 'على', 'الإقامة', 'وإذا', 'علمت', 'ذلك', 'فليست', 'السين', 'والت', '##اء', 'للطلب', 'بل', 'زائد', '##تان', 'فإن', 'قلت', 'إذا', 'كانتا', 'زائد', '##تين', 'فيرج', '##ع', 'للأم', '##ان', 'قلت', 'هذه', 'حقائق', 'اصط', '##لاحي', '##ة', 'لهذه', 'الألفاظ', 'فلا', 'يرد', 'شيء', 'قوله', 'في', 'غير', 'معركة', 'لا', 'حاجة', 'لهذا', 'القيد', 'لأنه', 'إذا', 'قتل', 'في', 'معركة', 'وكان', 'ماله', 'معه', 'فهو', 'غنيمة', 'للمسلمين', 'قوله', 'ولم', 'يؤ', '##سر', 'قبل', 'موته', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "256\n",
      "['[CLS]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "max_token_len = 256  # Set the desired maximum token length\n",
    "\n",
    "tokens_for_all_sentences = []\n",
    "\n",
    "for sentence in list_of_string_sentences:\n",
    "    tokens_in_sent = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # If the sequence is shorter than 256 tokens, pad it\n",
    "    if len(tokens_in_sent) < max_token_len - 2:\n",
    "        tokens_in_sent=['[CLS]'] + tokens_in_sent + ['[SEP]']\n",
    "        padding_length = max_token_len - len(tokens_in_sent)\n",
    "        tokens_in_sent += ['[PAD]'] * padding_length\n",
    "        tokens_for_all_sentences.append(tokens_in_sent)\n",
    "\n",
    "    # If the sequence is longer than 256 tokens, segment it\n",
    "    else:\n",
    "        segments = []\n",
    "        # Create overlapping segments\n",
    "        for i in range(0, len(tokens_in_sent), max_token_len-2):\n",
    "            segment = ['[CLS]'] + tokens_in_sent[i:i + max_token_len - 2] + ['[SEP]']\n",
    "            #Now we need to check that segment reach 256\n",
    "            if(len(segment)<256):\n",
    "                padding_length = max_token_len - len(segment)\n",
    "                segment += ['[PAD]'] * padding_length\n",
    "\n",
    "            segments.append(segment)\n",
    "        tokens_for_all_sentences.extend(segments)\n",
    "\n",
    "print(len(tokens_for_all_sentences))\n",
    "print(len(tokens_for_all_sentences[0]))\n",
    "print(tokens_for_all_sentences[0])\n",
    "print(len(tokens_for_all_sentences[42113]))\n",
    "print(tokens_for_all_sentences[42113])\n",
    "print(len(tokens_for_all_sentences[42114]))\n",
    "print(tokens_for_all_sentences[42114])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check lengths of sequences\n",
    "sum=0\n",
    "for i, seq in enumerate(tokens_for_all_sentences):\n",
    "    if(len(seq)!=256):\n",
    "        sum=sum+1\n",
    "        print(f\"Sequence {i+1} length: {len(seq)}\")\n",
    "print(sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4151, 440, 4377, 810, 11035, 22323, 754, 19567, 19593, 193, 2298, 25425, 4151, 33729, 231, 24607, 195, 15527, 12978, 304, 650, 2643, 30771, 14513, 394, 1805, 14513, 195, 306, 1220, 2096, 15527, 10648, 24950, 198, 1211, 1871, 41872, 26823, 304, 2298, 25425, 5316, 2298, 18644, 440, 8842, 51492, 583, 30897, 213, 43496, 183, 29583, 28919, 305, 22300, 12007, 438, 1573, 6000, 196, 3373, 13361, 19042, 563, 56110, 582, 5316, 8210, 11674, 195, 331, 33948, 59474, 10515, 5698, 754, 8210, 583, 2818, 2126, 4299, 1217, 733, 22413, 8970, 2222, 1174, 885, 13655, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "token_ids_for_sentences=[]\n",
    "for tokens_in_sentence in tokens_for_all_sentences:\n",
    "    token_ids_for_sentences.append(tokenizer.convert_tokens_to_ids(tokens_in_sentence))\n",
    "print(token_ids_for_sentences[0])\n",
    "print(token_ids_for_sentences[42114])\n",
    "#I am not sure if CLS and SEP have token IDs of 2 and 3 or 102 and 103?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check lengths of sequences\n",
    "sum=0\n",
    "for i, seq in enumerate(token_ids_for_sentences):\n",
    "    if(len(seq)!=256):\n",
    "        sum=sum+1\n",
    "        print(f\"Sequence {i+1} length: {len(seq)}\")\n",
    "print(sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNUSED_952]',\n",
       " 'هتاف',\n",
       " 'متى',\n",
       " 'جلسته',\n",
       " 'وأنشأ',\n",
       " '[UNUSED_2168]',\n",
       " '##هاء',\n",
       " 'ادارات',\n",
       " 'المشغولات',\n",
       " '##ابرز',\n",
       " '##واري',\n",
       " 'يحترم',\n",
       " 'جاهدا',\n",
       " 'مساره',\n",
       " '192',\n",
       " 'بوسائل',\n",
       " '##فاضلة',\n",
       " 'واستخراج',\n",
       " 'المبرمة',\n",
       " 'والحد']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[5000:5020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "tokens_to_check = [\"[CLS]\", \"[SEP]\"]\n",
    "\n",
    "# Convert tokens to token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens_to_check)\n",
    "\n",
    "print(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark each of the tokens as belonging to sentence \"1\".\n",
    "segment_ids_in_sentences=[]\n",
    "for i,segment_sent in enumerate(token_ids_for_sentences):\n",
    "    segment_ids= [i]* len(segment_sent)\n",
    "    segment_ids_in_sentences.append(segment_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42115\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114, 42114]\n"
     ]
    }
   ],
   "source": [
    "print(len(segment_ids_in_sentences))\n",
    "print(segment_ids_in_sentences[0])\n",
    "print(segment_ids_in_sentences[42114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42115\n",
      "42115\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "print(len(token_ids_for_sentences))\n",
    "print(len(segment_ids_in_sentences))\n",
    "\n",
    "print(len(token_ids_for_sentences[800]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert token IDs and segment IDs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor(token_ids_for_sentences)\n",
    "segments_tensor = torch.tensor(segment_ids_in_sentences)\n",
    "\n",
    "batch_size = 100  # Set an appropriate batch size\n",
    "tokens_tensor = tokens_tensor[:batch_size]\n",
    "segments_tensor = segments_tensor[:batch_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 256, 768])\n",
      "tensor([[ 0.7184,  1.4945,  0.5356,  ..., -0.8636, -0.2222, -0.0933],\n",
      "        [ 0.6897,  1.5559,  0.5676,  ..., -0.8897, -0.3539, -0.1131],\n",
      "        [ 0.6841,  1.6203,  0.5395,  ..., -0.8518, -0.3706, -0.0906],\n",
      "        ...,\n",
      "        [ 0.6723,  1.3007,  0.5677,  ..., -0.8933, -0.2771, -0.0411],\n",
      "        [ 0.6737,  1.3088,  0.5674,  ..., -0.8945, -0.2773, -0.0404],\n",
      "        [ 0.6749,  1.3187,  0.5686,  ..., -0.8955, -0.2789, -0.0407]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forward pass through the model to get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensor)\n",
    "\n",
    "# Extract embeddings from the last hidden states\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Now, 'last_hidden_states' contains the embeddings for each token in each sentence\n",
    "print(last_hidden_states.shape)   # (batch size, sequence length, hidden size)  # all sequences must have 256 length\n",
    "print(last_hidden_states[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we can do pooling for sentence-level embeddings (batch size, hidden size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_tensor = torch.tensor([indexed_tokens])\n",
    "# segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs= model(tokens_tensor,segments_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vecmodel(input):\n",
    "    # Train Word2Vec model\n",
    "    Word2Vec_model = Word2Vec(input, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
    "    Word2Vec_model.train(input,total_examples=len(listOfwordsWith_NoDiacritics),epochs=10)\n",
    "    word_embeddings = {word: Word2Vec_model.wv[word] for word in Word2Vec_model.wv.index_to_key}\n",
    "    return Word2Vec_model, word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_model, word_embeddings= word2vecmodel(list_of_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-Building the LSTM model for character level classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatingXandYtrain(listofDiacritrcs_ToWord,listOfwordsWith_NoDiacritics):\n",
    "    for i in range(len(listofDiacritrcs_ToWord)):\n",
    "        listofDiacritrcs_ToWord[i] = \" \".join(listofDiacritrcs_ToWord[i])\n",
    "    Y_train= np.array([listofDiacritrcs_ToWord],dtype=object).T\n",
    "    X_train = np.array([listOfwordsWith_NoDiacritics],dtype=object)\n",
    "    print(str(X_train.shape))\n",
    "    print(str(Y_train.shape))\n",
    "    return X_train,Y_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2102054)\n",
      "(2102054, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train= formatingXandYtrain(listofDiacritrcs_ToWord,listOfwordsWith_NoDiacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDiacritics(listOfwordsWith_NoDiacritics,listofDiacritrcs_ToWord):\n",
    "    # Tokenize the input words and diacritics\n",
    "    word_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters='')\n",
    "    word_tokenizer.fit_on_texts(listOfwordsWith_NoDiacritics)\n",
    "    word_sequences = word_tokenizer.texts_to_sequences(listOfwordsWith_NoDiacritics)\n",
    "\n",
    "    diacritic_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters='')\n",
    "    diacritic_tokenizer.fit_on_texts([''.join(d) for d in listofDiacritrcs_ToWord])\n",
    "    diacritic_sequences = diacritic_tokenizer.texts_to_sequences([''.join(d) for d in listofDiacritrcs_ToWord])\n",
    "\n",
    "    # Pad sequences to have the same length\n",
    "    max_len = max(max(len(seq) for seq in word_sequences), max(len(seq) for seq in diacritic_sequences))\n",
    "    padded_word_sequences = pad_sequences(word_sequences, maxlen=max_len, padding='post')\n",
    "    padded_diacritic_sequences = pad_sequences(diacritic_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=50, input_length=max_len))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(Dense(len(diacritic_tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(padded_word_sequences, np.expand_dims(padded_diacritic_sequences, -1), epochs=10, batch_size=32)\n",
    "\n",
    "    # Save the model for later use\n",
    "    model.save(\"diacritic_prediction_model.h5\")\n",
    "\n",
    "    return model,word_tokenizer,diacritic_tokenizer,max_len\n",
    "\n",
    "# Now, you can use the trained model to predict diacritics for new Arabic words\n",
    "def predict_diacritics(model,word_tokenizer, diacritic_tokenizer,max_len, word):\n",
    "    word_sequence = word_tokenizer.texts_to_sequences([word])\n",
    "    padded_word_sequence = pad_sequences(word_sequence, maxlen=max_len, padding='post')\n",
    "    predicted_diacritic_sequence = model.predict(padded_word_sequence)\n",
    "    predicted_diacritic_sequence = np.argmax(predicted_diacritic_sequence, axis=-1)\n",
    "    predicted_diacritic = diacritic_tokenizer.sequences_to_texts(predicted_diacritic_sequence)\n",
    "    return predicted_diacritic[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model,word_tokenizer,diacritic_tokenizer,max_len=trainDiacritics(listOfwordsWith_NoDiacritics,listofDiacritrcs_ToWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19628\\2301089671.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'diacritic_prediction_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\u001b[0m in \u001b[0;36m_process_kwargs\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m                 )\n\u001b[0;32m    114\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                 raise TypeError(\n\u001b[0m\u001b[0;32m    116\u001b[0m                     \u001b[1;34mf\"{k} is not a valid argument, kwargs should be empty \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                     \u001b[1;34m\" for `optimizer_experimental.Optimizer`.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`."
     ]
    }
   ],
   "source": [
    "model = load_model('diacritic_prediction_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "['َ', ' ', ' ', ' ', ' ', ' ', 'َ']\n",
      "Input word: قال\n",
      "Predicted diacritics: َ     َ\n",
      "َ\n",
      "َ\n",
      "قَاَ\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[' ', ' ', 'ْ', ' ', 'ّ', ' ', 'َ', ' ', ' ', ' ', 'ْ', ' ', ' ', ' ', 'َ', ' ', ' ', ' ', 'ِ', ' ', ' ', ' ', 'ّ', ' ', 'ُ']\n",
      "Input word: الزركشي\n",
      "Predicted diacritics:   ْ ّ َ   ْ   َ   ِ   ّ ُ\n",
      "ْ\n",
      "ّ\n",
      "here\n",
      "ْ\n",
      "َ\n",
      "ِ\n",
      "ّ\n",
      "here\n",
      "اْلَّزْرَكِشُّ\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "[' ', ' ', 'ْ', ' ', ' ', ' ', 'ِ']\n",
      "Input word: ابن\n",
      "Predicted diacritics:   ْ   ِ\n",
      "ْ\n",
      "ِ\n",
      "اْبِ\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "['َ', ' ', ' ', ' ', 'َ', ' ', ' ', ' ', 'َ', ' ', ' ', ' ', 'َ']\n",
      "Input word: عرفة\n",
      "Predicted diacritics: َ   َ   َ   َ\n",
      "َ\n",
      "َ\n",
      "َ\n",
      "َ\n",
      "عَرَفَةَ\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "['َ', ' ', ' ', ' ', 'ْ', ' ', ' ', ' ', 'ُ', ' ', ' ', ' ', 'ُ']\n",
      "Input word: قوله\n",
      "Predicted diacritics: َ   ْ   ُ   ُ\n",
      "َ\n",
      "ْ\n",
      "ُ\n",
      "ُ\n",
      "قَوْلُهُ\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "['ِ', ' ', ' ', ' ', 'َ', ' ', ' ', ' ', 'ْ', ' ', ' ', ' ', 'ِ']\n",
      "Input word: بلفظ\n",
      "Predicted diacritics: ِ   َ   ْ   ِ\n",
      "ِ\n",
      "َ\n",
      "ْ\n",
      "ِ\n",
      "بِلَفْظِ\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "['َ', ' ', ' ', ' ', 'ْ', ' ', ' ', ' ', 'َ', ' ', ' ', ' ', 'ِ', ' ', ' ', ' ', ' ', ' ', 'ُ']\n",
      "Input word: يقتضيه\n",
      "Predicted diacritics: َ   ْ   َ   ِ     ُ\n",
      "َ\n",
      "ْ\n",
      "َ\n",
      "ِ\n",
      "ُ\n",
      "يَقْتَضِيُ\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "['َ', ' ', ' ', ' ', 'ِ', ' ', ' ', ' ', 'ْ', ' ', ' ', ' ', 'َ', ' ', ' ', ' ', ' ', ' ', 'ِ']\n",
      "Input word: كإنكار\n",
      "Predicted diacritics: َ   ِ   ْ   َ     ِ\n",
      "َ\n",
      "ِ\n",
      "ْ\n",
      "َ\n",
      "ِ\n",
      "كَإِنْكَاِ\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "['َ', ' ', ' ', ' ', 'ْ', ' ', ' ', ' ', 'ِ']\n",
      "Input word: غير\n",
      "Predicted diacritics: َ   ْ   ِ\n",
      "َ\n",
      "ْ\n",
      "ِ\n",
      "غَيْرِ\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "['َ', ' ', ' ', ' ', 'ِ', ' ', ' ', ' ', ' ', ' ', 'ِ']\n",
      "Input word: حديث\n",
      "Predicted diacritics: َ   ِ     ِ\n",
      "َ\n",
      "ِ\n",
      "ِ\n",
      "حَدِيِ\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "['ِ', ' ', ' ', ' ', ' ', ' ', 'ْ', ' ', ' ', ' ', 'ِ', ' ', ' ', ' ', 'ْ', ' ', ' ', ' ', 'َ', ' ', ' ', ' ', ' ', ' ', 'ِ']\n",
      "Input word: بالإسلام\n",
      "Predicted diacritics: ِ     ْ   ِ   ْ   َ     ِ\n",
      "ِ\n",
      "ْ\n",
      "ِ\n",
      "ْ\n",
      "َ\n",
      "ِ\n",
      "بِاْلِإْسَلِ\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "['َ', ' ', ' ', ' ', 'ُ', ' ', ' ', ' ', ' ', ' ', 'ُ']\n",
      "Input word: وجوب\n",
      "Predicted diacritics: َ   ُ     ُ\n",
      "َ\n",
      "ُ\n",
      "ُ\n",
      "وَجُوُ\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "['َ']\n",
      "Input word: ما\n",
      "Predicted diacritics: َ\n",
      "َ\n",
      "مَ\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "['َ', ' ', ' ', ' ', 'َ', ' ', ' ', ' ', 'َ']\n",
      "Input word: علم\n",
      "Predicted diacritics: َ   َ   َ\n",
      "َ\n",
      "َ\n",
      "َ\n",
      "عَلَمَ\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "['َ', ' ', ' ', ' ', 'ُ', ' ', ' ', ' ', ' ', ' ', 'ِ', ' ', ' ', ' ', 'ُ']\n",
      "Input word: وجوبه\n",
      "Predicted diacritics: َ   ُ     ِ   ُ\n",
      "َ\n",
      "ُ\n",
      "ِ\n",
      "ُ\n",
      "وَجُوِبُ\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "['َ', ' ', ' ', ' ', 'ْ']\n",
      "Input word: من\n",
      "Predicted diacritics: َ   ْ\n",
      "َ\n",
      "ْ\n",
      "مَنْ\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "[' ', ' ', 'ْ', ' ', 'ّ', ' ', 'ِ', ' ', ' ', ' ', 'ْ', ' ', ' ', ' ', 'ِ']\n",
      "Input word: الدين\n",
      "Predicted diacritics:   ْ ّ ِ   ْ   ِ\n",
      "ْ\n",
      "ّ\n",
      "here\n",
      "ْ\n",
      "ِ\n",
      "اْلِّدْيِ\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "['َ', ' ', ' ', ' ', 'ُ', ' ', ' ', ' ', ' ', ' ', 'َ', ' ', ' ', ' ', 'ٍ']\n",
      "Input word: ضرورة\n",
      "Predicted diacritics: َ   ُ     َ   ٍ\n",
      "َ\n",
      "ُ\n",
      "َ\n",
      "ٍ\n",
      "ضَرُوَرٍ\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "['َ', ' ', ' ', ' ', 'ِ', ' ', ' ', ' ', 'ْ', ' ', ' ', ' ', 'َ', ' ', ' ', ' ', ' ', ' ', 'ِ']\n",
      "Input word: كإلقاء\n",
      "Predicted diacritics: َ   ِ   ْ   َ     ِ\n",
      "َ\n",
      "ِ\n",
      "ْ\n",
      "َ\n",
      "ِ\n",
      "كَإِلْقَاِ\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "['َ', ' ', ' ', ' ', 'ْ', ' ', ' ', ' ', 'َ', ' ', ' ', ' ', 'ٍ']\n",
      "Input word: مصحف\n",
      "Predicted diacritics: َ   ْ   َ   ٍ\n",
      "َ\n",
      "ْ\n",
      "َ\n",
      "ٍ\n",
      "مَصْحَفٍ\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "['ِ', ' ', ' ', ' ', 'َ', ' ', ' ', ' ', 'َ']\n",
      "Input word: بقذ\n",
      "Predicted diacritics: ِ   َ   َ\n",
      "ِ\n",
      "َ\n",
      "َ\n",
      "بِقَذَ\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# input_word = \"الذي علم بالقلم\"\n",
    "# input_word = \"ذهب علي إلى الشاطئ\"\n",
    "input = \"قال الزركشي ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر\"\n",
    "input_word = input.split(' ')\n",
    "word_vectors = [word_embeddings_model.wv[word] for word in input_word if word in Word2Vec_model.wv]\n",
    "# print(word_vectors)\n",
    "\n",
    "for word in input_word: \n",
    "  predicted_diacritics1=predict_diacritics(model,word_tokenizer, diacritic_tokenizer,max_len, word)\n",
    "  print(list(predicted_diacritics1))\n",
    "  print(f\"Input word: {word}\")\n",
    "  print(f\"Predicted diacritics: {predicted_diacritics1}\")\n",
    "  tempString=str()\n",
    "  counterWord=0\n",
    "  counterDiacrtic=0\n",
    "  while counterDiacrtic<(len(predicted_diacritics1)):\n",
    "    if predicted_diacritics1[counterDiacrtic]!=' ':\n",
    "      tempString=tempString+word[counterWord]\n",
    "      tempString=tempString+predicted_diacritics1[counterDiacrtic]\n",
    "      print(predicted_diacritics1[counterDiacrtic])\n",
    "      if predicted_diacritics1[counterDiacrtic] == 'ّ':\n",
    "        print(\"here\")\n",
    "        tempString=tempString+predicted_diacritics1[counterDiacrtic+2]\n",
    "        counterDiacrtic=counterDiacrtic+2\n",
    "      counterWord+=1\n",
    "    counterDiacrtic+=1\n",
    "\n",
    "  print(tempString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
